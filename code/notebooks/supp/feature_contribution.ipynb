{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-25T16:08:46.665750Z",
     "start_time": "2019-08-25T16:08:43.103145Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hypertools as hyp\n",
    "from os.path import abspath, join as opj\n",
    "from scipy.stats import pearsonr, sem\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import analysis helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Functions and variables used across multiple notebooks can be found [here](https://github.com/contextlab/sherlock-topic-model-paper/blob/master/code/helpers/analysis_helpers.py)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sys.path.insert(0, abspath('../../helpers/'))\n",
    "from analysis_helpers import (\n",
    "    N_TOPICS,\n",
    "    VIDEO_WSIZE,\n",
    "    RECALL_WSIZE,\n",
    "    VECTORIZER_PARAMS,\n",
    "    SEMANTIC_PARAMS,\n",
    "    format_text,\n",
    "    parse_windows,\n",
    "    get_video_timepoints,\n",
    "    warp_recall,\n",
    "    create_diag_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set paths & parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-25T16:08:46.670239Z",
     "start_time": "2019-08-25T16:08:46.667549Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rawdir = '../../../data/raw/'\n",
    "datadir = '../../../data/processed/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wrap full topic modeling pipeline\n",
    "def transform_video(annotations):\n",
    "    dropcols = ['Start Time (s) ', 'End Time (s) ', \n",
    "                'Start Time (TRs, 1.5s)', 'End Time (TRs, 1.5s)']\n",
    "    features = annotations.drop(columns=dropcols)\n",
    "    scenes_list = features.apply(format_text, axis=1).tolist()\n",
    "    video_windows, window_bounds = parse_windows(scenes_list, VIDEO_WSIZE)\n",
    "    \n",
    "    video_model = hyp.tools.format_data(video_windows, \n",
    "                                    vectorizer=VECTORIZER_PARAMS, \n",
    "                                    semantic=SEMANTIC_PARAMS, \n",
    "                                    corpus=video_windows)[0]\n",
    "    \n",
    "    tr_spans = video_text[['Start Time (TRs, 1.5s)', 'End Time (TRs, 1.5s)']]\n",
    "    starts, stops = tr_spans.values.T\n",
    "    video_model_TRs = np.empty((1976, 100))\n",
    "    xvals = get_video_timepoints(window_bounds, annotations)\n",
    "    xvals_TR = np.array(xvals) * 1976 / 2963\n",
    "    TR_times = np.arange(1, 1977)\n",
    "    interp_func = interp1d(xvals_TR, video_model, axis=0, fill_value='extrapolate')\n",
    "    video_model_TRs = interp_func(TR_times)\n",
    "    return video_model_TRs, video_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_recalls(recall_windows, video_windows, video_traj):\n",
    "    recall_models = hyp.tools.format_data(recall_windows, \n",
    "                                          vectorizer=VECTORIZER_PARAMS, \n",
    "                                          semantic=SEMANTIC_PARAMS, \n",
    "                                          corpus=video_windows)\n",
    "    # warp recall trajectores to video trajectory length\n",
    "    recalls_warped = [warp_recall(r, video_traj, return_paths=False) \n",
    "                      for r in recall_models]\n",
    "    return recalls_warped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correlate_structures(video, other):\n",
    "    assert video.shape == other.shape\n",
    "    vcorr = np.corrcoef(video)\n",
    "    ocorr = np.corrcoef(other)\n",
    "    diag_mask = create_diag_mask(vcorr, diag_limit=238)  # precomputed from intact video\n",
    "    v = vcorr[diag_mask]\n",
    "    o = ocorr[diag_mask]\n",
    "    return pearsonr(v, o)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-25T16:08:46.908326Z",
     "start_time": "2019-08-25T16:08:46.671869Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "video_text = pd.read_excel(opj(rawdir, 'Sherlock_Segments_1000_NN_2017.xlsx'))\n",
    "video_text['Scene Segments'].fillna(method='ffill', inplace=True)\n",
    "video_text.drop(index=[480, 481], inplace=True)\n",
    "video_text.reset_index(drop=True, inplace=True)\n",
    "video_text.loc[480:, 'Start Time (s) ': 'End Time (s) '] += video_text.loc[479, 'End Time (s) ']\n",
    "keep_cols = np.append(video_text.columns[1:5], video_text.columns[6:15])\n",
    "video_text = video_text.loc[:, keep_cols]\n",
    "video_text.columns = list(video_text.columns[:4]) + ['Narrative details', 'Indoor vs outdoor', \n",
    "                                                     'Characters on screen', 'Character in focus', \n",
    "                                                     'Character speaking', 'Location', 'Camera angle', \n",
    "                                                     'Music presence', 'Text on screen']\n",
    "\n",
    "# trajectories created from all features\n",
    "full_trajs = np.load(opj(datadir, 'models_t100_v50_r10.npy'), allow_pickle=True)\n",
    "full_video, full_recalls = full_trajs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recall_w = []\n",
    "for sub in range(1, 18):\n",
    "    transcript_path = opj(rawdir, f'NN{sub} transcript.txt')\n",
    "    with open(transcript_path, 'r', encoding='cp1252') as f:\n",
    "        recall = f.read().replace(b'\\x92'.decode('cp1252'), \"'\").strip()\n",
    "    recall_fmt = format_text(recall).split('.')\n",
    "    if not recall_fmt[-1]:\n",
    "        recall_fmt = recall_fmt[:-1]\n",
    "    sub_recall_w = parse_windows(recall_fmt, RECALL_WSIZE)[0]\n",
    "    recall_w.append(sub_recall_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteratively hold out one feature and transform remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-25T16:12:20.469876Z",
     "start_time": "2019-08-25T16:08:47.110997Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Narrative details:\n",
      "\tsimilarity to full video: 0.7840464784236999\n",
      "\tvideo-recall structure similarity: 0.49789433751038686, SEM: 0.03575362265021856\n",
      "\n",
      "Indoor vs outdoor:\n",
      "\tsimilarity to full video: 0.8576974275883418\n",
      "\tvideo-recall structure similarity: 0.6569000158177016, SEM: 0.027134409621705116\n",
      "\n",
      "Characters on screen:\n",
      "\tsimilarity to full video: 0.7976882761325998\n",
      "\tvideo-recall structure similarity: 0.6818517096348622, SEM: 0.027818639564926455\n",
      "\n",
      "Character in focus:\n",
      "\tsimilarity to full video: 0.873694587735584\n",
      "\tvideo-recall structure similarity: 0.5982538568209149, SEM: 0.03024753485130499\n",
      "\n",
      "Character speaking:\n",
      "\tsimilarity to full video: 0.8651720030104021\n",
      "\tvideo-recall structure similarity: 0.6303607927331502, SEM: 0.03204785143227152\n",
      "\n",
      "Location:\n",
      "\tsimilarity to full video: 0.7871552037453584\n",
      "\tvideo-recall structure similarity: 0.68326123617632, SEM: 0.025844925341076984\n",
      "\n",
      "Camera angle:\n",
      "\tsimilarity to full video: 0.8431489481128159\n",
      "\tvideo-recall structure similarity: 0.6603313082637992, SEM: 0.028157417303993324\n",
      "\n",
      "Music presence:\n",
      "\tsimilarity to full video: 0.8612995667404652\n",
      "\tvideo-recall structure similarity: 0.6342366880258695, SEM: 0.028670715955777595\n",
      "\n",
      "Text on screen:\n",
      "\tsimilarity to full video: 0.860868030083184\n",
      "\tvideo-recall structure similarity: 0.653464523885414, SEM: 0.028586727849905673\n",
      "\n",
      "All features\n",
      "\tvideo-recall structure similarity: 0.6520455401584276, SEM: 0.026843846623524615\n"
     ]
    }
   ],
   "source": [
    "features = video_text.columns[4:]\n",
    "# dropfeat_corrs = dict.fromkeys(features)\n",
    "analyses = ['full vid corr', 'vid rec corr', 'vid rec sem']\n",
    "dropfeat_corrs = pd.DataFrame(index=features, columns=analyses)\n",
    "\n",
    "for feature in features:\n",
    "    print(f'{feature}:')\n",
    "    # transform remaining annotations\n",
    "    other_features = video_text.drop(feature, axis=1)\n",
    "#     other_features = partial_df.loc[:, partial_df.columns[4:]]\n",
    "    dropfeat_vid, dropfeat_vid_ws = transform_video(other_features)\n",
    "    \n",
    "    # compute similarity with full-feature video trajectory structure\n",
    "    full_video_corr = correlate_structures(full_video, dropfeat_vid)\n",
    "    \n",
    "    # transform recalls using feature-removed corpus\n",
    "    dropfeat_recs = transform_recalls(recall_w, dropfeat_vid_ws, dropfeat_vid)\n",
    "    \n",
    "    # compare structures to partial video model\n",
    "    rec_corrs = np.array([correlate_structures(dropfeat_vid, r) \n",
    "                          for r in dropfeat_recs])\n",
    "    feat_corr, feat_sem = rec_corrs.mean(), sem(rec_corrs)\n",
    "\n",
    "    dropfeat_corrs.loc[feature] = [full_video_corr, feat_corr, feat_sem]\n",
    "    print(f'\\tsimilarity to full video: {full_video_corr}')\n",
    "    print(f'\\tvideo-recall structure similarity: {feat_corr}, SEM: {feat_sem}\\n')\n",
    "    \n",
    "# add data for full model\n",
    "rec_corr_full = np.array([correlate_structures(full_video, warp_recall(r, full_video, return_paths=False)) \n",
    "                          for r in full_recalls])\n",
    "dropfeat_corrs.loc['All features'] = [1, rec_corr_full.mean(), sem(rec_corr_full)]\n",
    "print('All features')\n",
    "print(f'\\tvideo-recall structure similarity: {rec_corr_full.mean()}, SEM: {sem(rec_corr_full)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dropfeat_corrs.to_pickle(opj(datadir, 'feature_contribution.p'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
