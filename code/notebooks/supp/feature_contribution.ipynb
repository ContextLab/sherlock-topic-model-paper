{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-25T16:08:46.665750Z",
     "start_time": "2019-08-25T16:08:43.103145Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hypertools as hyp\n",
    "from os.path import join as opj\n",
    "from scipy.stats import pearsonr, sem\n",
    "from scipy.interpolate import interp1d\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set paths & parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-25T16:08:46.670239Z",
     "start_time": "2019-08-25T16:08:46.667549Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rawdir = '../../../data/raw/'\n",
    "datadir = '../../../data/processed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-25T16:08:46.915490Z",
     "start_time": "2019-08-25T16:08:46.909957Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_topics = 100\n",
    "video_wsize = 50\n",
    "recall_wsize = 10\n",
    "\n",
    "vectorizer = {\n",
    "    'model' : 'CountVectorizer', \n",
    "    'params' : {\n",
    "        'stop_words' : 'english'\n",
    "    }\n",
    "}\n",
    "\n",
    "semantic = {\n",
    "    'model' : 'LatentDirichletAllocation', \n",
    "    'params' : {\n",
    "        'n_components' : n_topics,\n",
    "        'learning_method' : 'batch',\n",
    "        'random_state' : 0\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_text(text):\n",
    "    if isinstance(text, pd.Series):\n",
    "        text = ' '.join(list(text.dropna()))\n",
    "        pattern = \"[^\\w\\s]+\"\n",
    "    else:\n",
    "        pattern = \"[^.\\w\\s]+\"\n",
    "\n",
    "    no_possessive = text.lower().replace(\"'s\", '')\n",
    "    punc_stripped = re.sub(pattern, '', no_possessive)\n",
    "    spaced = ' '.join(punc_stripped.split())\n",
    "    return spaced\n",
    "\n",
    "def parse_windows(textlist, wsize):\n",
    "    windows = []\n",
    "    w_lengths = []\n",
    "    for ix in range(1, wsize):\n",
    "        start, end = 0, ix\n",
    "        w_lengths.append((start, end))\n",
    "        windows.append(' '.join(textlist[start : end]))\n",
    "\n",
    "    for ix in range(len(textlist)):\n",
    "        start = ix\n",
    "        end = ix + wsize if ix + wsize <= len(textlist) else len(textlist)\n",
    "        w_lengths.append((start, end))\n",
    "        windows.append(' '.join(textlist[start : end]))\n",
    "\n",
    "    return windows, w_lengths\n",
    "\n",
    "\n",
    "def get_video_timepoints(window_spans):\n",
    "    timepoints = []\n",
    "    for first, last in window_spans:\n",
    "        window_onset = video_text.loc[first, 'Start Time (s) ']\n",
    "        window_offset = video_text.loc[last - 1, 'End Time (s) ']\n",
    "        timepoints.append((window_onset + window_offset) / 2)\n",
    "        \n",
    "    return np.array(timepoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wrap full topic modeling pipeline\n",
    "def transform_video(annotations):\n",
    "    scenes_list = annotations.apply(format_text, axis=1).tolist()\n",
    "    video_windows, window_bounds = parse_windows(scenes_list, video_wsize)\n",
    "    \n",
    "    video_model = hyp.tools.format_data(video_windows, \n",
    "                                    vectorizer=vectorizer, \n",
    "                                    semantic=semantic, \n",
    "                                    corpus=video_windows)[0]\n",
    "    \n",
    "    tr_spans = video_text[['Start Time (TRs, 1.5s)', 'End Time (TRs, 1.5s)']]\n",
    "    starts, stops = tr_spans.values.T\n",
    "    video_model_TRs = np.empty((1976, 100))\n",
    "    xvals = get_video_timepoints(window_bounds)\n",
    "    xvals_TR = np.array(xvals) * 1976 / 2963\n",
    "    TR_times = np.arange(1, 1977)\n",
    "    interp_func = interp1d(xvals_TR, video_model, axis=0, fill_value='extrapolate')\n",
    "    video_model_TRs = interp_func(TR_times)\n",
    "    return video_model_TRs, video_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def interpolate_recall(recall_traj):\n",
    "    n_windows = recall_traj.shape[0]\n",
    "    window_xvals = np.linspace(0, 1976, n_windows)\n",
    "    TR_times = np.arange(1, 1977)\n",
    "    interp_func = interp1d(window_xvals, recall_traj, axis=0, fill_value='extrapolate')\n",
    "    recall_interp = interp_func(TR_times)\n",
    "    return recall_interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_recalls(recall_windows, video_windows):\n",
    "    recall_models = hyp.tools.format_data(recall_windows, \n",
    "                                          vectorizer=vectorizer, \n",
    "                                          semantic=semantic, \n",
    "                                          corpus=video_windows)\n",
    "    # need to interpolate recall trajectores to video trajectory \n",
    "    # length in order to correlate structure by timepoint\n",
    "    recalls_interp = [interpolate_recall(r) for r in recall_models]\n",
    "    return recalls_interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correlate_structures(traj1, traj2):\n",
    "    return np.corrcoef(np.corrcoef(traj1).ravel(), np.corrcoef(traj2).ravel())[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-25T16:08:46.908326Z",
     "start_time": "2019-08-25T16:08:46.671869Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "video_text = pd.read_excel(opj(rawdir, 'Sherlock_Segments_1000_NN_2017.xlsx'))\n",
    "video_text['Scene Segments'].fillna(method='ffill', inplace=True)\n",
    "video_text.drop(index=[480, 481], inplace=True)\n",
    "video_text.reset_index(drop=True, inplace=True)\n",
    "video_text.loc[480:, 'Start Time (s) ': 'End Time (s) '] += video_text.loc[479, 'End Time (s) ']\n",
    "keep_cols = np.append(video_text.columns[1:5], video_text.columns[6:15])\n",
    "video_text = video_text.loc[:, keep_cols]\n",
    "video_text.columns = list(video_text.columns[:4]) + ['Narrative details', 'Indoor vs outdoor', \n",
    "                                                      'Characters on screen', 'Character in focus', \n",
    "                                                      'Character speaking', 'Location', 'Camera angle', \n",
    "                                                      'Music presence', 'Text on screen']\n",
    "\n",
    "# trajectories created from all features\n",
    "full_trajs = np.load(opj(datadir, 'models_t100_v50_r10.npy'), allow_pickle=True)\n",
    "full_video, full_recalls = full_trajs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recall_w = []\n",
    "for sub in range(1, 18):\n",
    "    transcript_path = opj(rawdir, f'NN{sub} transcript.txt')\n",
    "    with open(transcript_path, 'r', encoding='cp1252') as f:\n",
    "        recall = f.read().replace(b'\\x92'.decode('cp1252'), \"'\").strip()\n",
    "    recall_fmt = format_text(recall).split('.')[:-1]\n",
    "    sub_recall_w = parse_windows(recall_fmt, recall_wsize)[0]\n",
    "    recall_w.append(sub_recall_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteratively hold out one feature and transform remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-25T16:12:20.469876Z",
     "start_time": "2019-08-25T16:08:47.110997Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Narrative details:\n",
      "\tsimilarity to full video: 0.8443054639939905\n",
      "\tvideo-recall structure similarity: 0.48188776935201066, SEM: 0.016918737522603162\n",
      "\n",
      "Indoor vs outdoor:\n",
      "\tsimilarity to full video: 0.8971390164942661\n",
      "\tvideo-recall structure similarity: 0.6474647676550814, SEM: 0.008310889260031845\n",
      "\n",
      "Characters on screen:\n",
      "\tsimilarity to full video: 0.8547134294050613\n",
      "\tvideo-recall structure similarity: 0.6009995043539669, SEM: 0.010974366775287523\n",
      "\n",
      "Character in focus:\n",
      "\tsimilarity to full video: 0.9047312908200504\n",
      "\tvideo-recall structure similarity: 0.6365909022819201, SEM: 0.008586433972141292\n",
      "\n",
      "Character speaking:\n",
      "\tsimilarity to full video: 0.9030557339281645\n",
      "\tvideo-recall structure similarity: 0.665620723708486, SEM: 0.0066340754516595905\n",
      "\n",
      "Location:\n",
      "\tsimilarity to full video: 0.7831423838622706\n",
      "\tvideo-recall structure similarity: 0.534950476962315, SEM: 0.013052940653500288\n",
      "\n",
      "Camera angle:\n",
      "\tsimilarity to full video: 0.875478071000944\n",
      "\tvideo-recall structure similarity: 0.5816011491642284, SEM: 0.014146867462315029\n",
      "\n",
      "Music presence:\n",
      "\tsimilarity to full video: 0.8969804494849503\n",
      "\tvideo-recall structure similarity: 0.6326488935226428, SEM: 0.007915271705822438\n",
      "\n",
      "Text on screen:\n",
      "\tsimilarity to full video: 0.9002853921795219\n",
      "\tvideo-recall structure similarity: 0.5985845514984897, SEM: 0.009693138787051877\n",
      "\n",
      "All features\n",
      "\tvideo-recall structure similarity: 0.653948055596789, SEM: 0.007957121581555059\n"
     ]
    }
   ],
   "source": [
    "features = video_text.columns[4:]\n",
    "# dropfeat_corrs = dict.fromkeys(features)\n",
    "analyses = ['full vid corr', 'vid rec corr', 'vid rec sem']\n",
    "dropfeat_corrs = pd.DataFrame(index=features, columns=analyses)\n",
    "\n",
    "for feature in features:\n",
    "    print(f'{feature}:')\n",
    "    # transform remaining annotations\n",
    "    partial_df = video_text.drop(feature, axis=1)\n",
    "    other_features = partial_df.loc[:, partial_df.columns[4:]]\n",
    "    dropfeat_traj, dropfeat_windows = transform_video(other_features)\n",
    "    \n",
    "    # compute similarity with full-feature video trajectory structure\n",
    "    full_video_corr = correlate_structures(dropfeat_traj, full_video)\n",
    "    \n",
    "    # transform recalls using feature-removed corpus\n",
    "    recall_trajs = transform_recalls(recall_w, dropfeat_windows)\n",
    "    \n",
    "    # compare structures to partial video model\n",
    "    rec_corrs = np.array([correlate_structures(r, dropfeat_traj) \n",
    "                          for r in recall_trajs])\n",
    "    feat_corr, feat_sem = rec_corrs.mean(), sem(rec_corrs)\n",
    "\n",
    "    dropfeat_corrs.loc[feature] = [full_video_corr, feat_corr, feat_sem]\n",
    "    print(f'\\tsimilarity to full video: {full_video_corr}')\n",
    "    print(f'\\tvideo-recall structure similarity: {feat_corr}, SEM: {feat_sem}\\n')\n",
    "    \n",
    "# add data for full model\n",
    "rec_corr_full = np.array([correlate_structures(interpolate_recall(r), full_video) \n",
    "                          for r in full_recalls])\n",
    "dropfeat_corrs.loc['Full model'] = [1, rec_corr_full.mean(), sem(rec_corr_full)]\n",
    "print('All features')\n",
    "print(f'\\tvideo-recall structure similarity: {rec_corr_full.mean()}, SEM: {sem(rec_corr_full)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropfeat_corrs.to_pickle(opj(datadir, 'feature_contribution.p'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
