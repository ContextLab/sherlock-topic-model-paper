{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T01:19:35.130115Z",
     "start_time": "2019-08-24T01:19:32.830826Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hypertools as hyp\n",
    "from os.path import join as opj\n",
    "from scipy.stats import pearsonr, sem\n",
    "from scipy.interpolate import interp1d\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set paths & parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T01:19:35.134487Z",
     "start_time": "2019-08-24T01:19:35.131604Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rawdir = '../../../data/raw/'\n",
    "datadir = '../../../data/processed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_topics = 100\n",
    "video_wsize = 50\n",
    "\n",
    "vectorizer = {\n",
    "    'model' : 'CountVectorizer', \n",
    "    'params' : {\n",
    "        'stop_words' : 'english'\n",
    "    }\n",
    "}\n",
    "\n",
    "semantic = {\n",
    "    'model' : 'LatentDirichletAllocation', \n",
    "    'params' : {\n",
    "        'n_components' : n_topics,\n",
    "        'learning_method' : 'batch',\n",
    "        'random_state' : 0\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_text(text):\n",
    "    if isinstance(text, pd.Series):\n",
    "        text = ' '.join(list(text.fillna('')))\n",
    "        \n",
    "    no_possessive = text.lower().replace(\"'s\", '')\n",
    "    punc_stripped = re.sub(\"[^\\w\\s]+\", '', no_possessive)\n",
    "    spaced = ' '.join(punc_stripped.split())\n",
    "    return spaced\n",
    "\n",
    "def parse_windows(textlist, wsize):\n",
    "    windows = []\n",
    "    w_lengths = []\n",
    "    for ix in range(1, wsize):\n",
    "        start, end = 0, ix\n",
    "        w_lengths.append((start, end))\n",
    "        windows.append(' '.join(textlist[start : end]))\n",
    "\n",
    "    for ix in range(len(textlist)):\n",
    "        start = ix\n",
    "        end = ix + wsize if ix + wsize <= len(textlist) else len(textlist)\n",
    "        w_lengths.append((start, end))\n",
    "        windows.append(' '.join(textlist[start : end]))\n",
    "\n",
    "    return windows, w_lengths\n",
    "\n",
    "\n",
    "def get_video_timepoints(window_spans):\n",
    "    timepoints = []\n",
    "    for first, last in window_spans:\n",
    "        window_onset = video_text.loc[first, 'Start Time (s) ']\n",
    "        window_offset = video_text.loc[last - 1, 'End Time (s) ']\n",
    "        timepoints.append((window_onset + window_offset) / 2)\n",
    "        \n",
    "    return np.array(timepoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_single_feature(feature, annotations):\n",
    "    feat_list = annotations[feature].fillna('').apply(format_text).tolist()\n",
    "    feature_windows, feature_bounds = parse_windows(feat_list, video_wsize)\n",
    "    feature_model = hyp.tools.format_data(feature_windows, \n",
    "                                          vectorizer=vectorizer, \n",
    "                                          semantic=semantic,\n",
    "                                          corpus=corpus)[0]\n",
    "    \n",
    "    tr_spans = video_text[['Start Time (TRs, 1.5s)', 'End Time (TRs, 1.5s)']]\n",
    "    starts, stops = tr_spans.values.T\n",
    "    feature_model_TRs = np.empty((1976, 100))\n",
    "    xvals = get_video_timepoints(feature_bounds)\n",
    "    xvals_TR = np.array(xvals) * 1976 / 2963\n",
    "    TR_times = np.arange(1, 1977)\n",
    "    interp_func = interp1d(xvals_TR, feature_model, axis=0, fill_value='extrapolate')\n",
    "    feature_model_TRs = interp_func(TR_times)\n",
    "    return feature_model_TRs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_text = pd.read_excel(opj(rawdir, 'Sherlock_Segments_1000_NN_2017.xlsx'))\n",
    "video_text['Scene Segments'].fillna(method='ffill', inplace=True)\n",
    "video_text.drop(index=[480, 481], inplace=True)\n",
    "video_text.reset_index(drop=True, inplace=True)\n",
    "video_text.loc[480:, 'Start Time (s) ': 'End Time (s) '] += video_text.loc[479, 'End Time (s) ']\n",
    "keep_cols = np.append(video_text.columns[1:5], video_text.columns[6:15])\n",
    "video_text = video_text.loc[:, keep_cols]\n",
    "features = ['Narrative details', 'Indoor vs outdoor', 'Characters on screen', \n",
    "            'Character in focus', 'Character speaking', 'location', \n",
    "            'Camera angle', 'Music presence', 'Text on screen']\n",
    "video_text.columns = list(video_text.columns[:4]) + features\n",
    "\n",
    "# trajectories created from all features\n",
    "full_traj = np.load(opj(datadir, 'models_t100_v50_r10.npy'), allow_pickle=True)[0]\n",
    "\n",
    "# create corpus from all features for fitting each individual feature topic model\n",
    "features_df = video_text.loc[:, 'Narrative details':]\n",
    "corpus = parse_windows(features_df.apply(format_text, axis=1).tolist(), video_wsize)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform each individual feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T01:23:41.122907Z",
     "start_time": "2019-08-24T01:19:35.902587Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming Narrative details...\n",
      "transforming Indoor vs outdoor...\n",
      "transforming Characters on screen...\n",
      "transforming Character in focus...\n",
      "transforming Character speaking...\n",
      "transforming location...\n",
      "transforming Camera angle...\n",
      "transforming Music presence...\n",
      "transforming Text on screen...\n"
     ]
    }
   ],
   "source": [
    "feature_models = {}\n",
    "\n",
    "# iteratively isolate one feature from the descriptions\n",
    "for feature in features:\n",
    "    print(f'transforming {feature}...')\n",
    "    feature_traj = transform_single_feature(feature, features_df)\n",
    "    # compute feature trajectory's temporal autocorrelation matrix\n",
    "    feature_structure = np.corrcoef(feature_traj)\n",
    "    feature_models[feature] = feature_structure\n",
    "    \n",
    "# add the intact trajectory's structure\n",
    "feature_models['All features'] = np.corrcoef(full_traj)\n",
    "\n",
    "# compute correlation matrix of feature models' structures\n",
    "feature_corrs = pd.DataFrame({feat : cm.ravel() for feat, cm in feature_models.items()}).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_corrs.to_pickle(opj(datadir, 'feature_similarity.p'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
