{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook fits a topic model to the Sherlock text descriptions and then transformed the recall transcripts with the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hypertools as hyp\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.signal import resample\n",
    "from scipy import ndimage\n",
    "from scipy.stats import zscore\n",
    "from skimage.transform import resize\n",
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import correlation\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def _z2r(z):\n",
    "    \"\"\"\n",
    "    Function that calculates the inverse Fisher z-transformation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    z : int or ndarray\n",
    "        Fishers z transformed correlation value\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    result : int or ndarray\n",
    "        Correlation value\n",
    "\n",
    "    \"\"\"\n",
    "    with np.errstate(invalid='ignore', divide='ignore'):\n",
    "        return (np.exp(2 * z) - 1) / (np.exp(2 * z) + 1)\n",
    "\n",
    "\n",
    "def _r2z(r):\n",
    "    \"\"\"\n",
    "    Function that calculates the Fisher z-transformation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    r : int or ndarray\n",
    "        Correlation value\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    result : int or ndarray\n",
    "        Fishers z transformed correlation value\n",
    "\n",
    "    \"\"\"\n",
    "    with np.errstate(invalid='ignore', divide='ignore'):\n",
    "        return 0.5 * (np.log(1 + r) - np.log(1 - r))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdir = '../../data/raw/' \n",
    "datadir = '../../data/processed/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_wsize = 50\n",
    "n_topics = 100\n",
    "recall_wsize = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and forward fill the segment labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "video_text = pd.read_excel(rawdir+'Sherlock_Segments_1000_NN_2017.xlsx')\n",
    "video_text['Scene Segments'].fillna(method='ffill', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit topic model to manually-annotated movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a list of text samples from the scene descriptions / details to train the topic model\n",
    "video = video_text.loc[:,'Scene Details - A Level ':'Words on Screen '].apply(lambda x: ', '.join(x.fillna('')), axis=1).values.tolist()\n",
    "\n",
    "# create a list of overlapping text samples\n",
    "video_w = []\n",
    "for idx, sentence in enumerate(video):\n",
    "    video_w.append(','.join(video[idx:idx+video_wsize]))\n",
    "\n",
    "# vectorizer parameters\n",
    "vectorizer = {\n",
    "    'model' : 'CountVectorizer', \n",
    "    'params' : {\n",
    "        'stop_words' : 'english'\n",
    "    }\n",
    "}\n",
    "\n",
    "# topic model parameters\n",
    "semantic = {\n",
    "    'model' : 'LatentDirichletAllocation', \n",
    "    'params' : {\n",
    "        'n_components' : n_topics,\n",
    "        'learning_method' : 'batch',\n",
    "        'random_state' : 0,\n",
    "    }\n",
    "}\n",
    "\n",
    "# create video model with hypertools\n",
    "video_model = hyp.tools.format_data(video_w, vectorizer=vectorizer, semantic=semantic, corpus=video_w)[0]\n",
    "\n",
    "# description are by scene, not TR so stretch the model to be in TRs\n",
    "ranges =[[d['Start Time (TRs, 1.5s)'],d['End Time (TRs, 1.5s)']] for i, d in video_text.iterrows()] \n",
    "expanded = []\n",
    "for i in range(1976):\n",
    "    try:\n",
    "        idx = np.where([i>=r[0] and i<=r[1] for r in ranges])[0][0]\n",
    "        expanded.append(video_model[idx, :])\n",
    "    except:\n",
    "        expanded.append(video_model[0, :])\n",
    "video_model = np.array(expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loop over subjects\n",
    "recall_w = []\n",
    "for sub in range(1, 18):\n",
    "    \n",
    "    # load subject data\n",
    "    recall = pd.read_csv(rawdir+'NN'+str(sub)+' transcript.txt', header=None, sep='.', error_bad_lines=False, encoding='latin-1').values.tolist()[0][:-1]\n",
    "    \n",
    "    rs = []  \n",
    "    # loop over sentences\n",
    "    for sentence in recall:\n",
    "        try:\n",
    "            s = sentence.encode('utf-8').strip()\n",
    "            rs.append(sentence)\n",
    "        except:\n",
    "            pass # skips over nans\n",
    "    \n",
    "    # create overlapping windows of n sentences\n",
    "    sub_recall_w = []\n",
    "    for idx, sentence in enumerate(rs):\n",
    "        sub_recall_w.append(','.join(rs[idx:idx+recall_wsize]))\n",
    "        \n",
    "    recall_w.append(sub_recall_w)\n",
    "    \n",
    "# create recall models\n",
    "recall_models = hyp.tools.format_data(recall_w, vectorizer=vectorizer, semantic=semantic, corpus=video_w)\n",
    "\n",
    "# resample the models\n",
    "recall_models_rs = list(map(lambda x: resample(x, 1976), recall_models))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save video and recall models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(datadir+'models_t%s_v%s_r%s' % (str(n_topics), str(video_wsize), str(recall_wsize)), [video_model, recall_models])\n",
    "np.save(datadir+'models_t%s_v%s_r%s_resampled' % (str(n_topics), str(video_wsize), str(recall_wsize)), [video_model, recall_models_rs])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
