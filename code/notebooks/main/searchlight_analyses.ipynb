{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook loads the output of the searchlight analyses run using the scripts in [`code/scripts/searchlights`](https://github.com/ContextLab/sherlock-topic-model-paper/tree/master/code/scripts/searchlights). The fMRI data used in the searchlight analyses can be downloaded using the script at [`code/scripts/download_neural_data.sh`](https://github.com/ContextLab/sherlock-topic-model-paper/blob/master/code/scripts/download_neural_data.sh)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Helper functions and variables used across multiple notebooks can be found in `/mnt/code/sherlock_helpers/sherlock_helpers`, or on GitHub, [here](https://github.com/ContextLab/sherlock-topic-model-paper/tree/master/code/sherlock_helpers).<br />You can also view source code directly from the notebook with:<br /><pre>    from sherlock_helpers.functions import show_source<br />    show_source(foo)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from warnings import filterwarnings\n",
    "\n",
    "import numpy as np\n",
    "from nilearn import datasets, surface\n",
    "from nilearn.image import concat_imgs, load_img, new_img_like\n",
    "from scipy.stats import ttest_1samp as ttest\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from sherlock_helpers.constants import DATA_DIR\n",
    "from sherlock_helpers.functions import r2z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sl_vdir = DATA_DIR.joinpath('searchlight_video')\n",
    "sl_rdir = DATA_DIR.joinpath('searchlight_recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter some harmless warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# masked voxels in brain data are filled with nans, causes RuntimeWarnings\n",
    "filterwarnings('ignore', category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ac91eefa4e548a5b32c14aaf7b0ce64"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "video_perms = []\n",
    "recall_perms = []\n",
    "for perm in trange(100, leave=False):\n",
    "    video_perm = load_img(str(sl_vdir.joinpath(f'perm{perm}.nii.gz')))\n",
    "    recall_perm = load_img(str(sl_rdir.joinpath(f'perm{perm}.nii.gz')))\n",
    "    video_perms.append(video_perm)\n",
    "    recall_perms.append(recall_perm)\n",
    "    \n",
    "video_perms = concat_imgs(video_perms).dataobj.astype(np.float64)\n",
    "recall_perms = concat_imgs(recall_perms).dataobj.astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea82f4f06e9408195c403bed2f1b836"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ref_img = load_img(str(sl_vdir.joinpath('ref_img.nii.gz')))\n",
    "                   \n",
    "vid_imgs = []\n",
    "rec_imgs = []\n",
    "for sub in trange(1, 18, leave=False):\n",
    "    sub_vdata = np.load(sl_vdir.joinpath(f'sub{sub}.npy'), allow_pickle=True)\n",
    "    sub_rdata = np.load(sl_rdir.joinpath(f'sub{sub}.npy'), allow_pickle=True)\n",
    "    vid_img = new_img_like(ref_img, sub_vdata.astype(np.float64))\n",
    "    rec_img = new_img_like(ref_img, sub_rdata.astype(np.float64))\n",
    "    vid_imgs.append(vid_img)\n",
    "    rec_imgs.append(rec_img)\n",
    "    \n",
    "vid_imgs = concat_imgs(vid_imgs)\n",
    "rec_imgs = concat_imgs(rec_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get stats for real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "video_data = vid_imgs.dataobj.astype(np.float64)\n",
    "video_statmap = ttest(np.moveaxis(r2z(video_data), -1, 0), 0).statistic\n",
    "video_img = new_img_like(ref_img, video_statmap.astype(np.float64))\n",
    "\n",
    "\n",
    "recall_data = rec_imgs.dataobj.astype(np.float64)\n",
    "recall_statmap = ttest(np.moveaxis(r2z(recall_data), -1, 0), 0).statistic\n",
    "recall_img = new_img_like(ref_img, recall_statmap.astype(np.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do permutation correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "real_video = video_img.dataobj.astype(np.float64)\n",
    "real_recall = recall_img.dataobj.astype(np.float64)\n",
    "\n",
    "zval_video = (real_video - np.nanmean(video_perms, axis=3)) / np.nanstd(video_perms, axis=3)\n",
    "pval_video = (real_video[:, :, :, np.newaxis] < video_perms).sum(axis=3) / 100\n",
    "zval_recall = (real_recall - np.nanmean(recall_perms, axis=3)) / np.nanstd(recall_perms, axis=3)\n",
    "pval_recall = (real_recall[:, :, :, np.newaxis] < recall_perms).sum(axis=3) / 100\n",
    "\n",
    "zval_video = np.nan_to_num(zval_video)\n",
    "pval_video = np.nan_to_num(pval_video)\n",
    "zval_recall = np.nan_to_num(zval_recall)\n",
    "pval_recall = np.nan_to_num(pval_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export unthresholded z-score maps for neurosynth decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zmap_video = new_img_like(ref_img, zval_video.astype(np.float64))\n",
    "zmap_recall = new_img_like(ref_img, zval_recall.astype(np.float64))\n",
    "\n",
    "# zmap_video.to_filename(str(sl_vdir.joinpath('zmap_video.nii.gz')))\n",
    "# zmap_recall.to_filename(str(sl_rdir.joinpath('zmap_recall.nii.gz')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zval_video[pval_video > .05] = 0\n",
    "zval_video[zval_video < 0] = 0\n",
    "\n",
    "zval_recall[pval_recall > .05] = 0\n",
    "zval_recall[zval_recall < 0] = 0\n",
    "\n",
    "zmap_video = new_img_like(ref_img, zval_video.astype(np.float64))\n",
    "zmap_recall = new_img_like(ref_img, zval_recall.astype(np.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to surface maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fsaverage = datasets.fetch_surf_fsaverage(mesh='fsaverage5')\n",
    "vid_texture_pl = surface.vol_to_surf(zmap_video, fsaverage.pial_left)\n",
    "vid_texture_pr = surface.vol_to_surf(zmap_video, fsaverage.pial_right)\n",
    "rec_texture_pl = surface.vol_to_surf(zmap_recall, fsaverage.pial_left)\n",
    "rec_texture_pr = surface.vol_to_surf(zmap_recall, fsaverage.pial_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.save(sl_vdir.joinpath('video_surface_left.npy'), vid_texture_pl)\n",
    "# np.save(sl_vdir.joinpath('video_surface_right.npy'), vid_texture_pr)\n",
    "# np.save(sl_rdir.joinpath('recall_surface_left.npy'), rec_texture_pl)\n",
    "# np.save(sl_rdir.joinpath('recall_surface_right.npy'), rec_texture_pr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
