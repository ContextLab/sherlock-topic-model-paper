{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook fits a topic model to the Sherlock text descriptions and then transformed the recall transcripts with the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T17:51:01.902342Z",
     "start_time": "2019-12-01T17:50:57.154117Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hypertools as hyp\n",
    "from os.path import abspath, join as opj\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import analysis helpers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Functions and variables used across multiple notebooks can be found [here](https://github.com/contextlab/sherlock-topic-model-paper/blob/master/code/helpers/analysis_helpers.py)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sys.path.insert(0, abspath('../../helpers/'))\n",
    "from analysis_helpers import (\n",
    "    N_TOPICS,\n",
    "    VIDEO_WSIZE,\n",
    "    RECALL_WSIZE,\n",
    "    VECTORIZER_PARAMS,\n",
    "    SEMANTIC_PARAMS,\n",
    "    format_text,\n",
    "    parse_windows,\n",
    "    get_video_timepoints\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of topics: 100\n",
      "Video window size: 50\n",
      "Video window size: 10\n",
      "Vectorizer params: {'model': 'CountVectorizer', 'params': {'stop_words': 'english'}}\n",
      "LDA params: {'model': 'LatentDirichletAllocation', 'params': {'n_components': 100, 'learning_method': 'batch', 'random_state': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Show parameters\n",
    "print(f'Number of topics: {N_TOPICS}')\n",
    "print(f'Video window size: {VIDEO_WSIZE}')\n",
    "print(f'Video window size: {RECALL_WSIZE}')\n",
    "print(f'Vectorizer params: {VECTORIZER_PARAMS}')\n",
    "print(f'LDA params: {SEMANTIC_PARAMS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T17:51:02.906541Z",
     "start_time": "2019-12-01T17:51:02.898319Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rawdir = '../../../data/raw/' \n",
    "datadir = '../../../data/processed/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T17:51:36.799229Z",
     "start_time": "2019-12-01T17:51:36.536798Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "video_text = pd.read_excel(opj(rawdir, 'Sherlock_Segments_1000_NN_2017.xlsx'))\n",
    "video_text['Scene Segments'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "# drop 1s shot & 6s of black screen after end of 1st scan\n",
    "video_text.drop(index=[480, 481], inplace=True)\n",
    "video_text.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# timestamps for 2nd scan restart from 0; add duration of 1st scan to values\n",
    "video_text.loc[480:, 'Start Time (s) ': 'End Time (s) '] += video_text.loc[479, 'End Time (s) ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit topic model to manually-annotated movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T22:57:52.952525Z",
     "start_time": "2019-11-20T22:57:29.462076Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a list of text samples from the scene descriptions / details to train the topic model\n",
    "video = video_text.loc[:,'Scene Details - A Level ':'Words on Screen '].apply(format_text, axis=1).tolist()\n",
    "video_windows, window_bounds = parse_windows(video, VIDEO_WSIZE)\n",
    "\n",
    "# create video model with hypertools\n",
    "video_model = hyp.tools.format_data(video_windows, \n",
    "                                    vectorizer=VECTORIZER_PARAMS, \n",
    "                                    semantic=SEMANTIC_PARAMS, \n",
    "                                    corpus=video_windows)[0]\n",
    "\n",
    "# description are by scene, not TR so stretch the model to be in TRs\n",
    "tr_spans = video_text[['Start Time (TRs, 1.5s)', 'End Time (TRs, 1.5s)']]\n",
    "starts, stops = tr_spans.values.T\n",
    "video_model_TRs = np.empty((1976, 100))\n",
    "\n",
    "xvals = get_video_timepoints(window_bounds, video_text)\n",
    "xvals_TR = xvals * 1976 / 2963\n",
    "TR_times = np.arange(1, 1977)\n",
    "interp_func = interp1d(xvals_TR, video_model, axis=0, fill_value='extrapolate')\n",
    "video_model_TRs = interp_func(TR_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T23:09:47.284433Z",
     "start_time": "2019-11-20T23:09:47.225670Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loop over subjects\n",
    "recall_w = []\n",
    "for sub in range(1, 18):\n",
    "    # load subject data\n",
    "    transcript_path = opj(rawdir, f'NN{sub} transcript.txt')\n",
    "    with open(transcript_path, 'r', encoding='cp1252') as f:\n",
    "        recall = f.read().replace(b'\\x92'.decode('cp1252'), \"'\").strip()\n",
    "\n",
    "    # create overlapping windows of n sentences\n",
    "    recall_fmt = format_text(recall).split('.')\n",
    "    if not recall_fmt[-1]:\n",
    "        recall_fmt = recall_fmt[:-1]\n",
    "    sub_recall_w = parse_windows(recall_fmt, RECALL_WSIZE)[0]\n",
    "    recall_w.append(sub_recall_w)\n",
    "    \n",
    "    # save example participant's recall windows \n",
    "    if sub == 17:\n",
    "        np.save(opj(datadir, 'recall_text.npy'), sub_recall_w)\n",
    "    \n",
    "# create recall models\n",
    "recall_models = hyp.tools.format_data(recall_w, \n",
    "                                      vectorizer=VECTORIZER_PARAMS, \n",
    "                                      semantic=SEMANTIC_PARAMS, \n",
    "                                      corpus=video_windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save video model, recall models, and text corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T19:13:28.479854Z",
     "start_time": "2019-11-18T19:13:28.244526Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(opj(datadir, f'models_t{N_TOPICS}_v{VIDEO_WSIZE}_r{RECALL_WSIZE}'), [video_model_TRs, recall_models])\n",
    "np.save(opj(datadir, 'video_text.npy'), video_windows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
