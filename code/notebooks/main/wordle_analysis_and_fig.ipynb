{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-25T18:29:51.029932Z",
     "start_time": "2019-08-25T18:29:49.503297Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from os.path import abspath, join as opj\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from wordcloud import WordCloud, get_single_color_func\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "import seaborn as sns\n",
    "\n",
    "cmap = plt.cm.Spectral\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import analysis helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.insert(0, abspath('../../helpers/'))\n",
    "from analysis_helpers import (\n",
    "    N_TOPICS,\n",
    "    SCALE,\n",
    "    r2z,\n",
    "    z2r,\n",
    "    corr_mean,\n",
    "    add_arrows\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some custom functions/classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-25T18:29:51.323602Z",
     "start_time": "2019-08-25T18:29:51.031243Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topn(d, n):\n",
    "    c = collections.Counter(d)\n",
    "    return {k:v for k, v in c.most_common(n)}\n",
    "\n",
    "\n",
    "def get_normalized_model(m, tm):\n",
    "    m = np.dot(m, tm.components_)\n",
    "    m-=m.mean(0)\n",
    "    m-=np.min(m)\n",
    "    m/=np.max(m)\n",
    "    return m\n",
    "\n",
    "\n",
    "class SimpleGroupedColorFunc(object):\n",
    "    \"\"\"Create a color function object which assigns EXACT colors\n",
    "       to certain words based on the color to words mapping\n",
    "       Parameters\n",
    "       ----------\n",
    "       color_to_words : dict(str -> list(str))\n",
    "         A dictionary that maps a color to the list of words.\n",
    "       default_color : str\n",
    "         Color that will be assigned to a word that's not a member\n",
    "         of any value from color_to_words.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, color_to_words, default_color):\n",
    "        self.word_to_color = {word: color\n",
    "                              for (color, words) in color_to_words.items()\n",
    "                              for word in words}\n",
    "\n",
    "        self.default_color = default_color\n",
    "\n",
    "    def __call__(self, word, **kwargs):\n",
    "        return self.word_to_color.get(word, self.default_color)\n",
    "    \n",
    "    \n",
    "def plot_wordle(ax, textdict, maskpath=None):\n",
    "    circle = np.array(Image.open(maskpath))\n",
    "    wc = WordCloud(max_font_size=50, collocations=False, max_words=200, background_color=\"white\", mask=circle, width=2000, height=1000, colormap=plt.cm.Reds)\n",
    "    wc.generate_from_frequencies(textdict)\n",
    "    ax.imshow(wc.recolor(color_func=grouped_color_func, random_state=3),\n",
    "           interpolation=\"bilinear\")\n",
    "    ax.axis(\"off\")\n",
    "    \n",
    "    \n",
    "def plot_image(x, y, image, ax=None, zoom=1):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    try:\n",
    "        image = plt.imread(image)\n",
    "    except TypeError:\n",
    "        pass\n",
    "    im = OffsetImage(image, zoom=zoom)\n",
    "    x, y = np.atleast_1d(x, y)\n",
    "    im.image.axes=ax\n",
    "    artists = []\n",
    "    ab = AnnotationBbox(im, (x, y), xycoords='data', frameon=False)\n",
    "    artists.append(ax.add_artist(ab))\n",
    "    return artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-25T18:29:51.389920Z",
     "start_time": "2019-08-25T18:29:51.325171Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bootstrap_ci_corrs(M, ci=95, n_boots=1000, color='#1f77b4', alpha=0.2, label=None):\n",
    "    evs = np.arange(M.shape[0])\n",
    "    y = corr_mean(M, axis=1)\n",
    "    ci_low = (100 - ci) / 2\n",
    "    ci_high = 100 - ci_low\n",
    "    L, U = np.empty(evs.shape), np.empty(evs.shape)\n",
    "    # constructs a single resample\n",
    "    boot_mean = lambda x: np.nanmean(np.random.choice(x, size=len(x), replace=True))\n",
    "    \n",
    "    for ev in evs:\n",
    "        zev_dists = r2z(M[ev])\n",
    "        boot_iter = (boot_mean(zev_dists) for n in range(n_boots))\n",
    "        zboots = np.fromiter(boot_iter, dtype=float)\n",
    "        # use percentile bootstrap (seaborn default method)\n",
    "        L[ev], U[ev] = z2r(np.percentile(zboots, ci_low)), z2r(np.percentile(zboots, ci_high))\n",
    "    \n",
    "    # error ribbons\n",
    "    h1 = plt.fill_between(evs, L, U, color=color, alpha=alpha)\n",
    "    # opaque line\n",
    "    h2 = plt.plot(evs, y, color=color, label=label)\n",
    "    return h1, h2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set path and params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-25T18:29:51.394785Z",
     "start_time": "2019-08-25T18:29:51.391653Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datadir = '../../../data/processed/'\n",
    "figdir = '../../../paper/figs/'\n",
    "tmp_dir = opj(figdir, 'tmp')\n",
    "# os.mkdir(tmp_dir)\n",
    "\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "default_color = 'grey'\n",
    "n = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-25T18:29:51.546052Z",
     "start_time": "2019-08-25T18:29:51.396146Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "video_events = np.load(opj(datadir, 'video_events.npy'))\n",
    "recall_events = np.load(opj(datadir, 'recall_events.npy'), allow_pickle=True)\n",
    "avg_recall_events = np.load(opj(datadir, 'avg_recall_events.npy'), allow_pickle=True)\n",
    "matches = np.load(opj(datadir, 'labels.npy'), allow_pickle=True)\n",
    "text_corpus = np.load(opj(datadir, 'video_text.npy'), allow_pickle=True)\n",
    "embeddings = np.load(opj(datadir, 'embeddings.npy'), allow_pickle=True)\n",
    "video_embedding = embeddings[0]\n",
    "recall_embeddings = embeddings[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dists = []\n",
    "for v in video_events:\n",
    "    dist = []\n",
    "    for sub in recall_events:\n",
    "        dist.append(np.max(1 - cdist(np.atleast_2d(v), sub, 'correlation')))\n",
    "    dists.append(dist)\n",
    "dists = np.array(dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-25T18:29:53.008067Z",
     "start_time": "2019-08-25T18:29:51.598604Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "bootstrap_ci_corrs(dists)\n",
    "plt.xlim(0,29)\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel('Event number')\n",
    "plt.ylabel('Average correlation')\n",
    "plt.tight_layout()\n",
    "# plt.savefig(opj(tmp_dir, 'precision.pdf'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit topic model to video annotation sliding windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "fit_cv = cv.fit_transform(text_corpus)\n",
    "tm = LatentDirichletAllocation(n_components=N_TOPICS, learning_method='batch', random_state=0).fit(fit_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(opj(datadir, 'count_vectorizer_model'), cv)\n",
    "np.save(opj(datadir, 'topic_model'), tm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-25T18:29:53.013863Z",
     "start_time": "2019-08-25T18:29:53.009418Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm_video_events = get_normalized_model(video_events, tm)\n",
    "norm_avg_recall_events = get_normalized_model(avg_recall_events, tm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordle figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-25T18:30:03.759581Z",
     "start_time": "2019-08-25T18:29:53.015000Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for seg in range(video_events.shape[0]):\n",
    "    weights = norm_video_events[seg, :]\n",
    "    textdict_video = topn({word: weight for word, weight in zip(cv.get_feature_names(), weights)}, n)\n",
    "    weights = norm_avg_recall_events[seg,:]\n",
    "    textdict_recall = topn({word: weight for word, weight in zip(cv.get_feature_names(), weights)}, n)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    color_to_words = {'black': list(set(textdict_video))}\n",
    "    grouped_color_func = SimpleGroupedColorFunc(color_to_words, default_color)\n",
    "    plot_wordle(ax1, textdict_video, maskpath=opj(datadir, \"half-moon-left.jpg\"))\n",
    "    color_to_words = {'black': list(set(textdict_recall))}\n",
    "    grouped_color_func = SimpleGroupedColorFunc(color_to_words, default_color)\n",
    "    plot_wordle(ax2, textdict_recall, maskpath=opj(datadir, \"half-moon.jpg\"))\n",
    "    plt.subplots_adjust(wspace=-.5, hspace=-.5)\n",
    "    fig.patch.set_visible(False)\n",
    "#     plt.savefig(opj(tmp_dir, f'wordle_event{seg}.png'), dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted average of the event vectors by memorability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-25T18:30:03.775327Z",
     "start_time": "2019-08-25T18:30:03.761571Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mdist = corr_mean(dists, axis=1)\n",
    "rvec = np.zeros_like(video_events[0])\n",
    "fvec = np.zeros_like(video_events[0])\n",
    "rsum = 0\n",
    "fsum = 0\n",
    "for v, w in zip(video_events, mdist):\n",
    "    rvec += v * w\n",
    "    rsum += w\n",
    "    fvec += v * (1 - w)\n",
    "    fsum += (1 - w)\n",
    "r = rvec / rsum\n",
    "r = r - video_events.mean(0)\n",
    "f = fvec / fsum\n",
    "f = f - video_events.mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most memorable words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-25T18:30:04.292805Z",
     "start_time": "2019-08-25T18:30:03.776890Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rweights = np.dot(r, tm.components_)\n",
    "rdict = topn({word:weight for word, weight in zip(cv.get_feature_names(), rweights)}, 200)\n",
    "fig, ax1 = plt.subplots(1, 1)\n",
    "color_to_words = {'black': list(set(rdict))}\n",
    "grouped_color_func = SimpleGroupedColorFunc(color_to_words, default_color)\n",
    "plot_wordle(ax1, rdict, maskpath=opj(datadir, \"oval2.jpg\"))\n",
    "# plt.savefig(opj(tmp_dir, 'most_memorable.png'), dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least memorable words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-25T18:30:04.834113Z",
     "start_time": "2019-08-25T18:30:04.294279Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fweights = np.dot(f, tm.components_)\n",
    "fdict = topn({word:weight for word, weight in zip(cv.get_feature_names(), fweights)}, 200)\n",
    "fig, ax1 = plt.subplots(1, 1)\n",
    "color_to_words = {'black': list(set(fdict))}\n",
    "grouped_color_func = SimpleGroupedColorFunc(color_to_words, default_color)\n",
    "plot_wordle(ax1, fdict, maskpath=opj(datadir, \"oval2.jpg\"))\n",
    "# plt.savefig(opj(tmp_dir, 'least_memorable.png'), dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trajectory distribution figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-25T18:30:06.761368Z",
     "start_time": "2019-08-25T18:30:04.835458Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = cmap(np.linspace(0, 1, 10))\n",
    "sub_color = cmap(np.linspace(0, 1, 17))\n",
    "subj_points = np.vstack(recall_embeddings)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "for i, (sub, sub_match) in enumerate(zip(recall_embeddings, matches)):\n",
    "    for j, (p, m) in enumerate(zip(sub, sub_match)):\n",
    "        ax.plot(p[0], p[1], 'o', c=cmap(m/video_embedding.shape[0]), alpha=.75, zorder=2, markersize=7)\n",
    "        ax.plot(p[0], p[1], 'o', c='k', alpha=.5, zorder=1, markersize=8)\n",
    "hinges = video_embedding\n",
    "for i in range(len(hinges)-1):\n",
    "    ax.plot([hinges[i, 0], hinges[i+1, 0]], [hinges[i, 1], hinges[i+1, 1]], c='k', linewidth=2, alpha=1)\n",
    "for i in range(len(hinges)):\n",
    "    ax.plot(hinges[i,0], hinges[i,1], 'o', c=cmap(i/hinges.shape[0]), zorder=4, \n",
    "            markersize=mdist[i]*SCALE/3+5, alpha=.9)\n",
    "    ax.plot(hinges[i,0], hinges[i,1], 'ko', zorder=3, markersize=mdist[i]*SCALE/3+7, alpha=.9)\n",
    "add_arrows(ax, hinges[:, 0], hinges[:, 1], zorder=3, alpha=1, color='k', fill=True)\n",
    "ax.axis('off')\n",
    "# plt.savefig(opj(tmp_dir, 'trajectory_distribution.pdf'))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
