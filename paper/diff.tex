\documentclass{article}
%DIF LATEXDIFF DIFFERENCE FILE
%DIF DEL old.tex    Fri Mar 13 21:00:44 2020
%DIF ADD main.tex   Fri Mar 13 17:06:39 2020
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{geometry}
\usepackage{natbib}
\usepackage{pxfonts}
\usepackage{graphicx}
\usepackage{newfloat}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{lineno}
%DIF 13a13
\usepackage{placeins} %DIF > 
%DIF -------

\newcommand{\argmax}{\mathop{\mathrm{argmax}}\limits}

\newcommand{\topicopt}{S1}
\newcommand{\topics}{S2}
\newcommand{\featureimportance}{S3}
%DIF 19-21c20-22
%DIF < \newcommand{\listlearning}{S4}
%DIF < \newcommand{\corrmats}{S5}
%DIF < \newcommand{\matchmats}{S6}
%DIF -------
\newcommand{\corrmats}{S4} %DIF > 
\newcommand{\matchmats}{S5} %DIF > 
\newcommand{\kopt}{S6} %DIF > 
%DIF -------

\doublespacing
\linenumbers

%DIF 26d27
%DIF < 
%DIF -------
\title{\DIFdelbegin \DIFdel{How is experience transformed into memory?}\DIFdelend \DIFaddbegin \DIFadd{Memory for television episodes preserves event content while introducing new across-event similarities}\DIFaddend }
%DIF 28a28-34
% I think this title still needs work. In this version of the paper, we donâ€™t really focus on the across-event similarities. Let's revisit this once the edits are complete - AH %DIF > 
 %DIF > 
% Constructing mnemonic events from meaningfully structured experiences %DIF > 
% Transforming meaningfully structured experiences into mnemonic events %DIF > 
 %DIF > 
 %DIF > 
 %DIF > 
%DIF -------
\author{Andrew C. Heusser\DIFaddbegin \DIFadd{\textsuperscript{1, 2, \textdagger}}\DIFaddend , Paxton C. Fitzpatrick\DIFaddbegin \DIFadd{\textsuperscript{1, \textdagger}}\DIFaddend , and Jeremy R. Manning\DIFaddbegin \DIFadd{\textsuperscript{1, *}}\DIFaddend \\\DIFaddbegin \DIFadd{\textsuperscript{1}}\DIFaddend Department of Psychological and Brain Sciences\\Dartmouth College, Hanover, NH 03755, USA\\\DIFaddbegin \DIFadd{\textsuperscript{2}Akili Interactive}\\\DIFadd{Boston, MA 02110}\\\DIFadd{\textsuperscript{\textdagger}Denotes equal contribution}\\\DIFadd{\textsuperscript{*}}\DIFaddend Corresponding author: jeremy.r.manning@dartmouth.edu}

\bibliographystyle{apa}
%DIF PREAMBLE EXTENSION ADDED BY LATEXDIFF
%DIF UNDERLINE PREAMBLE %DIF PREAMBLE
\RequirePackage[normalem]{ulem} %DIF PREAMBLE
\RequirePackage{color}\definecolor{RED}{rgb}{1,0,0}\definecolor{BLUE}{rgb}{0,0,1} %DIF PREAMBLE
\providecommand{\DIFaddtex}[1]{{\protect\color{blue}\uwave{#1}}} %DIF PREAMBLE
\providecommand{\DIFdeltex}[1]{{\protect\color{red}\sout{#1}}}                      %DIF PREAMBLE
%DIF SAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddbegin}{} %DIF PREAMBLE
\providecommand{\DIFaddend}{} %DIF PREAMBLE
\providecommand{\DIFdelbegin}{} %DIF PREAMBLE
\providecommand{\DIFdelend}{} %DIF PREAMBLE
\providecommand{\DIFmodbegin}{} %DIF PREAMBLE
\providecommand{\DIFmodend}{} %DIF PREAMBLE
%DIF FLOATSAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddFL}[1]{\DIFadd{#1}} %DIF PREAMBLE
\providecommand{\DIFdelFL}[1]{\DIFdel{#1}} %DIF PREAMBLE
\providecommand{\DIFaddbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFaddendFL}{} %DIF PREAMBLE
\providecommand{\DIFdelbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFdelendFL}{} %DIF PREAMBLE
%DIF HYPERREF PREAMBLE %DIF PREAMBLE
\providecommand{\DIFadd}[1]{\texorpdfstring{\DIFaddtex{#1}}{#1}} %DIF PREAMBLE
\providecommand{\DIFdel}[1]{\texorpdfstring{\DIFdeltex{#1}}{}} %DIF PREAMBLE
\newcommand{\DIFscaledelfig}{0.5}
%DIF HIGHLIGHTGRAPHICS PREAMBLE %DIF PREAMBLE
\RequirePackage{settobox} %DIF PREAMBLE
\RequirePackage{letltxmacro} %DIF PREAMBLE
\newsavebox{\DIFdelgraphicsbox} %DIF PREAMBLE
\newlength{\DIFdelgraphicswidth} %DIF PREAMBLE
\newlength{\DIFdelgraphicsheight} %DIF PREAMBLE
% store original definition of \includegraphics %DIF PREAMBLE
\LetLtxMacro{\DIFOincludegraphics}{\includegraphics} %DIF PREAMBLE
\newcommand{\DIFaddincludegraphics}[2][]{{\color{blue}\fbox{\DIFOincludegraphics[#1]{#2}}}} %DIF PREAMBLE
\newcommand{\DIFdelincludegraphics}[2][]{% %DIF PREAMBLE
\sbox{\DIFdelgraphicsbox}{\DIFOincludegraphics[#1]{#2}}% %DIF PREAMBLE
\settoboxwidth{\DIFdelgraphicswidth}{\DIFdelgraphicsbox} %DIF PREAMBLE
\settoboxtotalheight{\DIFdelgraphicsheight}{\DIFdelgraphicsbox} %DIF PREAMBLE
\scalebox{\DIFscaledelfig}{% %DIF PREAMBLE
\parbox[b]{\DIFdelgraphicswidth}{\usebox{\DIFdelgraphicsbox}\\[-\baselineskip] \rule{\DIFdelgraphicswidth}{0em}}\llap{\resizebox{\DIFdelgraphicswidth}{\DIFdelgraphicsheight}{% %DIF PREAMBLE
\setlength{\unitlength}{\DIFdelgraphicswidth}% %DIF PREAMBLE
\begin{picture}(1,1)% %DIF PREAMBLE
\thicklines\linethickness{2pt} %DIF PREAMBLE
{\color[rgb]{1,0,0}\put(0,0){\framebox(1,1){}}}% %DIF PREAMBLE
{\color[rgb]{1,0,0}\put(0,0){\line( 1,1){1}}}% %DIF PREAMBLE
{\color[rgb]{1,0,0}\put(0,1){\line(1,-1){1}}}% %DIF PREAMBLE
\end{picture}% %DIF PREAMBLE
}\hspace*{3pt}}} %DIF PREAMBLE
} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddbegin}{\DIFaddbegin} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddend}{\DIFaddend} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelbegin}{\DIFdelbegin} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelend}{\DIFdelend} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddbegin}{\DIFOaddbegin \let\includegraphics\DIFaddincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddend}{\DIFOaddend \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelbegin}{\DIFOdelbegin \let\includegraphics\DIFdelincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelend}{\DIFOaddend \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddbeginFL}{\DIFaddbeginFL} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddendFL}{\DIFaddendFL} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelbeginFL}{\DIFdelbeginFL} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelendFL}{\DIFdelendFL} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddbeginFL}{\DIFOaddbeginFL \let\includegraphics\DIFaddincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddendFL}{\DIFOaddendFL \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelbeginFL}{\DIFOdelbeginFL \let\includegraphics\DIFdelincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelendFL}{\DIFOaddendFL \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
%DIF LISTINGS PREAMBLE %DIF PREAMBLE
\RequirePackage{listings} %DIF PREAMBLE
\RequirePackage{color} %DIF PREAMBLE
\lstdefinelanguage{DIFcode}{ %DIF PREAMBLE
%DIF DIFCODE_UNDERLINE %DIF PREAMBLE
  moredelim=[il][\color{red}\sout]{\%DIF\ <\ }, %DIF PREAMBLE
  moredelim=[il][\color{blue}\uwave]{\%DIF\ >\ } %DIF PREAMBLE
} %DIF PREAMBLE
\lstdefinestyle{DIFverbatimstyle}{ %DIF PREAMBLE
	language=DIFcode, %DIF PREAMBLE
	basicstyle=\ttfamily, %DIF PREAMBLE
	columns=fullflexible, %DIF PREAMBLE
	keepspaces=true %DIF PREAMBLE
} %DIF PREAMBLE
\lstnewenvironment{DIFverbatim}{\lstset{style=DIFverbatimstyle}}{} %DIF PREAMBLE
\lstnewenvironment{DIFverbatim*}{\lstset{style=DIFverbatimstyle,showspaces=true}}{} %DIF PREAMBLE
%DIF END PREAMBLE EXTENSION ADDED BY LATEXDIFF

\begin{document}
\maketitle

\begin{abstract}
\DIFdelbegin \DIFdel{The ways our }\DIFdelend \DIFaddbegin \DIFadd{Our }\DIFaddend experiences unfold over time \DIFdelbegin \DIFdel{define }\DIFdelend \DIFaddbegin \DIFadd{defining }\DIFaddend unique \textit{trajectories} through the relevant representational spaces.  \DIFdelbegin \DIFdel{Within this geometric framework}\DIFdelend \DIFaddbegin \DIFadd{By casting our life events as temporally evolving trajectories}\DIFaddend , one can compare the shape of the trajectory formed by an experience to that defined by our later remembering of that experience.  We propose a framework for mapping naturalistic experiences onto geometric spaces that characterize how \DIFdelbegin \DIFdel{they unfold }\DIFdelend \DIFaddbegin \DIFadd{experiences are segmented into discrete events, and how the contents of event sequences evolve }\DIFaddend over time.  We apply this approach to a naturalistic memory experiment which had participants view and recount a \DIFdelbegin \DIFdel{video.  We found that the shapes of the trajectories formed by }\DIFdelend \DIFaddbegin \DIFadd{television episode.  The content of }\DIFaddend participants' recountings \DIFdelbegin \DIFdel{were all highly similar to that of the original video, but participantsdiffered in the level of detail they remembered.  We also }\DIFdelend \DIFaddbegin \DIFadd{of events from the original episode closely matched the original episode's content. Further, we introduce two novel metrics for assessing memory quality (precision and distinctiveness), both of which relate to participants' ability to recapitulate the experience.  Lastly, we }\DIFaddend identified a network of brain structures that are sensitive to the ``shapes'' of \DIFdelbegin \DIFdel{our }\DIFdelend ongoing experiences, and an overlapping network that is sensitive \DIFdelbegin \DIFdel{to how we will later remember those experiences .
}\DIFdelend \DIFaddbegin \DIFadd{(at the time of encoding) to how people later remembered those experiences in relation to other experiences.  In this way, modeling the content of richly structured experiences can reveal how (geometrically and conceptually) those experiences are segmented into events and integrated into our memories of other experiences.
}\DIFaddend \end{abstract}


\section*{Introduction}
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend What does it mean to \textit{remember} something? In traditional episodic memory experiments \citep[e.g., list-learning or trial-based experiments;][]{Murd62a, Kaha96}, remembering is often cast as a discrete and binary operation: each studied item may be separated from \DIFdelbegin \DIFdel{the rest of one's experiences, and that item may be }\DIFdelend \DIFaddbegin \DIFadd{all others, and }\DIFaddend labeled as having been recalled \DIFdelbegin \DIFdel{versus }\DIFdelend \DIFaddbegin \DIFadd{or }\DIFaddend forgotten. More nuanced studies might incorporate self-reported confidence measures as a proxy for memory strength, or ask participants to discriminate between ``recollecting'' the (contextual) details of an experience or having a general feeling of ``familiarity'' \citep{Yone02}. \DIFaddbegin \DIFadd{Using well-controlled, trial-based experimental designs, the field has amassed a wealth of valuable information regarding human episodic memory.  }\DIFaddend However, \DIFdelbegin \DIFdel{characterizing and evaluating memoryin more realistic contexts (e.  g., recounting a recent experience to a friend) is fundamentally different in at least three ways}\DIFdelend \DIFaddbegin \DIFadd{there are fundamental properties of the external world and our memories that trial-based experiments are not well suited to capture}\DIFaddend ~\citep[for review also see][]{KoriGold94, HukEtal18}.  First, \DIFdelbegin \DIFdel{real world recall is }\DIFdelend \DIFaddbegin \DIFadd{our experiences and memories are }\DIFaddend continuous, rather than \DIFdelbegin \DIFdel{discrete.  Unlike in trial-based experiments, removing }\DIFdelend \DIFaddbegin \DIFadd{discrete---removing }\DIFaddend a (naturalistic) event from the context in which it occurs can substantially change its meaning.  Second, the specific \DIFdelbegin \DIFdel{words }\DIFdelend \DIFaddbegin \DIFadd{language }\DIFaddend used to describe an experience \DIFdelbegin \DIFdel{have }\DIFdelend \DIFaddbegin \DIFadd{has }\DIFaddend little bearing on whether the experience should be considered to have been ``remembered.''  Asking whether the rememberer has precisely reproduced a specific set of words to describe a given experience is nearly orthogonal to whether they were actually able to remember it.  In classic (e.g., list-learning) memory studies, by contrast, \DIFdelbegin \DIFdel{counting }\DIFdelend the number or proportion of precise recalls is often a primary metric \DIFdelbegin \DIFdel{of }\DIFdelend \DIFaddbegin \DIFadd{for }\DIFaddend assessing the quality of participants' memories.  Third, one might remember the \textit{\DIFdelbegin \DIFdel{gist}%DIFDELCMD < \MBLOCKRIGHTBRACE %%%
\DIFdel{or }\DIFdelend essence\DIFaddbegin } \DIFadd{(or a general summary) }\DIFaddend of an experience but forget (or neglect to recount) particular details.  Capturing the \DIFdelbegin \DIFdel{gist }\DIFdelend \DIFaddbegin \DIFadd{essence }\DIFaddend of what happened is typically the main ``point'' of recounting a memory to a listener\DIFdelbegin \DIFdel{whereas, depending on the circumstances, accurate recall of any specific detail may be irrelevant.  There is no analog of the gist of an experiencein most traditional memory experiments; rather we tend to assess participants' abilities to recover specific details (e.
g., computing the proportion of specific stimuli they remember, which presentation positions the remembered stimuli came from, etc.).
}\DIFdelend \DIFaddbegin \DIFadd{, while the addition of highly specific details may add comparatively little to successful conveyance of an experience.
}\DIFaddend 

How might one go about formally characterizing the \DIFdelbegin \DIFdel{gist }\DIFdelend \DIFaddbegin \DIFadd{``essence" }\DIFaddend of an experience, or whether \DIFdelbegin \DIFdel{that gist }\DIFdelend \DIFaddbegin \DIFadd{it }\DIFaddend has been recovered by the rememberer?  Any given moment of an experience derives meaning from surrounding moments, as well as from longer-range temporal associations~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\citep[e.g., ][]{LernEtal11}}\hspace{0pt}%DIFAUXCMD
.  Therefore}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\citep{LernEtal11, Mann19}}\hspace{0pt}%DIFAUXCMD
.  Therefore, }\DIFaddend the timecourse describing how an event unfolds is fundamental to its overall meaning.  Further, this hierarchy formed by our subjective experiences at different timescales defines a \textit{context} for each new moment~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\citep[e.g., ][]{HowaKaha02, HowaEtal14}}\hspace{0pt}%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\citep[e.g., ][]{HowaKaha02a, HowaEtal14}}\hspace{0pt}%DIFAUXCMD
}\DIFaddend , and plays an important role in how we interpret that moment and remember it later~\citep[for review see][]{MannEtal15}.  Our memory systems can \DIFdelbegin \DIFdel{then }\DIFdelend leverage these associations to form predictions that help guide our behaviors~\citep{RangRitc12}.  For example, as we navigate the world, the features of our subjective experiences tend to change gradually (e.g., the room or situation we are in \DIFaddbegin \DIFadd{at any given moment }\DIFaddend is strongly temporally autocorrelated), allowing us to form stable estimates of our current situation and behave accordingly~\citep{ZackEtal07, ZwaaRadv98}.
\DIFdelbegin \DIFdel{Although our experiences most often change gradually, they also occasionally change suddenly}\DIFdelend \DIFaddbegin 

\DIFadd{Occasionally, this gradual ``drift" of our ongoing experience is punctuated by sudden changes, or ``shifts"}\DIFaddend ~\citep[e.g., when we walk through a doorway; ][]{RadvZack17}.  Prior research suggests that these sharp transitions (termed \textit{event boundaries}) \DIFdelbegin \DIFdel{during an experience }\DIFdelend help to discretize our experiences \DIFaddbegin \DIFadd{(and their mental representations) }\DIFaddend into \textit{events}~\citep{RadvZack17, BrunEtal18, HeusEtal18b, ClewDava17, EzzyDava11, DuBrDava13}.  The interplay between the stable (\DIFdelbegin \DIFdel{within event}\DIFdelend \DIFaddbegin \DIFadd{within-event}\DIFaddend ) and transient (\DIFdelbegin \DIFdel{across event}\DIFdelend \DIFaddbegin \DIFadd{across-event}\DIFaddend ) temporal dynamics of an experience also provides a potential framework for transforming experiences into memories that distill those experiences down to their \DIFdelbegin \DIFdel{essence-- i.  e., their gists.  }\DIFdelend \DIFaddbegin \DIFadd{essence.  }\DIFaddend For example, prior work has shown that event boundaries can influence how we learn sequences of items~\citep{HeusEtal18b, DuBrDava13}, navigate~\citep{BrunEtal18}, and remember and understand narratives~\citep{ZwaaRadv98, EzzyDava11}. \DIFaddbegin \DIFadd{Prior research has implicated a network of brain regions (including the hippocampus and the medial prefrontal cortex) as playing a critical role in transforming experiences into structured and consolidated memories ~\mbox{%DIFAUXCMD
\citep{TompDava17}}\hspace{0pt}%DIFAUXCMD
.
}\DIFaddend 

Here we sought to examine how the temporal dynamics of a ``naturalistic'' experience were \DIFaddbegin \DIFadd{later }\DIFaddend reflected in participants' \DIFdelbegin \DIFdel{later memoriesof that experience}\DIFdelend \DIFaddbegin \DIFadd{memories}\DIFaddend .  We analyzed an open dataset that comprised behavioral and functional Magnetic Resonance Imaging (fMRI) data collected as participants viewed and then verbally \DIFdelbegin \DIFdel{recalled }\DIFdelend \DIFaddbegin \DIFadd{recounted }\DIFaddend an episode of the BBC television series \textit{Sherlock}~\citep{ChenEtal17}.  We developed a computational framework for characterizing the temporal dynamics of the moment-by-moment content of the episode\DIFaddbegin \DIFadd{, }\DIFaddend and of participants' verbal recalls.  Specifically, we use topic modeling~\citep{BleiEtal03} to characterize the thematic conceptual (semantic) content present in each moment of the episode and recalls, and \DIFdelbegin \DIFdel{we use }\DIFdelend Hidden Markov Models~\citep{Rabi89, BaldEtal17} to discretize \DIFdelbegin \DIFdel{the }\DIFdelend \DIFaddbegin \DIFadd{this }\DIFaddend evolving semantic content into events.  In this way, we cast naturalistic experiences (and recalls of those experiences) as \DIFaddbegin \DIFadd{geometric }\DIFaddend \textit{\DIFdelbegin \DIFdel{topic }\DIFdelend trajectories} that describe how the experiences evolve over time. \DIFdelbegin \DIFdel{In other words, the episode's topic trajectory is a formalization of its gist.  }\DIFdelend Under this framework, successful remembering entails verbally ``traversing'' the \DIFdelbegin \DIFdel{topic }\DIFdelend \DIFaddbegin \DIFadd{content }\DIFaddend trajectory of the \DIFdelbegin \DIFdel{original }\DIFdelend episode, thereby reproducing the \DIFdelbegin \DIFdel{original episode's gist.  In addition, comparing }\DIFdelend \DIFaddbegin \DIFadd{shape (or essence) of the original experience.  Comparing }\DIFaddend the shapes of the topic trajectories of the \DIFdelbegin \DIFdel{original }\DIFdelend episode and of participants' retellings of the episode \DIFaddbegin \DIFadd{then }\DIFaddend reveals which aspects of the episode were preserved (or lost) in the translation into memory.  We \DIFdelbegin \DIFdel{also identified a network of }\DIFdelend \DIFaddbegin \DIFadd{further introduce two novel metrics for assessing memory quality: the }\textit{\DIFadd{precision}} \DIFadd{with which a participant recounts each event and 2) the }\textit{\DIFadd{distinctiveness}} \DIFadd{of each recall event (relative to other recalled events).  We examine how these metrics relate to participants' overall memory performance, and discuss the ways in which they improve upon classic ``proportion-recalled" measures for analyzing naturalistic memory.  Last, we utilize our framework to identify networks of }\DIFaddend brain structures whose responses (as participants watched the episode) reflected the \DIFdelbegin \DIFdel{gist }\DIFdelend \DIFaddbegin \DIFadd{temporal dynamics }\DIFaddend of the episode, and \DIFdelbegin \DIFdel{a second network whose responses reflected }\DIFdelend how participants would later recount \DIFdelbegin \DIFdel{the episode}\DIFdelend \DIFaddbegin \DIFadd{it}\DIFaddend .


\section*{Results}
To characterize the \DIFdelbegin \DIFdel{gists }\DIFdelend \DIFaddbegin \DIFadd{temporally dynamic contents }\DIFaddend of the \textit{Sherlock} episode \DIFdelbegin \DIFdel{participants watched and their subsequent recountingsof the episode}\DIFdelend \DIFaddbegin \DIFadd{and participants' subsequent recountings}\DIFaddend , we used a topic model~\citep{BleiEtal03} to discover the latent \DIFdelbegin \DIFdel{thematic content in the video}\DIFdelend \DIFaddbegin \DIFadd{themes}\DIFaddend .  Topic models take as inputs a vocabulary of words to consider and a collection of text documents\DIFdelbegin \DIFdel{; they return as output two }\DIFdelend \DIFaddbegin \DIFadd{, and return two output }\DIFaddend matrices.  The first \DIFdelbegin \DIFdel{output }\DIFdelend \DIFaddbegin \DIFadd{of these }\DIFaddend is a \textit{topics matrix} whose rows are topics (latent themes) and whose columns correspond to words in the vocabulary. The entries of the topics matrix define how each word in the vocabulary is weighted by each discovered topic.  For example, a detective-themed topic might weight heavily on words like ``crime,'' and ``search.''  The second output is a \textit{topic proportions matrix}, with one row per document and one column per topic.  The topic proportions matrix describes \DIFdelbegin \DIFdel{which mix }\DIFdelend \DIFaddbegin \DIFadd{what mixture of discovered }\DIFaddend topics is reflected in each document.

\cite{ChenEtal17} collected hand-annotated information about each of 1000 (manually identified) \DIFdelbegin \DIFdel{scenes }\DIFdelend \DIFaddbegin \DIFadd{time segments }\DIFaddend spanning the roughly \DIFdelbegin \DIFdel{45 }\DIFdelend \DIFaddbegin \DIFadd{50 }\DIFaddend minute video used in their experiment.  This information included: a brief narrative description of what was happening\DIFdelbegin \DIFdel{; whether the }\DIFdelend \DIFaddbegin \DIFadd{, the location where the }\DIFaddend scene took place\DIFdelbegin \DIFdel{indoors vs. outdoors; }\DIFdelend \DIFaddbegin \DIFadd{, the }\DIFaddend names of any characters on the screen\DIFdelbegin \DIFdel{; names of any characters who were in focus in the camera shot; names of characters who were speaking; the location where the scene took place; the camera angle (close up, medium, long, etc.); whether or not background music was present; }\DIFdelend \DIFaddbegin \DIFadd{, }\DIFaddend and other similar details (for a full list of annotated features\DIFaddbegin \DIFadd{, }\DIFaddend see \textit{Methods}).  We took from these annotations the union of all unique words (excluding stop words, such as ``and,\DIFdelbegin \DIFdel{'' }\DIFdelend \DIFaddbegin \DIFadd{" }\DIFaddend ``or,\DIFdelbegin \DIFdel{'' }\DIFdelend \DIFaddbegin \DIFadd{" }\DIFaddend ``but,\DIFdelbegin \DIFdel{'' }\DIFdelend \DIFaddbegin \DIFadd{" }\DIFaddend etc.) across all features and scenes as the ``vocabulary\DIFdelbegin \DIFdel{'' }\DIFdelend \DIFaddbegin \DIFadd{" }\DIFaddend for the topic model.  We then concatenated the sets of words across all features contained in overlapping\DIFdelbegin \DIFdel{50-scene sliding windows }\DIFdelend \DIFaddbegin \DIFadd{, sliding windows of (up to) 50 scenes}\DIFaddend , and treated each \DIFdelbegin \DIFdel{50-scene sequence }\DIFdelend \DIFaddbegin \DIFadd{window }\DIFaddend as a single ``document\DIFdelbegin \DIFdel{'' for the purposes }\DIFdelend \DIFaddbegin \DIFadd{" for the purpose }\DIFaddend of fitting the topic model.  Next, we fit a topic model with (up to) $K = 100$ topics to this collection of documents.  We found that \DIFdelbegin \DIFdel{27 }\DIFdelend \DIFaddbegin \DIFadd{32 }\DIFaddend unique topics (with non-zero weights) were sufficient to describe the time-varying content of the video (see \textit{Methods}; Figs.~\ref{fig:schematic}, \topics).  Note that our approach is similar in some respects to Dynamic Topic Models~\citep{BleiLaff06} \DIFdelbegin \DIFdel{, }\DIFdelend in that we sought to characterize how the thematic content of the episode evolved over time.  However, whereas Dynamic Topic Models are designed to characterize how the properties of \textit{collections} of documents change over time, our sliding window approach allows us to examine the topic dynamics within a single document (or video).  Specifically, our approach yielded (via the topic proportions matrix) a single \textit{topic vector} for each \DIFdelbegin \DIFdel{timepoint of the episode (we set timepoints }\DIFdelend \DIFaddbegin \DIFadd{sliding window of annotations transformed by the topic model.  We then stretched (interpolated) the resulting windows-by-topics matrix }\DIFaddend to match the \DIFdelbegin \DIFdel{acquisition times }\DIFdelend \DIFaddbegin \DIFadd{time series }\DIFaddend of the 1976 fMRI volumes collected as participants viewed the episode\DIFdelbegin \DIFdel{)}\DIFdelend .

\begin{figure}[tp]
\centering
\includegraphics[width=1\textwidth]{figs/schematic}
\caption{\small \textbf{Methods overview.} We used hand-annotated descriptions of each moment of video to fit a topic model.  Three example video frames and their associated descriptions are displayed (top two rows).  Participants later recalled the video (in the third row, we show example recalls of the same three scenes from participant \DIFdelbeginFL \DIFdelFL{13}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{17}\DIFaddendFL ).  We used the topic model (fit to the annotations) to estimate topic vectors for each moment of video and each sentence the participants recalled.  Example topic vectors are displayed in the bottom row (blue: video annotations; green: example participant's recalls).  Three topic dimensions are shown (the highest-weighted topics for each of the three example scenes, respectively).  We also show the \DIFdelbeginFL \DIFdelFL{ten }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{10 }\DIFaddendFL highest-weighted words for each topic.  Figure~\topics~provides a full list of the top 10 words from each of the discovered topics.}
\label{fig:schematic}
\end{figure}
%DIF >  Odd name for figure in "Results" section rather than "methods" section
%DIF >  - Paxton
%DIF >  The 10 top words in this figure seem to be out of order or incorrect. Paxton, can you look into this? - Andy

The \DIFaddbegin \DIFadd{32 }\DIFaddend topics we found were heavily character-focused (\DIFdelbegin \DIFdel{e.g.}\DIFdelend \DIFaddbegin \DIFadd{i.e.}\DIFaddend , the top-weighted word in each topic was nearly always a character) and could be roughly divided into themes \DIFdelbegin \DIFdel{that were primarily Sherlock Holmes-focused (Sherlock is }\DIFdelend \DIFaddbegin \DIFadd{centered around Sherlock Holmes (}\DIFaddend the titular character)\DIFdelbegin \DIFdel{; primarily John Watson-focused (John is }\DIFdelend \DIFaddbegin \DIFadd{, John Watson (}\DIFaddend Sherlock's close confidant and assistant)\DIFdelbegin \DIFdel{; or that involved Sherlockand John interacting (}\DIFdelend \DIFaddbegin \DIFadd{, supporting characters (e.g., Inspector Lestrade, Sergeant Donovan, or Sherlock's brother Mycroft), or the interactions between various pairs of these characters (see }\DIFaddend Fig.~\topics).  Several of the \DIFaddbegin \DIFadd{identified }\DIFaddend topics were highly similar, which we hypothesized might allow us to distinguish between subtle narrative differences \DIFdelbegin \DIFdel{(}\DIFdelend if the distinctions between those overlapping topics were meaningful\DIFdelbegin \DIFdel{; also see Fig.  ~\featureimportance).  }\DIFdelend \DIFaddbegin \DIFadd{.  }\DIFaddend The topic vectors for each timepoint were \textit{sparse}, in that only a small number (usually one or two) of topics tended to be ``active\DIFdelbegin \DIFdel{'' }\DIFdelend \DIFaddbegin \DIFadd{" }\DIFaddend in any given timepoint (Fig.~\ref{fig:model}A).  Further, the dynamics of the topic activations appeared to exhibit \textit{\DIFdelbegin \DIFdel{persistance}\DIFdelend \DIFaddbegin \DIFadd{persistence}\DIFaddend } (i.e., given that a topic was active in one timepoint, it was likely to be active in the following timepoint) along with \textit{occasional rapid changes} (i.e., occasionally topics would appear to spring into or out of existence).  These two properties of the topic dynamics may be seen in the block diagonal structure of the timepoint-by-timepoint correlation matrix (Fig.~\ref{fig:model}B) \DIFdelbegin \DIFdel{.  Following \mbox{%DIFAUXCMD
\cite{BaldEtal17}}\hspace{0pt}%DIFAUXCMD
, we }\DIFdelend \DIFaddbegin \DIFadd{and reflect the gradual drift and sudden shifts fundamental to the temporal dynamics of real-world experiences.  Given this observation, we adapted an approach devised by \mbox{%DIFAUXCMD
\cite{BaldEtal17}}\hspace{0pt}%DIFAUXCMD
, and }\DIFaddend used a Hidden Markov Model (HMM) to identify the \textit{event boundaries} where the topic activations changed rapidly (i.e., at the boundaries of the blocks in the correlation matrix; event boundaries identified by the HMM are outlined in yellow \DIFaddbegin \DIFadd{in Fig.~\ref{fig:model}B}\DIFaddend ).  Part of our model fitting procedure required selecting an appropriate number of ``events\DIFdelbegin \DIFdel{'' to segment the timeseries into .  We }\DIFdelend \DIFaddbegin \DIFadd{" into which the topic trajectory should be segmented.  To accomplish this, we }\DIFaddend used an optimization procedure \DIFdelbegin \DIFdel{to identify the number of events that maximized within-event stability while also minimizing across-event correlations }\DIFdelend \DIFaddbegin \DIFadd{that maximized the difference between the topic weights for timepoints within an event and across multiple events }\DIFaddend (see \textit{Methods} for additional details).  \DIFdelbegin \DIFdel{To create }\DIFdelend \DIFaddbegin \DIFadd{We then created }\DIFaddend a stable ``summary'' of the \DIFdelbegin \DIFdel{video, we computed the average topic vector within each event }\DIFdelend \DIFaddbegin \DIFadd{content within each video event by averaging the topic vectors across timepoints each event spanned }\DIFaddend (Fig.~\ref{fig:model}C).

\begin{figure}[tp]
\centering
\includegraphics[width=\textwidth]{figs/eventseg}
\caption{\small \textbf{Modelling naturalistic stimuli and recalls.} All panels: darker colors indicate greater values; range: [0, 1].  \textbf{A.} Topic vectors ($K = 100$) for each of the 1976 video timepoints.  \textbf{B.} Timepoint-by-timepoint correlation matrix of the topic vectors displayed in Panel A.  Event boundaries \DIFdelbeginFL \DIFdelFL{detected }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{discovered }\DIFaddendFL by the HMM are denoted in yellow (\DIFdelbeginFL \DIFdelFL{34 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{30 }\DIFaddendFL events detected).  \textbf{C.} Average topic vectors for each of the \DIFdelbeginFL \DIFdelFL{34 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{30 }\DIFaddendFL video events. \textbf{D.} Topic vectors for each of \DIFdelbeginFL \DIFdelFL{294 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{265 sliding windows of }\DIFaddendFL sentences spoken by an example participant while recalling the video.  \textbf{E.} Timepoint-by-timepoint correlation matrix of the topic vectors displayed in Panel D. Event boundaries detected by the HMM are denoted in yellow (\DIFdelbeginFL \DIFdelFL{27 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{22 }\DIFaddendFL events detected).  \DIFaddbeginFL \DIFaddFL{For similar plots for all participants see Figure~\corrmats.  }\DIFaddendFL \textbf{F.} Average topic vectors for each of the \DIFdelbeginFL \DIFdelFL{27 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{22 }\DIFaddendFL recalled events from the example participant.  \textbf{G.} Correlations between the topic vectors for every pair of video events (Panel C) and recalled events (from the example participant; Panel F).  For similar plots for all participants\DIFaddbeginFL \DIFaddFL{, }\DIFaddendFL see Figure~\matchmats.  \textbf{H.} Average correlations between each pair of video events and recalled events (across all 17 participants).  To create the figure, each recalled event was assigned to the video event with the most correlated topic vector (yellow boxes in panels G and H).\DIFdelbeginFL \DIFdelFL{The heat maps in each panel were created using }\texttt{\DIFdelFL{Seaborn}}%DIFAUXCMD
\DIFdelFL{~\mbox{%DIFAUXCMD
\citep{WaskEtal16}}\hspace{0pt}%DIFAUXCMD
.}\DIFdelendFL }
\label{fig:model}
\end{figure}
%DIF >  I'm all for citing software packages, but where what's the distinction between packages we do cite (seaborn, brainiak, sklearn, hypertools, quail, umap, etc.) and don't cite (wordcloud, nilearn, fastdtw, etc.)?
%DIF >  - Paxton
%DIF >  I also flagged the seaborn reference for removal here. Def not critical to understanding the figure - Andy

Given that the time-varying content of the video could be segmented cleanly into discrete events, we wondered whether participants' recalls of the video also displayed a similar structure.  We applied the same topic model (already trained on the video annotations) to each participant's recalls.  Analogous to how we \DIFdelbegin \DIFdel{analyzed }\DIFdelend \DIFaddbegin \DIFadd{parsed }\DIFaddend the time-varying content of the video, to obtain similar estimates for \DIFdelbegin \DIFdel{participants'recalls}\DIFdelend \DIFaddbegin \DIFadd{each participant's recall}\DIFaddend , we treated each \DIFdelbegin \DIFdel{(overlapping  ) }\DIFdelend \DIFaddbegin \DIFadd{overlapping  ``window" of (up to }\DIFaddend 10\DIFdelbegin \DIFdel{sentence ``window'' of }\DIFdelend \DIFaddbegin \DIFadd{) sentences from }\DIFaddend their transcript as a ``document\DIFdelbegin \DIFdel{'' and then }\DIFdelend \DIFaddbegin \DIFadd{," and }\DIFaddend computed the most probable mix of topics reflected in each timepoint's sentences.  This yielded, for each participant, a \DIFdelbegin \DIFdel{number-of-sentences }\DIFdelend \DIFaddbegin \DIFadd{number-of-windows }\DIFaddend by number-of-topics topic proportions matrix that characterized how the topics identified in the original video were reflected in the participant's recalls.  Note that an important feature of our approach is that it allows us to compare \DIFdelbegin \DIFdel{participant' s }\DIFdelend \DIFaddbegin \DIFadd{participants' }\DIFaddend recalls to events from the original video, despite \DIFdelbegin \DIFdel{that different participants may have used different }\DIFdelend \DIFaddbegin \DIFadd{different participants using widely varying }\DIFaddend language to describe the same event, and that those descriptions may not match the original annotations.  This is a \DIFdelbegin \DIFdel{huge }\DIFdelend \DIFaddbegin \DIFadd{substantial }\DIFaddend benefit of projecting the video and recalls into a shared ``topic\DIFdelbegin \DIFdel{'' }\DIFdelend \DIFaddbegin \DIFadd{" }\DIFaddend space.  An example topic proportions matrix from one participant's recalls is shown in Figure~\ref{fig:model}D.

Although the example participant's recall topic proportions matrix has some visual similarity to the video topic proportions matrix, the time-varying topic proportions for the example participant's recalls are not as sparse as \DIFaddbegin \DIFadd{those }\DIFaddend for the video (\DIFdelbegin \DIFdel{e.g., }\DIFdelend compare Figs.~\ref{fig:model}A and D).  Similarly, although there do appear to be periods of stability in the recall topic dynamics (\DIFdelbegin \DIFdel{e.g.}\DIFdelend \DIFaddbegin \DIFadd{i.e.}\DIFaddend , most topics are active or inactive over contiguous blocks of time), the \DIFaddbegin \DIFadd{individual topics' }\DIFaddend overall timecourses are not as cleanly delineated as the video topics\DIFdelbegin \DIFdel{are}\DIFdelend \DIFaddbegin \DIFadd{'}\DIFaddend .  To examine these patterns in detail, we computed the timepoint-by-timepoint correlation matrix for the example participant's recall topic \DIFdelbegin \DIFdel{proportions }\DIFdelend \DIFaddbegin \DIFadd{trajectory }\DIFaddend (Fig.~\ref{fig:model}E).  As in the video correlation matrix (Fig.~\ref{fig:model}B), the example participant's recall correlation matrix has a strong block diagonal structure, indicating that their recalls are discretized into separated events.  As for the video correlation matrix, we \DIFdelbegin \DIFdel{can use an HMM, along with the aforementioned number-of-events }\DIFdelend \DIFaddbegin \DIFadd{leveraged an HMM-based }\DIFaddend optimization procedure (\DIFdelbegin \DIFdel{also }\DIFdelend see \textit{Methods}) to determine how many events are reflected in the participant's recalls and where specifically the event boundaries fall (outlined in yellow).  We carried out a similar analysis on all 17 participants' recall topic proportions matrices (Fig.~\corrmats).

Two clear patterns emerged from this set of analyses.  First, although every individual participant's recalls could be segmented into discrete events (i.e., every individual participant's recall correlation matrix exhibited clear block diagonal structure; Fig.~\corrmats), each participant appeared to have a unique \textit{recall resolution}, reflected in the sizes of those blocks.  \DIFdelbegin \DIFdel{For example}\DIFdelend \DIFaddbegin \DIFadd{While}\DIFaddend , some participants' recall topic proportions segmented into just a few events (e.g., Participants \DIFdelbegin \DIFdel{P1, }\DIFdelend P4, \DIFdelbegin \DIFdel{and P15), while others' recalls }\DIFdelend \DIFaddbegin \DIFadd{P5, and P7), others' }\DIFaddend segmented into many shorter duration events (e.g., Participants P12, P13, and P17).  This suggests that different participants may be recalling the video with different levels of detail-- e.g., some might touch on just the major plot points, whereas others might attempt to recall every minor scene \DIFaddbegin \DIFadd{or action}\DIFaddend .  The second clear pattern present in every individual participant's recall correlation matrix is that, unlike in the video correlation matrix, there are substantial off-diagonal correlations\DIFdelbegin \DIFdel{in participant's recalls}\DIFdelend .  Whereas each event in the original video \DIFdelbegin \DIFdel{(was }\DIFdelend \DIFaddbegin \DIFadd{was (}\DIFaddend largely) separable from the others (Fig.~\ref{fig:model}B), in transforming those separable events into memory\DIFaddbegin \DIFadd{, }\DIFaddend participants appear to be integrating \DIFdelbegin \textit{\DIFdel{across}} %DIFAUXCMD
\DIFdel{different }\DIFdelend \DIFaddbegin \DIFadd{across multiple }\DIFaddend events, blending elements of previously recalled and not-yet-recalled \DIFdelbegin \DIFdel{events }\DIFdelend \DIFaddbegin \DIFadd{content }\DIFaddend into each newly recalled event~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\citep[Figs.~\ref{fig:model}D, \corrmats; also see][]{MannEtal11, HowaEtal12}}\hspace{0pt}%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\citep[Figs.~\ref{fig:model}E, \corrmats; also see][]{MannEtal11, HowaEtal12}}\hspace{0pt}%DIFAUXCMD
}\DIFaddend .

The above results indicate that both the structure of the original video and participants' recalls of the video exhibit event boundaries that can be identified automatically by characterizing the dynamic content using a shared topic model and segmenting the content into events \DIFdelbegin \DIFdel{using }\DIFdelend \DIFaddbegin \DIFadd{via }\DIFaddend HMMs.  Next\DIFaddbegin \DIFadd{, }\DIFaddend we asked whether some correspondence might be made between the specific content of the events the participants experienced in the video, and the events they later recalled.  One approach to linking the experienced (video) and recalled events is to label each recalled event as matching the video event with the most similar (i.e., most highly correlated) topic vector (Figs.~\ref{fig:model}G, \matchmats).  This yields a sequence of ``presented\DIFdelbegin \DIFdel{'' }\DIFdelend \DIFaddbegin \DIFadd{" }\DIFaddend events from the original video, and a \DIFdelbegin \DIFdel{sequence of }\DIFdelend (potentially differently ordered) \DIFdelbegin \DIFdel{``recalled'' }\DIFdelend \DIFaddbegin \DIFadd{sequence of ``recalled" }\DIFaddend events for each participant.  Analogous to classic list-learning studies, we can then examine participants' recall sequences by asking which events they tended to recall first~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\citep[probability of first recall; Fig.~\listlearning A;][]{WelcBurn24, PostPhil65, AtkiShif68}}\hspace{0pt}%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\citep[probability of first recall; Fig.~\ref{fig:list-learning}A;][]{AtkiShif68, PostPhil65, WelcBurn24}}\hspace{0pt}%DIFAUXCMD
}\DIFaddend ; how participants most often transition between recalls of the events as a function of the temporal distance between them~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\citep[lag-conditional response probability; Fig.~\listlearning B;][]{Kaha96}}\hspace{0pt}%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\citep[lag-conditional response probability; Fig.~\ref{fig:list-learning}B;][]{Kaha96}}\hspace{0pt}%DIFAUXCMD
}\DIFaddend ; and which events they were likely to remember overall~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\citep[serial position recall analyses; Fig.~\listlearning C;][]{Murd62a}}\hspace{0pt}%DIFAUXCMD
. In }\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\citep[serial position recall analyses; Fig.~\ref{fig:list-learning}C;][]{Murd62a}}\hspace{0pt}%DIFAUXCMD
. Interestingly, for two of these analyses (probability of first recall and lag-conditional response probability curves) we observe patterns comparable to classic effects from the }\DIFaddend list-learning \DIFdelbegin \DIFdel{studies, this set of three analyses may be used to gain a nearly complete view into the sequences of recalls participants made~\mbox{%DIFAUXCMD
\citep[e.g., ][]{Kaha12}}\hspace{0pt}%DIFAUXCMD
. Extending these analyses to apply to naturalistic stimuli and recall~\mbox{%DIFAUXCMD
\citep{HeusEtal17b} }\hspace{0pt}%DIFAUXCMD
highlights that , in naturalistic recall, these analyses provide a wholly incomplete picture:they leave out any attempt to quantify participants' abilities to capture the }\textit{\DIFdel{content}} %DIFAUXCMD
\DIFdel{of what occurred in the video-- their only experimental instruction!
}\DIFdelend \DIFaddbegin \DIFadd{literature: namely, a higher probability of initiating recall with the first event in the sequence (Fig.~\ref{fig:list-learning}A) and a higher probability of transitioning to neighboring events with an asymmetric forward bias (Fig.~\ref{fig:list-learning}B). In contrast, we do not observe a pattern comparable to the serial position effect (Fig.~\ref{fig:list-learning}C), but rather we see higher memory for specific events distributed somewhat evenly throughout the video.
}\DIFaddend 

\DIFdelbegin \DIFdel{The dynamic }\DIFdelend \DIFaddbegin \DIFadd{We can also apply two list-learning-native analyses that describe how participants group items in their recall sequences: temporal clustering and semantic clustering \mbox{%DIFAUXCMD
\citep[][see \textit{Methods} for details]{PolyEtal09}}\hspace{0pt}%DIFAUXCMD
.  Temporal clustering refers to the extent to which participants group their recall responses according to encoding position.  Overall, we found that sequentially viewed video events were clustered heavily in participants' recall event sequences (mean clustering score: 0.767, SEM: 0.029), and that participants with higher temporal clustering scores tended to perform better according to both \mbox{%DIFAUXCMD
\cite{ChenEtal17}}\hspace{0pt}%DIFAUXCMD
's hand-annotated memory scores (Pearson's $r(15) = 0.62,~p = 0.008$) and our model's estimate (Pearson's $r(15) = 0.54,~p = 0.024$).  Semantic clustering measures the extent to which participants cluster their recall responses according to semantic similarity.  We found that participants tended to recall semantically similar video events together (mean clustering score: 0.787, SEM: 0.018), and that semantic clustering score was also related to both hand-annotated  (Pearson's $r(15) = 0.65,~p = 0.004$) and model-derived (Pearson's $r(15) = 0.63,~p = 0.007$) memory performance.
%DIF >  NOTE:
%DIF >  Semantic and temporal clustering are both positively correlated with n_events -- r=0.49; p=0.046 and r=0.61; p=0.009, respectively.
%DIF >  - Paxton
}

\begin{figure}[t]
  \centering
  \includegraphics[width=1\textwidth]{figs/list_learning}
  \caption{\small \textbf{\DIFaddFL{Naturalistic extensions of classic list-learning memory analyses.}} \textbf{\DIFaddFL{A.}} \DIFaddFL{The probability of first recall as a function of the serial position of the event in the video. }\textbf{\DIFaddFL{B}}\DIFaddFL{.  The probability of recalling each event, conditioned on having most recently recalled the event }\textit{\DIFaddFL{lag}} \DIFaddFL{events away in the video.  }\textbf{\DIFaddFL{C.}} \DIFaddFL{The proportion of participants who recalled each event, as a function of the serial position of the events in the video.  All panels: error bars denote bootstrap-estimated standard error of the mean.}}
  \label{fig:list-learning}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=1\textwidth]{figs/precision_distinctiveness}
  \caption{\small \textbf{\DIFaddFL{Novel content-based metrics of naturalistic memory: precision and distinctiveness.}} \textbf{\DIFaddFL{A.}} \DIFaddFL{The video-recall correlation matrix for a representative participant (17).  The yellow boxes highlight the maximum correlation in each column.  The example participant's overall precision score was computed as the average across correlation values in the yellow boxes.  Their distinctiveness score was computed as the average (over recall events) of 1 minus the average correlation between each recall event and all other recall events that do not display a box in the same row.  }\textbf{\DIFaddFL{B.}} \DIFaddFL{The (Pearson's) correlation between precision and hand-annotated memory performance. }\textbf{\DIFaddFL{C.}} \DIFaddFL{The correlation between distinctiveness and hand-annotated memory performance. }\textbf{\DIFaddFL{D.}} \DIFaddFL{The correlation between precision and the number of video events successfully recalled, as determined by our model. }\textbf{\DIFaddFL{E.}} \DIFaddFL{The correlation between distinctiveness and the number of video events successfully recalled, as determined by our model.}}
  \label{fig:precision-distinctiveness}
\end{figure}
%DIF >  Need to reconcile the description of distinctiveness with how we computed it... - Andy

\DIFadd{Statistical models of memory studies often treat recall success as binary (i.e., an item either was or was not recalled), or occasionally categorical \mbox{%DIFAUXCMD
\citep[e.g., to distinguish familiarity from recollection;][]{YoneEtal02}}\hspace{0pt}%DIFAUXCMD
.  Such approaches are tenable in classical list-learning or recognition memory paradigms, as the presented stimuli tend to be very simple (e.g., a sequence of individual words or items).  However, the feature-rich content of a naturalistic experiences may later be described with many, highly variable levels of success.  Our framework produces a content-based model of individual stimulus and recall events by projecting the dynamic content of the video and participants' recalls into a shared topic space.  This allows for direct, quantitative comparison between all stimulus and recall events, as well as between the recall events themselves.  Leveraging these content-based models of the stimulus/recall events, we developed two novel, }\textit{\DIFadd{continuous}} \DIFadd{metrics for analyzing naturalistic memory:~}\textit{\DIFadd{precision}} \DIFadd{and }\textit{\DIFadd{distinctiveness}}\DIFadd{.  We define precision as the ``completeness" of recall, or how fully the presented content was recapitulated in memory.  Under our framework, we quantify this for a given recall event as the correlation between the topic proportions of the recall event and the maximally correlated video event (Fig.~\ref{fig:precision-distinctiveness}).  A second novel metric we introduce here is }\textit{\DIFadd{distinctiveness}}\DIFadd{, which we define as the ``specificity" of recall, or how unique the description of a given section of content was, compared to descriptions for other sections of content.  We quantify this for each recall event as 1 minus the average correlation between the given recall event and all other recall events not matched to the same video event.  In addition to individual events, one may also use these metrics to describe each participant's overall performance (i.e., by averaging across a participant's event-wise precision or distinctiveness scores).  Participants whose recall events are more veridical descriptions of what happened in the video event will presumably have higher precision scores. We find that, across participants, a higher precision score is correlated to both hand-annotated memory performance (Pearson's $r(15) = 0.60, p = 0.010$) and the number of video events successfully remembered, as determined by our model (Pearson's $r(15) = 0.90, p < 0.001$).  We also hypothesized that participants who recounted events in a more distinctive way would display better overall memory.  We find that this distinctiveness score is related to our model's estimated number of recalled events (Pearson's $r(15) = 0.55, p = 0.021$), and while we do not find distinctiveness to be related to hand-annotated memory performance (Pearson's $r(15) = 0.28, p = 0.275$), this is not entirely surprising given how the hand-annotated memory scores were computed (see }\textit{\DIFadd{Methods}} \DIFadd{and }\textit{\DIFadd{Discussion}} \DIFadd{for details).
}

\begin{figure}[t]
  \centering
  \includegraphics[width=1\textwidth]{figs/precision_detail}
  \caption{\small \textbf{\DIFaddFL{Precision metric reflects completeness of recall.}} \textbf{\DIFaddFL{A.}} \DIFaddFL{Recall precision by video event.  Grey violin plots display kernel density estimates for the distribution of recall precision scores for a single video event.  Colored dots within each violin plot represent individual participants' recall precision for the given event.  Video events are ordered along the $x$-axis by the average precision with which they were remembered.  }\textbf{\DIFaddFL{B.}} \DIFaddFL{The set of ``Narrative Details" video annotations (generated by \mbox{%DIFAUXCMD
\citealp{ChenEtal17}}\hspace{0pt}%DIFAUXCMD
) for scenes comprising an example video event (22) identified by the HMM.  Each action or feature is highlighted in a different color.  }\textbf{\DIFaddFL{C.}} \DIFaddFL{A subset of the sentences comprising the most precise (P17) and least precise (P6) participants' recalls of video event 22.  Descriptions of specific actions or features reflecting those highlighted in panel B are highlighted in the corresponding color.}}
  \label{fig:precision-detail}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=1\textwidth]{figs/distinctiveness_detail}
  \caption{\small \textbf{\DIFaddFL{Distinctiveness metric reflects specificity of recall.}} \textbf{\DIFaddFL{A.}} \DIFaddFL{Recall distinctiveness by video event.  Kernel density estimates for each video event's distribution of recall distinctiveness scores, analogous to Fig.~\ref{fig:precision-detail}A.  }\textbf{\DIFaddFL{B.}} \DIFaddFL{The sets of ``Narrative Details" video annotations (generated by \mbox{%DIFAUXCMD
\citealp{ChenEtal17}}\hspace{0pt}%DIFAUXCMD
) for scenes comprising video events described by the example participants in panel C.  Each event's text is highlighted in a different color.  }\textbf{\DIFaddFL{C.}} \DIFaddFL{The sentences comprising the most distinctive (P13) and least distinctive (P11) participants' recalls of video event 19.  Sections of recall describing each each video event in panel B are highlighted with the corresponding color.}}
  \label{fig:distinctiveness-detail}
\end{figure}

\DIFadd{Further intuition for the behaviors captured by these two metrics may be gained by directly examining the }\DIFaddend content of the video and \DIFdelbegin \DIFdel{participants' recalls is quantified in the corresponding }\DIFdelend \DIFaddbegin \DIFadd{recalls our framework models.  In Figure \ref{fig:precision-detail}, we contrast recalls for the same video event (event 22) from two participants: one with a high precision score (P17), the other with a low precision score (P6).  From the HMM-identified event boundaries, we recovered the set of annotations describing the content of an example video event (Fig.~\ref{fig:precision-detail}B), and divided them into different color-coded sections for each action or feature described.  We then similarly recovered the set of sentences comprising the corresponding recall event for each of the two example participants.  Because the recall sliding windows overlap heavily, and each recall event spans multiple recall timepoints (i.e., windows), we have stripped any sentences from the beginning and end that describe earlier or later video events for the sake of readability.  In other words, Fig.~\ref{fig:precision-detail}C shows a subset of the full recall event text, comprising sentences between the first and last descriptions of content from the example video event.  We then colored all words describing actions and features coded in panel B by their corresponding color.  Visual comparison of the transcripts reveals that the most precise participant's recall both captures more of the video event's content, and does so with far more detail.
}

\DIFadd{Figure \ref{fig:distinctiveness-detail} similarly contrasts two example participants' recalls for a common video event (event 19) to illustrate the tangible differences between high and low distinctiveness scores.  Here, we have extracted the full set of sentences comprising the most distinctive recall event (P13) and least distinctive recall event (P11) matched to the example video event (Fig.~\ref{fig:distinctiveness-detail}C).  We also extracted the annotations for the example video event, as well as those from each other video event whose content the example participants' single recall events described (Fig.~\ref{fig:distinctiveness-detail}B).  We then shaded the annotation text for each video event with a different color, and shaded each word of the example participants' recall text by the color of the video event it describes.  The majority of the most distinctive recall event text describes video event 19's content, with the first five and last one sentence describing the video events immediately preceding and succeeding the current one, respectively.  In contrast, the least precise participant's recall for video event 19 blends the content from five separate video events, does not transition between them in order, and often combines descriptions of two video events' content in the same sentence.
}

\DIFadd{The prior analyses leverage the correspondence between the 100-dimensional }\DIFaddend topic proportion matrices \DIFaddbegin \DIFadd{for the video and participants' recalls to characterize recall}\DIFaddend .  However, it is difficult to gain deep insights into \DIFdelbegin \DIFdel{that content }\DIFdelend \DIFaddbegin \DIFadd{the content of (or relationships between) experiences and memories }\DIFaddend solely by examining \DIFdelbegin \DIFdel{the topic proportion matrices }\DIFdelend \DIFaddbegin \DIFadd{these topic proportions }\DIFaddend (e.g., Figs.~\ref{fig:model}A, D) or the corresponding correlation matrices (Figs.~\ref{fig:model}B, E, \corrmats).  \DIFaddbegin \DIFadd{And while we can directly examine the original text underlying these topic vectors (e.g., Figs.~\ref{fig:precision-detail},~\ref{fig:distinctiveness-detail}) to show how relationships between them reflect real-world behavior, this comparison becomes prohibitively cumbersome at larger timescales.  }\DIFaddend To visualize the time-varying high-dimensional content in a more intuitive way~\citep{HeusEtal18a}\DIFaddbegin \DIFadd{, }\DIFaddend we projected the topic proportions matrices onto a two-dimensional space using Uniform Manifold Approximation and Projection~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\citep[UMAP; ][]{McInHeal18}}\hspace{0pt}%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\citep[UMAP; ][]{McInEtal18}}\hspace{0pt}%DIFAUXCMD
}\DIFaddend .  In this lower-dimensional space, each point represents a single video or recall event, and the distances between the points reflect the distances between the events' associated topic vectors (Fig.~\ref{fig:trajectory}). \DIFaddbegin \DIFadd{In other words, events that are nearer to each other in this space are more semantically similar, and those that are farther apart are less so.
}\DIFaddend 


\begin{figure}[tp]
\centering
\includegraphics[width=1\textwidth]{figs/trajectory}
\caption{\small \textbf{Trajectories through topic space capture the dynamic content of the video and recalls.}  All panels: the topic proportion matrices have been projected onto a shared two-dimensional space using UMAP.  \textbf{A.} The two-dimensional topic trajectory taken by the episode of \textit{Sherlock}.  Each dot indicates an event identified using the HMM (see \textit{Methods}); the dot colors denote the order of the events (early events are in red; later events are in blue), and the connecting lines indicate the transitions between successive events.  \textbf{B.} The average two-dimensional trajectory captured by participants' recall sequences, with the same format and coloring as the trajectory in Panel A.  To compute the event positions, we matched each recalled event with an event from the original video (see \textit{Results}), and then we averaged the positions of all events with the same label.  The arrows reflect the average transition direction through topic space taken by any participants whose trajectories crossed that part of topic space; blue denotes reliable agreement across participants via a Rayleigh test ($p < 0.05$, corrected).  \textbf{C.} The recall topic trajectories (gray) taken by each individual participant (P1--P17).  The video's trajectory is shown in black for reference.  \DIFaddbeginFL \DIFaddFL{Here, events }\DIFaddendFL (\DIFdelbeginFL \DIFdelFL{Same format and coloring as }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{dots) are colored by their matched video event (}\DIFaddendFL Panel A\DIFdelbeginFL \DIFdelFL{.}\DIFdelendFL )\DIFaddbeginFL \DIFaddFL{.}\DIFaddendFL }
\label{fig:trajectory}
\end{figure}


Visual inspection of the video and recall topic trajectories reveals a striking pattern.  First, the topic trajectory of the video (which reflects its dynamic content; Fig.~\ref{fig:trajectory}A) is captured nearly perfectly by the averaged topic trajectories of participants' recalls (Fig.~\ref{fig:trajectory}B).  To assess the consistency of these recall trajectories across participants, we asked: given that a participant's recall trajectory had entered a particular location in \DIFaddbegin \DIFadd{the reduced }\DIFaddend topic space, could the position of their \textit{next} recalled event be predicted reliably?  For each location in \DIFaddbegin \DIFadd{the the reduced }\DIFaddend topic space, we computed the set of line segments connecting successively recalled events (across all participants) that intersected that location (see \textit{Methods} for additional details).  We then computed (for each location) the distribution of angles formed by the lines defined by those line segments and a fixed reference line (the $x$-axis).  Rayleigh tests revealed the set of locations in topic space at which these across-participant distributions exhibited reliable peaks (blue arrows in Fig.~\ref{fig:trajectory}B reflect significant peaks at $p < 0.05$, corrected).  We observed that the locations traversed by nearly the entire video trajectory exhibited such peaks.  In other words, participants exhibited similar trajectories that also matched the trajectory of the original video (Fig.~\ref{fig:trajectory}C).  This is especially notable when considering the fact that the number of events participants recalled (dots in Fig.~\ref{fig:trajectory}C) varied considerably across people, and that every participant used different words to describe what they had remembered happening in the video.  Differences in the numbers of remembered events appear in participants' trajectories as differences in the sampling resolution along the trajectory.  We note that this framework also provides a means of \DIFdelbegin \DIFdel{detangling }\DIFdelend \DIFaddbegin \DIFadd{disentangling }\DIFaddend classic ``proportion recalled'' measures (i.e., the proportion of video events \DIFdelbegin \DIFdel{referenced }\DIFdelend \DIFaddbegin \DIFadd{described }\DIFaddend in participants' recalls) from participants' abilities to recapitulate the \DIFdelbegin \DIFdel{full gist }\DIFdelend \DIFaddbegin \DIFadd{overall unfolding }\DIFaddend of the original video\DIFaddbegin \DIFadd{'s content }\DIFaddend (i.e., the similarity \DIFdelbegin \DIFdel{in the shape }\DIFdelend \DIFaddbegin \DIFadd{between the shapes }\DIFaddend of the original video trajectory and that defined by each participant's recounting of the video).
%DIF >  ------ NOTE: ------
%DIF >  not sure it's accurate to use "topic space" in paragraph above.  These analyses were all conducted in the 2D manifold space
%DIF >  - Paxton
%DIF >  I added "reduced" before the first reference and second reference to topic space to make it clear the analyses were performed on the 2D space. - Andy

\DIFaddbegin \DIFadd{In addition to the more ``holistic'' measure of memory described in the previous section, our framework also affords the ability to drill down to individual words and quantify how each word relates to the  memorability of each event. The results displayed in Figures \ref{fig:list-learning}C and \ref{fig:precision-detail}A suggest that certain events were remembered better than others.  Given this, we next asked asked whether the events were generally remembered well or poorly tended to reflect particular content.  }\DIFaddend Because our analysis framework projects the dynamic video content and participants' recalls \DIFdelbegin \DIFdel{onto a shared topic }\DIFdelend \DIFaddbegin \DIFadd{into a shared }\DIFaddend space, and because the dimensions of that space \DIFdelbegin \DIFdel{are known (i.e., each topic dimension is a set }\DIFdelend \DIFaddbegin \DIFadd{represent topics (which are, in turn, sets }\DIFaddend of weights over words in the vocabulary\DIFdelbegin \DIFdel{; Fig.~\topics}\DIFdelend ), we \DIFdelbegin \DIFdel{can examine the topic trajectories to understand which specific content was remembered well (or poorly).For each video event, we can ask: what was the average correlation (across participants) between the video event's topic vectorand the closest matching recall event topic vectors from each participant? This yields a single correlation coefficient for each video event, describing how closely participants ' recalls of the event tended to reliably capture its content }\DIFdelend \DIFaddbegin \DIFadd{are able to recover the weighted combination of words that make up any point (i.e., topic vector) in this space.  We first computed the average precision with which participants recalled each of the 30 video events }\DIFaddend (Fig.~\ref{fig:topics}A\DIFdelbegin \DIFdel{).  (We also examined how different comparisons between each video event's topic vector and the corresponding recall event topic vectors related to hand-annotated characterizations of memory performance; see }\textit{\DIFdel{Supporting Information}}%DIFAUXCMD
\DIFdel{).  Given this summary of which events were recalled reliably (or not), we next asked whether the better-remembered or worse-remembered events tended to reflect particular topics.  We }\DIFdelend \DIFaddbegin \DIFadd{; note that this result is analogous to a serial position curve created from our continuous recall quality metric).  We then }\DIFaddend computed a weighted average of the topic vectors for each video event, where the weights reflected how reliably each event was recalled.  To visualize the result, we created a ``wordle'' image~\citep{MuelEtal18} where words weighted more heavily by better-remembered topics appear in a larger font (Fig.~\ref{fig:topics}B, green box).  \DIFdelbegin \DIFdel{Events }\DIFdelend \DIFaddbegin \DIFadd{Across the full video, content }\DIFaddend that reflected topics \DIFdelbegin \DIFdel{weighting heavily on characters like ``Sherlock'' and ``John'' (i.e., the main characters) and locations like ``221b Baker Street'' (i.e., a major recurring location }\DIFdelend \DIFaddbegin \DIFadd{necessary to convey the central focus of the video (e.g., the names of the two main characters, ``Sherlock" and ``John", }\DIFaddend and the address of \DIFdelbegin \DIFdel{the flat that Sherlock and John share}\DIFdelend \DIFaddbegin \DIFadd{a major recurring location, ``221B Baker Street"}\DIFaddend ) were best remembered.  An analogous analysis revealed which themes were poorly remembered.  Here in computing the weighted average over events' topic vectors\DIFaddbegin \DIFadd{, }\DIFaddend we weighted each event in \textit{inverse} proportion to how well it was remembered (Fig.~\ref{fig:topics}B, red box).  \DIFdelbegin \DIFdel{This revealed that events with }\DIFdelend \DIFaddbegin \DIFadd{The least well-remembered video content reflected information not necessary to later convey a general summary of the video, such as the proper names of }\DIFaddend relatively minor characters \DIFdelbegin \DIFdel{such as }\DIFdelend \DIFaddbegin \DIFadd{(e.g., }\DIFaddend ``Mike,\DIFdelbegin \DIFdel{'' ``Jeffrey,'' and ``Molly,'' as well as less-integral plot }\DIFdelend \DIFaddbegin \DIFadd{" ``Molly," and ``Lestrade") and }\DIFaddend locations (e.g., ``\DIFdelbegin \DIFdel{hospital'' and ``office'') were least well-remembered. This suggests that what is retained in memory are the major plot elements (i.e., the overall ``gist'' of what happened), whereas the more minor details are prone to pruning.
}\DIFdelend \DIFaddbegin \DIFadd{St. Bartholomew's Hospital").
%DIF > , as well as the brief, animated clip participants viewed at the beginning of each of the two scan session (involving ``singing" ``cartoon" characters).
}\DIFaddend 

\DIFdelbegin \DIFdel{In addition to constructing overall summaries, assessing the video and recall topic vectors from individual recalls can provide further insights.Specifically, for any given event we can construct }\DIFdelend \DIFaddbegin \begin{figure}[tp]
\centering
\includegraphics[width=1\textwidth]{figs/topics}
\caption{\small \textbf{\DIFaddFL{Language used in the most and least memorable events.}} \textbf{\DIFaddFL{A.}} \DIFaddFL{Average precision (video event-recall event topic vector correlation) across participants for each video event.  Error bars denote bootstrap-derived across-participant 95\% confidence intervals.  The stars denote the three best-remembered events (green) and worst-remembered events (red).  }\textbf{\DIFaddFL{B.}} \DIFaddFL{Wordles comprising the top 200 highest-weighted words reflected in the weighted-average topic vector across video events.  Green: video events were weighted by how well the topic vectors derived from recalls of those events matched the video events' topic vectors (Panel A).  Red: video events were weighted by the inverse of how well their topic vectors matched the recalled topic vectors.  }\textbf{\DIFaddFL{C.}}  \DIFaddFL{The set of all video and recall events is projected onto the two-dimensional space derived in Figure~\ref{fig:trajectory}.  The dots outlined in black denote video events (dot size reflects the average correlation between the video event's topic vector and the topic vectors from the closest matching recalled events from each participant; bigger dots denote stronger correlations).  The dots without black outlines denote recalled events.  All dots are colored using the same scheme as Figure~\ref{fig:trajectory}A.  Wordles for several example events are displayed (green: three best-remembered events; red: three worst-remembered events).  Within each circular wordle, the left side displays words associated with the topic vector for the video event, and the right side displays words associated with the (average) recall event topic vector, across all recall events matched to the given video event.}}
\label{fig:topics}
\end{figure}

\DIFadd{A similar result emerged from assessing the topic vectors for individual video and recall events (Fig.~\ref{fig:topics}C).  Here, for each of the three best- and worst-remembered video events, we have constructed }\DIFaddend two wordles: one from the original video event's topic vector \DIFdelbegin \DIFdel{, }\DIFdelend \DIFaddbegin \DIFadd{(left) }\DIFaddend and a second from the average \DIFdelbegin \DIFdel{topic vectors produced by all participants' recalls of that event.  We can then examine those wordles visually to gain an intuition for which aspects of the video event were recapitulated in participants' recalls of that event .  Several example wordles are displayed in Figure~\ref{fig:topics}C (wordles from the }\DIFdelend \DIFaddbegin \DIFadd{recall topic vector for that event (right).  The }\DIFaddend three best-remembered events \DIFdelbegin \DIFdel{are }\DIFdelend \DIFaddbegin \DIFadd{(}\DIFaddend circled in green\DIFdelbegin \DIFdel{; wordles from the }\DIFdelend \DIFaddbegin \DIFadd{) correspond to scenes important to the central plot-line: a mysterious figure spying on John in a phone booth; John meeting Sherlock at Baker St. to discuss the murders; and Sherlock laying a trap to catch the killer.  Meanwhile, the }\DIFaddend three worst-remembered events \DIFdelbegin \DIFdel{are }\DIFdelend \DIFaddbegin \DIFadd{(}\DIFaddend circled in red) \DIFdelbegin \DIFdel{.  Using wordles to visually compare the topical content of each video event and the (average) corresponding recall event reveals the specific content from the specific events that is reliably retained in the transformation into memory (green events) or not (red events)}\DIFdelend \DIFaddbegin \DIFadd{reflect scenes that are non-essential to summarizing the narrative's structure: the video of singing cartoon characters participants viewed prior to the main episode; John asking Molly about Sherlock's habit of over-analyzing people; and Sherlock noticing evidence of Anderson's and Donovan's affair}\DIFaddend .

\DIFdelbegin %DIFDELCMD < \begin{figure}[tp]
%DIFDELCMD < \centering
%DIFDELCMD < \includegraphics[width=1\textwidth]{figs/topics}
%DIFDELCMD < %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
%DIFDELCMD < \small %%%
\textbf{\DIFdelFL{Transforming experience into memory.}} %DIFAUXCMD
\textbf{\DIFdelFL{A.}} %DIFAUXCMD
\DIFdelFL{Average correlations (across participants) between the topic vectors from each video event and the closest-matching recall events.  Error bars denote bootstrap-derived across-participant 95\% confidence intervals.  The stars denote the three best-remembered events (green) and worst-remembered events (red).  }\textbf{\DIFdelFL{B.}} %DIFAUXCMD
\DIFdelFL{Wordles comprising the top 200 highest-weighted words reflected in the weighted-average topic vector across video events.  Green: video events were weighted by how well the topic vectors derived from recalls of those events matched the video events' topic vectors (Panel A).  Red: video events were weighted by the inverse of how well their topic vectors matched the recalled topic vectors.  }\textbf{\DIFdelFL{C.}}  %DIFAUXCMD
\DIFdelFL{The set of all video and recall events is projected onto the two-dimensional space derived in Figure~\ref{fig:trajectory}.  The dots outlined in black denote video events (dot size reflects the average correlation between the video event's topic vector and the topic vectors from the closest matching recalled events from each participant; bigger dots denote stronger correlations).  The dots without black outlines denote recalled events.  All dots are colored using the same scheme as Figure~\ref{fig:trajectory}A.  Wordles for several example events are displayed (green: three best-remembered events; red: three worst-remembered events).  Within each circular wordle, the left side displays words associated with the topic vector for the video event, and the right side displays words associated with the (average) recall event topic vector, across all recall events matched to the given video event.}}
%DIFAUXCMD
%DIFDELCMD < \label{fig:topics}
%DIFDELCMD < \end{figure}
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend The results thus far inform us about which aspects of the dynamic content in the episode participants watched were preserved or altered in participants' memories\DIFdelbegin \DIFdel{of the episode}\DIFdelend .  We next carried out a series of analyses aimed at understanding which brain structures might \DIFdelbegin \DIFdel{implement these processes.  In one analysis}\DIFdelend \DIFaddbegin \DIFadd{facilitate these preservations and transformations between the external world and memory.  In the first analysis, }\DIFaddend we sought to identify \DIFdelbegin \DIFdel{which brain structures }\DIFdelend \DIFaddbegin \DIFadd{brain structures that }\DIFaddend were sensitive to the \DIFaddbegin \DIFadd{dynamic unfolding of the }\DIFaddend video's \DIFdelbegin \DIFdel{dynamic }\DIFdelend content, as characterized by its topic trajectory.  \DIFdelbegin \DIFdel{Specifically, we }\DIFdelend \DIFaddbegin \DIFadd{We }\DIFaddend used a searchlight procedure to identify \DIFdelbegin \DIFdel{the extent to which each cluster of voxels exhibited a timecourse (as the }\DIFdelend \DIFaddbegin \DIFadd{clusters of voxels whose activity patterns displayed a proximal temporal correlation structure (as }\DIFaddend participants watched the video) \DIFdelbegin \DIFdel{whose temporal correlation matrix matched the temporal correlation matrix }\DIFdelend \DIFaddbegin \DIFadd{matching that }\DIFaddend of the original video's topic \DIFdelbegin \DIFdel{proportion matrix }\DIFdelend \DIFaddbegin \DIFadd{proportions }\DIFaddend (Fig.~\DIFdelbegin \DIFdel{\ref{fig:model}B).  As shown in Figure~\ref{fig:brainz}A, the analysis revealed a network of regions including bilateral frontal cortex and cingulate cortex, suggesting that these regions may play a role in maintaining information relevant to the narrative structure of the video}\DIFdelend \DIFaddbegin \DIFadd{\ref{fig:brainz}A; see }\textit{\DIFadd{Methods}} \DIFadd{for additional details)}\DIFaddend .  In a second analysis, we sought to identify \DIFdelbegin \DIFdel{which brain structures ' responses (while viewing the video }\DIFdelend \DIFaddbegin \DIFadd{brain structures whose responses (during video viewing}\DIFaddend ) reflected how each participant would later \DIFdelbegin \textit{\DIFdel{recall}} %DIFAUXCMD
\DIFdelend \DIFaddbegin \DIFadd{structure their recounting of }\DIFaddend the video.  We used an analogous searchlight procedure to identify clusters of voxels whose \DIFaddbegin \DIFadd{proximal }\DIFaddend temporal correlation matrices \DIFdelbegin \DIFdel{reflected the temporal correlation matrix }\DIFdelend \DIFaddbegin \DIFadd{matched that }\DIFaddend of the topic proportions for each individual's \DIFdelbegin \DIFdel{recalls }\DIFdelend \DIFaddbegin \DIFadd{recall }\DIFaddend (Figs.~\DIFdelbegin \DIFdel{\ref{fig:model}D, \corrmats)}\DIFdelend \DIFaddbegin \DIFadd{\ref{fig:brainz}B; see }\textit{\DIFadd{Methods}} \DIFadd{for additional details).  To ensure our searchlight procedure identified regions }\textit{\DIFadd{specifically}} \DIFadd{sensitive to the temporal structure of the video or recalls (i.e., rather than those with a temporal autocorrelation length similar to that of the video/recalls), we performed a phase shift-based permutation correction (see }\textit{\DIFadd{Methods}} \DIFadd{for additional details)}\DIFaddend . As shown in Figure~\ref{fig:brainz}\DIFdelbegin \DIFdel{B, the }\DIFdelend \DIFaddbegin \DIFadd{C, the video-driven searchlight }\DIFaddend analysis revealed a \DIFaddbegin \DIFadd{distributed }\DIFaddend network of regions \DIFdelbegin \DIFdel{including the ventromedial prefrontal cortex (vmPFC), anterior cingulate cortex, and right medial temporal lobe (rMTL) , suggesting that these regions may play a role in transforming each individual}\DIFdelend \DIFaddbegin \DIFadd{that may play a role in processing information relevant to the narrative structure of the video.  Similarly, the recall-driven searchlight analysis revealed a second network of regions (Fig.~\ref{fig:brainz}D) that may facilitate a person-specific transformation of one}\DIFaddend 's experience into memory.  In identifying regions whose responses to ongoing experiences reflect how those experiences will be remembered later, this latter analysis extends classic \textit{subsequent memory analyses}~\citep[e.g.,][]{PallWagn02} to domain of naturalistic stimuli.

\DIFdelbegin %DIFDELCMD < \begin{figure}[tp]
%DIFDELCMD < %%%
\DIFdelendFL \DIFaddbeginFL \begin{figure}[t]
\DIFaddendFL \centering
\includegraphics[width=1\textwidth]{figs/searchlights}
\caption{\small \textbf{Brain structures that underlie the transformation of experience into memory.} \textbf{A.} We \DIFdelbeginFL \DIFdelFL{searched for regions whose responses (as participants watched }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{isolated }\DIFaddendFL the \DIFdelbeginFL \DIFdelFL{video) matched }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{proximal diagonals from }\DIFaddendFL the \DIFdelbeginFL \DIFdelFL{temporal correlation matrix }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{upper triangle }\DIFaddendFL of the video \DIFdelbeginFL \DIFdelFL{topic proportions.  These regions are sensitive }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{correlation matrix, and applied this same diagonal mask }\DIFaddendFL to the \DIFdelbeginFL \DIFdelFL{narrative structure }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{voxel response correlation matrix for each cube }\DIFaddendFL of \DIFaddbeginFL \DIFaddFL{voxels in }\DIFaddendFL the \DIFdelbeginFL \DIFdelFL{video}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{brain}\DIFaddendFL . \DIFdelbeginFL \textbf{\DIFdelFL{B.}} %DIFAUXCMD
\DIFdelendFL We \DIFaddbeginFL \DIFaddFL{then }\DIFaddendFL searched for \DIFaddbeginFL \DIFaddFL{brain }\DIFaddendFL regions whose \DIFdelbeginFL \DIFdelFL{responses (as participants watched }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{activation timeseries consistently exhibited a similar proximal correlational structure to }\DIFaddendFL the video \DIFdelbeginFL \DIFdelFL{) matched }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{model, across participants.  }\textbf{\DIFaddFL{B.}} \DIFaddFL{We used dynamic time warping \mbox{%DIFAUXCMD
\citep{BernClif94} }\hspace{0pt}%DIFAUXCMD
to align each participant's recall timeseries to }\DIFaddendFL the \DIFdelbeginFL \DIFdelFL{temporal correlation matrix }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{TR timeseries }\DIFaddendFL of the \DIFdelbeginFL \DIFdelFL{topic proportions derived from }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{video.  We then applied the same diagonal mask used in Panel A to isolate the proximal temporal correlations and searched for brain regions whose activation timeseries for an individual consistently exhibited a similar proximal correlational structure to }\DIFaddendFL each individual's \DIFdelbeginFL \DIFdelFL{later }\DIFdelendFL recall\DIFdelbeginFL \DIFdelFL{of video}\DIFdelendFL .  \DIFdelbeginFL \DIFdelFL{These }\DIFdelendFL \DIFaddbeginFL \textbf{\DIFaddFL{C.}} \DIFaddFL{We identified a network of }\DIFaddendFL regions \DIFdelbeginFL \DIFdelFL{are }\DIFdelendFL sensitive to \DIFdelbeginFL \DIFdelFL{how }\DIFdelendFL the narrative structure of \DIFdelbeginFL \DIFdelFL{the video }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{participants' ongoing experience.  The map shown }\DIFaddendFL is \DIFdelbeginFL \DIFdelFL{transformed into }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{thresholded at at $p < 0.05$, corrected.  }\textbf{\DIFaddFL{D}}\DIFaddFL{. We also identified }\DIFaddendFL a \DIFdelbeginFL \DIFdelFL{memory of }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{network or regions sensitive to how individuals would later structure }\DIFaddendFL the video\DIFaddbeginFL \DIFaddFL{'s content in their recalls}\DIFaddendFL .  \DIFdelbeginFL \DIFdelFL{Both panels: the maps are }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{The map shown is }\DIFaddendFL thresholded at \DIFaddbeginFL \DIFaddFL{at }\DIFaddendFL $p < 0.05$, corrected.}
\label{fig:brainz}
\end{figure}

\DIFaddbegin \DIFadd{The searchlight analyses described above yielded two distributed networks of brain regions, whose activity timecourses mirrored to the temporal structure of the video (Fig.~\ref{fig:brainz}C) or participants' eventual recalls (Fig.~\ref{fig:brainz}D).  We next sought to gain greater insight into the structures and functional networks our results reflected.  To accomplish this, we performed an additional, exploratory analysis using }\texttt{\DIFadd{Neurosynth}} \DIFadd{\mbox{%DIFAUXCMD
\citep{YarkEtal11}}\hspace{0pt}%DIFAUXCMD
.  Given an arbitrary statistical map as input, }\texttt{\DIFadd{Neurosynth}} \DIFadd{performs a massive automated meta-analysis, returning a ranked list of terms reported in papers with similar significance maps. We ran }\texttt{\DIFadd{Neurosynth}} \DIFadd{on the significance maps for the video- and recall-driven searchlight analyses. These maps, along with the 10 terms with maximally similar meta-analysis images identified by }\texttt{\DIFadd{Neurosynth}} \DIFadd{are shown in Figure \ref{fig:neurosynth}.
}

\begin{figure}[t]
\centering
\includegraphics[width=1\textwidth]{figs/neurosynth_decoding}
\caption{\small \textbf{\DIFaddFL{Decoding distributed statistical maps via Neurosynth meta-analyses.}} \textbf{\DIFaddFL{A.}} \DIFaddFL{Video-searchlight significance and top 10 decoded terms.  We constructed a map of the permutation-derived $p$-values for the video-driven searchlight analysis (Fig.~\ref{fig:brainz}A, C) at each voxel with a positive permutation-derived $z$-score.  The top 10 terms decoded from this significance map are shown in red.  }\textbf{\DIFaddFL{B.}} \DIFaddFL{Recall-searchlight significance and top 10 decoded terms.  We constructed a map of the permutation-derived $p$-values for the recall-driven searchlight analysis (Fig.~\ref{fig:brainz}A, C) at each voxel with a positive permutation-derived $z$-score.  The top 10 terms decoded from this significance map are shown in blue.}}
\label{fig:neurosynth}
\end{figure}
\FloatBarrier


\DIFaddend \section*{Discussion}
\label{sec:discussion}

Our work casts remembering as reproducing (behaviorally and neurally) the topic trajectory, or \DIFdelbegin \DIFdel{``gist, '' of the original }\DIFdelend \DIFaddbegin \DIFadd{shape, of an }\DIFaddend experience.  This view draws inspiration from prior work aimed at elucidating the neural and behavioral underpinnings of how we process dynamic naturalistic experiences and remember them later.  One approach to identifying neural responses to naturalistic stimuli (including experiences) entails building a model of the stimulus and searching for brain regions whose responses are consistent with the model.  In prior work, a series of studies from Uri Hasson's group~\citep{LernEtal11, SimoEtal16, ChenEtal17, BaldEtal17, ZadbEtal17} have extended this approach with a clever twist\DIFdelbegin \DIFdel{.  Rather }\DIFdelend \DIFaddbegin \DIFadd{: rather }\DIFaddend than building an explicit stimulus model, these studies instead search for brain responses (while experiencing the stimulus) that are reliably similar across individuals.  So called \textit{inter-subject correlation} (ISC) and \textit{inter-subject functional connectivity} (ISFC) analyses effectively treat other people's brain responses to the stimulus as a ``model'' of how its features change over time.  By contrast, in our present work\DIFdelbegin \DIFdel{we used topic models and HMMs }\DIFdelend \DIFaddbegin \DIFadd{, we use topic models }\DIFaddend to construct an explicit \DIFdelbegin \DIFdel{stimulus model }\DIFdelend \DIFaddbegin \DIFadd{content model directly from the stimulus }\DIFaddend (i.e., the topic trajectory of the video).  \DIFdelbegin \DIFdel{When we searched for brain structures whose responses are consistent with the video's topic trajectory, we identified a network of structures that overlapped strongly with the ``long temporal receptive window'' network reported by the Hasson group~\mbox{%DIFAUXCMD
\citep[e.g., compare our Fig.~\ref{fig:brainz}A with the map of long temporal receptive window voxels in][]{LernEtal11}}\hspace{0pt}%DIFAUXCMD
.  This provides support for the notion that part of the long temporal receptive window network may be maintaining an explicit model }\DIFdelend \DIFaddbegin \DIFadd{Projecting each participant's recall into a space shared by both the stimulus and other participants then allows us to compare recalls both directly to the stimulus and to each other.  Similarly, prior work introducing the use of HMMs to discover latent event structure in naturalistic stimuli and recall \mbox{%DIFAUXCMD
\citep{BaldEtal17} }\hspace{0pt}%DIFAUXCMD
used between-subjects cross-validation to identify event boundaries shared across participants, and between stimulus and recall.  Our framework allows us to break from the restriction of a common, shared event-timeseries and identify the unique }\textit{\DIFadd{resolution}} \DIFadd{of each participant's recall event structure, and how that may differ from the video and that of other participants.
%DIF >  @JEREMY @ANDY PLEASE CHECK THIS TO MAKE SURE I'M NOT MISCHARACTERIZING BALDETAL17
}

\DIFadd{While a large number of language models exist \mbox{%DIFAUXCMD
\citep[e.g., WAS, LSA, word2vec, universal sentence encoder;][]{SteyEtal04, LandEtal98, MikoEtal13, CerEtal18}}\hspace{0pt}%DIFAUXCMD
, here we use latent dirichlet allocation (LDA)-based topic models for a few reasons.  First, topic models capture the }\textit{\DIFadd{essence}} \DIFadd{of a text passage devoid of the specific set and order of words used.  This was an important feature of our model since different people may accurately recall a scene using very different language. Second, words can mean different things in different contexts (e.g. ``bat" may be the act of hitting a baseball, the object used for that action, or as a flying mammal).  Topic models are robust to this, allowing words to exist as part of multiple topics.  Last, topic models provide a straightforward means to recover the weights for the particular words comprising a topic, enabling easy interpretation of an event's contents (e.g. Fig.~\ref{fig:topics}). Other models such as Google's Universal Sentence Encoder offer a context-sensitive encoding of text passages, but the encoding space is complex and non-linear, and thus recovering the original words used to fit the model is not straightforward. However, it's worth pointing out that our framework is divorced from the particular choice of language model. Moreover, many of the aspects of our framework could be swapped out for other choices. For example, the language model, the timeseries segmentation model and the video-recall matching function could all be customized for the particular problem. Indeed for some problems, recovery of the particular recall words may not be necessary, and thus other text-modeling approaches (such as universal sentence encoder) may be preferable. Future work will explore the influence of particular model choices on the framework's efficacy.
}

\DIFadd{In extending classical free recall analyses to our naturalistic memory framework, we recovered two patterns of recall dynamics central to list-learning studies: a heightened probability of initiating recall with the first presented ``item" (in our case, video events; Fig.~\ref{fig:list-learning}A) and a strong bias toward transitioning from recalling a given event to recalling the one immediately following it (Fig.~\ref{fig:list-learning}B).  However, equally noteworthy are the typical free recall results }\textit{\DIFadd{not}} \DIFadd{recovered in these analyses, as each highlights a fundamental difference between the list-learning paradigm and naturalistic memory paradigms like the one employed in the present study.  The most noticeable departure from hallmark free recall dynamics in these findings is the apparent lack of a serial position effect in Figure \ref{fig:list-learning}C, which instead shows greater and lesser recall probabilities for events distributed across the video.  Stimuli in free recall experiments most often comprise lists of simple, common words, presented to participants in a random order. \mbox{%DIFAUXCMD
\citep[In fact, numerous word pools have been developed based on these criteria; e.g.,][]{FrieEtal82}}\hspace{0pt}%DIFAUXCMD
.  These stimulus qualities enable two assumptions that are central to word list analyses, but frequently do not hold for real-world experiences.  First, researchers conducting list-learning studies may assume that the content at each presentation index is essentially equal, and does not possess attributes that would render it, on average, more or less memorable than others.  Such is rarely the case with real-world experiences or experiments meant to approximate them, and the effects of both intrinsic and observer-dependent factors on stimulus memorability are well established \mbox{%DIFAUXCMD
\citep[for review see][]{ChunTurk07, ByliEtal15, TyngEtal17}}\hspace{0pt}%DIFAUXCMD
.  Second, the random ordering of list items ensures that (across participants, on average) there is no relationship between the thematic similarity of individual stimuli and their presentation positions---in other words, two successively presented items are no more likely to be highly semantically similar than they are to be highly dissimilar.  In most cases, the exact opposite is true of real-world episodes.  Our internal thoughts, our actions, and the physical state of the world around us all tend to follow a direct, causal progression.  As a result, each moment of our experience tends to be inherently more similar to surrounding moments than to those in the distant past or future.  Memory literature has termed this strong temporal autocorrelation ``context," and in various media that depict real-world events (e.g., movies or written stories), we recognize it as a }\textit{\DIFadd{narrative structure}}\DIFadd{.  While a random word list (by definition) has no such structure, the logical progression between ideas and actions in a naturalistic stimulus prompts the rememberer to recount presented events in order, starting with the beginning.  This tendency is reflected in our findings' second departure from typical free recall dynamics: a lack of increased probability of first recall for end-of-sequence events (Fig.~\ref{fig:list-learning}A).
}

\DIFadd{Because they disregard presentation order-dependent variability in the stimulus content, analyses such as those in Figure \ref{fig:list-learning} enable a more sensitive analysis of presentation order-dependent temporal dynamics in free recall. Yet by the same token, they paint a wholly incomplete picture of memory for naturalistic episodes.  In an attempt to address this shortcoming, we have developed a framework in the present study that characterizes the explicit semantic content }\DIFaddend of the stimulus \DIFdelbegin \DIFdel{dynamics.  When we performed a similar analysis after swapping out the video's topic trajectory with the recall topic trajectories of each individual participant, this allowed us to identify }\DIFdelend \DIFaddbegin \DIFadd{and subsequent recalls.  However, sensitivity to stimulus and recall content introduces a new challenge: distinguishing between levels of recall quality for a stimulus (e.g., an event) that is considered to have been ``remembered."  When modeling memory in an experimental setting, recall quality for individual events is often cast as binary (e.g., a given list item was simply either remembered or not remembered).  Various models of memory (e.g., \mbox{%DIFAUXCMD
\citealp{Yone02}}\hspace{0pt}%DIFAUXCMD
) attempt to improve upon this by including confidence ratings, rendering this binary judgement instead categorical.  To better evaluate naturalistic memory quality, we introduce a continuous metric (}\textit{\DIFadd{precision}}\DIFadd{), which reflects the level of completeness of a participant's recall for a feature-rich experience.  Additionally, recall quality for a single event is typically assessed independently from that for all other events (e.g., it is difficult to ``compare" a participant's binary recall success for list item 1 to that of list item 10).  The second novel metric we introduce (}\textit{\DIFadd{distinctiveness}}\DIFadd{) is based on analyzing of the correlational structure of an individual's full set of recall events, and reflects the specificity of their memory for a single experienced event.  We find that both of these metrics relate to the overall number of video events participants successfully recalled, and that our precision metric additionally relates to \mbox{%DIFAUXCMD
\cite{ChenEtal17}}\hspace{0pt}%DIFAUXCMD
's hand-annotated memory memory scores.  Though we do not find participants' average recall distinctiveness related to the hand-annotated memory scores, this is not entirely surprising given the divergence of behavior they capture.  In hand-scoring each participant's verbal recall for each of 50 (manually-deliniated) scenes, ``}[\DIFadd{a}] \DIFadd{scene was counted as recalled if the participant described any part of the scene" \mbox{%DIFAUXCMD
\citep{ChenEtal17}}\hspace{0pt}%DIFAUXCMD
.  In other words, both an extensive description of a scene's content and a brief mention of some subset of its content were (binarily) considered equally successful recalls.  By contrast, we identify the event structure in participants' recalls in an unsupervised manner, independent of the video event-timeseries, prior to mapping between video and recall content.  Our HMM-based event-segmentation produces boundaries between timepoints where the topic proportions shift in a substantial way, and because a small handful of words is unlikely to contribute significantly to the topic proportions for any sliding window, such brief scene descriptions will most often not begat a sufficiently large shift in the resulting topic proportions for the HMM to identify an event boundary.  Instead, they will be grouped with a neighboring event, consequently lowering that event's distinctiveness score and by extension, the participant's overall distinctiveness score.  This is in essence the qualitative difference between distinctive and indistinctive recall, and reflects the comparison shown in Figure \ref{fig:distinctiveness-detail}C.  Intriguingly, prior studies show that pattern separation, or the ability to cleanly discriminate between similar experiences, is impaired in many cognitive disorders as well as natural aging \mbox{%DIFAUXCMD
\citep{StarEtal10, YassEtal11c, YassStar11b}}\hspace{0pt}%DIFAUXCMD
.  Future work might explore whether and how these metrics compare between cognitively impoverished groups and healthy controls.
}

\DIFadd{In the analyses outlined in Figure \ref{fig:brainz}, we identified two networks of }\DIFaddend brain regions whose responses \DIFdelbegin \DIFdel{(as the participants viewed the video ) reflected how }\DIFdelend \DIFaddbegin \DIFadd{during video viewing were consistent with the temporal structure of the video and recall topic trajectories, respectively. The network identified by }\DIFaddend the video trajectory \DIFdelbegin \DIFdel{would be transformed in memory (as reflected by the recall topic trajectories). The analysis revealed that the rMTL and vmPFC }\DIFdelend \DIFaddbegin \DIFadd{analysis included the ventromedial prefrontal cortex, left anterior temporal lobe, superior parietal and dorsal anterior cingulate cortex. The network from the video-recall trajectory analysis also included the ventromedial prefrontal and superior parietal cortices, in addition to the posterior medial cortex (PMC) and the inferior temporal regions. Notably, \mbox{%DIFAUXCMD
\cite{ChenEtal17} }\hspace{0pt}%DIFAUXCMD
also observed the PMC in a number of analyses including one that searched for regions whose activity patterns during encoding were reinstated during free recall. The PMC has been consistently identified in studies involving stimuli with meaningfully structured events \mbox{%DIFAUXCMD
\cite{CohnRang17}}\hspace{0pt}%DIFAUXCMD
. Further, the PMC is part of the "posterior medial" system, a network of brain regions thought to represent situation models ~\mbox{%DIFAUXCMD
\cite{ZackEtal07} }\hspace{0pt}%DIFAUXCMD
in support of memory, spatial navigation and social cognition ~\mbox{%DIFAUXCMD
\citep{RangRitc12}}\hspace{0pt}%DIFAUXCMD
. Given that we constructed our video-recall searchlight model to capture temporal structure in the episode's semantic content (and how one's later recall aligns with that structure), we speculate that the PMC }\DIFaddend may play a role in \DIFdelbegin \DIFdel{this person-specific transformation from experience into memory.
The role of the MTL in episodic memory encoding has been well-reported~\mbox{%DIFAUXCMD
\citep[e.g., ][]{PallWagn02, DavaEtAl03, RangEtal04, Dava06}}\hspace{0pt}%DIFAUXCMD
.  Prior work has also implicated the medial prefrontal cortex in representing ``schema'' knowledge~\mbox{%DIFAUXCMD
\citep[i.e., general knowledge about the format of an ongoing experience given prior similar experiences; ][]{KestEtal12, SchlPres15, GilbMarl17, SpalEtal18}}\hspace{0pt}%DIFAUXCMD
.  Integrating across our study and this prior work, one interpretation is that the person-specific transformations mediated (or represented)by the rMTL and vmPFC may reflect schema knowledge being leveraged,formed,or updated, incorporating ongoing experienceinto previously acquired knowledge.  }\DIFdelend \DIFaddbegin \DIFadd{constructing mnemonic events from meaningfully structured experiences.
}\DIFaddend 


\DIFaddbegin \DIFadd{Decoding the associated significance maps with }\texttt{\DIFadd{Neurosynth}} \DIFadd{revealed two intriguing results.  First, the top 10 terms returned for the video-driven searchlight significance map were centered around themes of language and semantic meaning (Fig.~\ref{fig:neurosynth}A).  In other words, the voxels identified as more reflective of the video's temporal structure (i.e., voxels with lower permutation correction-derived $p$-values), as defined by our model, were most likely to be reported as active in studies focused on the the neural underpinnings of semantic processing.  This finding is interesting, as our model specifically captures the temporal structure of the video's }\textit{\DIFadd{semantic}} \DIFadd{content (e.g., as opposed to that of the visual, auditory, or affective content).  This suggests that the network of structures displayed in Figure \ref{fig:brainz}C may play a roll in processing the evolving semantic structure of ongoing experiences.
}


\DIFadd{Our second searchlight analysis identified a largely separate network of regions (Fig.~\ref{fig:brainz}D) whose patterns of activity as participants viewed the video reflected the idiosyncratic structure of each individual's later recall.  Decoding the associated significance map yielded a set of terms that primarily reflected names of specific structural regions (such as ``thalamus," ``anterior insula," ``anterior cingulate" and ``inferior frontal"; Fig.~\ref{fig:neurosynth}B).  Interestingly, these regions share membership in a common, large-scale functional network (termed the ``salience network") involved in detecting and processing affective cues.  In particular, the latter three regions have been implicated in functions relevant to assigning personal meaning to an experience, including: ascribing subjective value to raw, sensory input \mbox{%DIFAUXCMD
\citep{MedfCrit10}}\hspace{0pt}%DIFAUXCMD
; modulating semantic and phonological processing in response to personally salient stimuli \mbox{%DIFAUXCMD
\citep{KellEtal07b}}\hspace{0pt}%DIFAUXCMD
; and directing and reallocating attention and working memory resources towards the most relevant stimuli \mbox{%DIFAUXCMD
\citep{MenoUddi10}}\hspace{0pt}%DIFAUXCMD
.  This suggests that the network of structures displayed in Figure \ref{fig:brainz}D may be play a roll in transforming and restructuring ongoing experiences through the lens of one's own personal values as they are encoded in memory.
}

\DIFaddend Our work has broad implications for how we characterize and assess memory in real-world settings\DIFaddbegin \DIFadd{, }\DIFaddend such as the classroom or physician's office.  For example, the most commonly used classroom evaluation tools involve \DIFaddbegin \DIFadd{simply }\DIFaddend computing the proportion of correctly answered exam questions.  Our work indicates that this approach is only loosely related to what educators might really want to measure: how well did the students understand the key ideas presented in the course?  \DIFdelbegin \DIFdel{One could apply the computational framework we developed to construct topic trajectories for the video and participants' recalls }\DIFdelend \DIFaddbegin \DIFadd{Under this typical framework of assessment, the same exam score of 50\% could be ascribed to two very different students: one who attended the full course but struggled to learn more than a broad overview of the material, and one who attended only half of the course but understood the material perfectly.  Instead, one could apply our computational framework }\DIFaddend to build explicit content models of the course material and exam questions.  This approach would provide a more nuanced and specific view into which aspects of the material students had learned well (or poorly).  In clinical settings, memory measures that incorporate such explicit content models might also provide more direct evaluations of patients' memories.


\section*{Methods}
\label{sec:methods}

\subsection*{Experimental design and data collection}
Data were collected by \cite{ChenEtal17}.  In brief, participants (\DIFdelbegin \DIFdel{$n=17$}\DIFdelend \DIFaddbegin \DIFadd{$n=22$}\DIFaddend ) viewed the first 48 minutes of ``A Study in Pink'', the first episode of the BBC television series \textit{Sherlock}, while fMRI volumes were collected (TR = 1500~ms).  \DIFaddbegin \DIFadd{Participants were pre-screened to ensure they had never seen any episode of the show before.  }\DIFaddend The stimulus was divided into a 23~min (946~TR) and a 25~min (1030~TR) segment to mitigate technical issues related to the scanner.  After finishing the clip, participants were instructed to \citep[quoting from][]{ChenEtal17} ``describe what they recalled of the [episode] in as much detail as they could, to try to recount events in the original order they were viewed in, and to speak for at least 10 minutes if possible but that longer was better. They were told that completeness and detail were more important than temporal order, and that if at any point they realized they had missed something, to return to it. Participants were then allowed to speak for as long as they wished, and verbally indicated when they were finished (e.g., `Iâ€™m done').''  \DIFaddbegin \DIFadd{Five participants were dropped from the original dataset due to excessive head motion (2 participants), insufficient recall length (2 participants), or falling asleep during stimulus viewing (1 participant), resulting in a final sample size of $n=17$.  }\DIFaddend For additional details about the experimental procedure and scanning parameters\DIFaddbegin \DIFadd{, }\DIFaddend see \cite{ChenEtal17}.  The experimental protocol was approved by Princeton University's Institutional Review Board.

After preprocessing the fMRI data and warping the images into a standard (3~mm$^3$ MNI) space, the voxel activations were $z$-scored (within voxel) and spatially smoothed using a 6~mm (full width at half maximum) Gaussian kernel.  The fMRI data were also cropped so that all video-viewing data were aligned across participants.  This included a constant 3 TR (4.5~s) shift to account for the lag in the hemodynamic response.  \citep[All of these preprocessing steps followed][where additional details may be found.]{ChenEtal17}

\DIFaddbegin \DIFadd{The video stimulus was divided into 1,000 fine-grained ``scenes" and annotated by an independent coder.  For each of these 1,000 scenes, the following information was recorded: a brief narrative description of what was happening, the location where the scene took place, whether that location was indoors or outdoors, the names of all characters on-screen, the name(s) of the character(s) in focus in the shot, the name(s) of the character(s) currently speaking, the camera angle of the shot, a transcription of any text appearing on-screen, and whether or not there was music present in the background.  Each scene was also tagged with its onset and offset time, in both seconds and TRs.
}

\DIFaddend \subsection*{Data and code availability}
The fMRI data we analyzed are available online \href{http://dataspace.princeton.edu/jspui/handle/88435/dsp01nz8062179}{\underline{here}}.  The behavioral data and all of our analysis code may be downloaded \href{https://github.com/ContextLab/sherlock-topic-model-paper}{\underline{here}}.

\subsection*{Statistics}
All statistical tests \DIFdelbegin \DIFdel{we performed }\DIFdelend \DIFaddbegin \DIFadd{performed in the behavioral analyses }\DIFaddend were two-sided.  \DIFaddbegin \DIFadd{All statistical tests performed in the neural data analyses were two-sided, except for the permutation-based thresholding, which was one-sided.  In this case, we were specifically interested in identifying voxels whose activation time series reflected the temporal structure of the video and recall trajectories to a }\textit{\DIFadd{greater}} \DIFadd{extent than that of the phase-shifted trajectories.
%DIF >  ------ NOTE: ------
%DIF >  from the Nature Human Behavior submission requirements: "Specify whether tests were one- or two-tailed, and justify the use of one-tailed tests."
%DIF >  - Paxton
}\DIFaddend 

\subsection*{Modeling the dynamic content of the video and recall transcripts}
\subsubsection*{Topic modeling}
The input to the topic model we trained to characterize the dynamic content of the video comprised \DIFaddbegin \DIFadd{998 }\DIFaddend hand-generated annotations of \DIFdelbegin \DIFdel{each of 1000 }\DIFdelend \DIFaddbegin \DIFadd{short (mean: 2.96s) }\DIFaddend scenes spanning the video clip~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\citep[generated by][]{ChenEtal17}}\hspace{0pt}%DIFAUXCMD
.  The features included: narrative details (a sentence or two describing what happened in that scene); whether the scene took place indoors or outdoors; names of any characters that appeared in the scene; name(s) of characters in camera focus; name(s) of characters who were speaking in the scene; the location (in the story) that the scene took place; camera angle (close up, medium, long, top, tracking, over the shoulder, etc.); whether music was playing in the scene or not; and a transcription of any on-screen text}\DIFdelend \DIFaddbegin \DIFadd{(\mbox{%DIFAUXCMD
\citealp{ChenEtal17} }\hspace{0pt}%DIFAUXCMD
generated 1000 annotations total; we removed two referring to the break between the first and second scan sessions, during which no fMRI data was collected)}\DIFaddend .  We concatenated the text for all of \DIFdelbegin \DIFdel{these }\DIFdelend \DIFaddbegin \DIFadd{the annotated }\DIFaddend features within each segment, creating a ``bag of words'' describing each scene \DIFdelbegin \DIFdel{.  }\DIFdelend \DIFaddbegin \DIFadd{and performed some minor preprocessing (e.g., stemming possessive nouns and removing punctuation).  }\DIFaddend We then re-organized the text descriptions into overlapping sliding windows spanning \DIFaddbegin \DIFadd{(up to) }\DIFaddend 50 scenes each.  In other words, \DIFdelbegin \DIFdel{the first text sample comprised the combined text from the first 50 scenes}\DIFdelend \DIFaddbegin \DIFadd{we created a ``context" for each scene comprising the text descriptions of the preceding 25 scenes, the present scene, and the following 24 scenes.  To model the ``context" for scenes near the beginning and end of the video }\DIFaddend (i.e., \DIFdelbegin \DIFdel{1--50), the second comprised the text from scenes 2--51, and so on.
}\DIFdelend \DIFaddbegin \DIFadd{within 25 scenes of the beginning or end), we created overlapping sliding windows that grew in size from one scene to the full length, then similarly tapered their length at the end.  This additionally ensured that each scene's content was represented in the text corpus an equal number of times.
%DIF >  ------ NOTE: ------
%DIF >  need to refine explanation here a little bit
%DIF >  - Paxton
}

\DIFaddend We trained our model using these overlapping text samples with \texttt{scikit-learn}~\citep[version 0.19.1; ][]{PedrEtal11}, called from our high-dimensional visualization and text analysis software, \texttt{HyperTools}~\citep{HeusEtal18a}.  Specifically, we \DIFdelbegin \DIFdel{use }\DIFdelend \DIFaddbegin \DIFadd{used }\DIFaddend the \texttt{CountVectorizer} class to transform the text from each \DIFdelbegin \DIFdel{scene }\DIFdelend \DIFaddbegin \DIFadd{window }\DIFaddend into a vector of word counts (using the union of all words across all scenes as the ``vocabulary,'' excluding English stop words); this \DIFdelbegin \DIFdel{yields a number-of-scenes }\DIFdelend \DIFaddbegin \DIFadd{yielded a number-of-windows }\DIFaddend by number-of-words \textit{word count} matrix.  We then \DIFdelbegin \DIFdel{use }\DIFdelend \DIFaddbegin \DIFadd{used }\DIFaddend the \texttt{LatentDirichletAllocation} class (topics=100, method=`batch') to fit a topic model~\citep{BleiEtal03} to the word count matrix, yielding a \DIFdelbegin \DIFdel{number-of-scenes (1000}\DIFdelend \DIFaddbegin \DIFadd{number-of-windows (1047}\DIFaddend ) by number-of-topics (100) \textit{topic proportions} matrix.  The topic proportions matrix describes \DIFdelbegin \DIFdel{which }\DIFdelend \DIFaddbegin \DIFadd{the gradually evolving }\DIFaddend mix of topics (latent themes) \DIFdelbegin \DIFdel{is }\DIFdelend present in each scene.  Next, we transformed the topic proportions matrix to match the 1976 fMRI volume acquisition times.  \DIFdelbegin \DIFdel{For each fMRI volume, we took the topic proportions from whatever scene was displayed for most of that volume's 1500~ms acquisition time.  This yielded a new }\DIFdelend \DIFaddbegin \DIFadd{We assigned each topic vector to the timepoint (in seconds) midway between the beginning of the first scene and the end of the last scene in its corresponding sliding text window.  By doing so, we warped the linear temporal distance between consecutive topic vectors to align with the inconsistent temporal distance between consecutive annotations (whose durations varied greatly).  We then rescaled these timepoints to 1.5s TR units, and used linear interpolation to estimate a topic vector for each TR.  This resulted in a }\DIFaddend number-of-TRs (1976) by number-of-topics (100) \DIFdelbegin \DIFdel{topic proportions }\DIFdelend matrix.

We created similar topic proportions matrices using hand-annotated transcripts of each participant's recall of the video~\citep[annotated by ][]{ChenEtal17}.  We tokenized the transcript into a list of sentences, and then re-organized the list into overlapping sliding windows spanning \DIFaddbegin \DIFadd{(up to) }\DIFaddend 10 sentences each\DIFdelbegin \DIFdel{; in turn}\DIFdelend \DIFaddbegin \DIFadd{, analogously to how we parsed the video annotations.  In turn, }\DIFaddend we transformed each window's sentences into a word count vector (using the same vocabulary as for the video model)\DIFdelbegin \DIFdel{.  We }\DIFdelend \DIFaddbegin \DIFadd{, }\DIFaddend then used the topic model already trained on the video scenes to compute the most probable topic proportions for each sliding window.  This yielded a \DIFdelbegin \DIFdel{number-of-sentences }\DIFdelend \DIFaddbegin \DIFadd{number-of-windows }\DIFaddend (range: \DIFdelbegin \DIFdel{68--294}\DIFdelend \DIFaddbegin \DIFadd{83--312}\DIFaddend ) by number-of-topics (100) topic proportions matrix \DIFdelbegin \DIFdel{, }\DIFdelend for each participant.  These reflected the dynamic content of each participant's recalls.  Note: for details on how we selected the video and recall window lengths and number of topics, see \textit{Supporting Information} and Figure~\topicopt.


\subsubsection*{Parsing topic trajectories into events using Hidden Markov Models}
We parsed the topic trajectories of the video and participants' recalls into events using Hidden Markov Models~\citep{Rabi89}.  Given the topic proportions matrix (describing the mix of topics at each timepoint) and a number of states, $K$, an HMM recovers the set of state transitions that segments the timeseries into $K$ discrete states.  Following \cite{BaldEtal17}, we imposed an additional set of constraints on the discovered state transitions that ensured that each state was encountered exactly once (i.e., never repeated).  We used the BrainIAK toolbox~\citep{Brainiak} to implement this segmentation.

We used an optimization procedure to select the appropriate $K$ for each topic proportions matrix.  \DIFdelbegin \DIFdel{Specifically, we computed (for each matrix)
}\[
  \DIFdel{\argmax_K \left[\frac{a}{b} - \frac{K}{\alpha}\right],
}\]%DIFAUXCMD
\DIFdelend \DIFaddbegin \DIFadd{Prior studies on narrative structure and processing have shown that we both perceive and internally represent the world around us at multiple, hierarchical timescales (e.g., \mbox{%DIFAUXCMD
\citealp{HassEtal08, LernEtal11, HassEtal15, ChenEtal17, BaldEtal17, BaldEtal18}}\hspace{0pt}%DIFAUXCMD
).  However, for the purposes of our framework, we sought to identify the single timeseries of event-representations that is emphasized }\textit{\DIFadd{most heavily}} \DIFadd{in the temporal structure of the video and of each participant's recall.  We quantified this as the set of $K$ states that maximized the similarity between topic vectors for timepoints comprising each state, while minimizing the similarity between topic vectors for timepoints across different states.  Specifically, we computed (for each matrix)
}\[
  \DIFadd{\argmax_K \left[W_{1}(a, b)\right],
}\]\DIFaddend 
where $a$ was the \DIFdelbegin \DIFdel{average correlation between the topic vectors of timepoints within the same state; }\DIFdelend \DIFaddbegin \DIFadd{distribution of within-state topic vector correlations, and }\DIFaddend $b$ was the \DIFdelbegin \DIFdel{average correlation between the topic vectors of timepoints within }\textit{\DIFdel{different}} %DIFAUXCMD
\DIFdel{states; and $\alpha$ was a regularization parameter that we set to 5 times the window length (i.  e., 250 scenes for the video topic trajectory and }\DIFdelend \DIFaddbegin \DIFadd{distribution of across-state topic vector correlations .  We computed the first Wasserstein distance ($W_{1}$; also known as ``earth mover's distance"; \mbox{%DIFAUXCMD
\citealp{Dobr70, RamdEtal17}}\hspace{0pt}%DIFAUXCMD
) between these distributions for a large range of possible $K$-values (range }[\DIFadd{2,}\DIFaddend 50\DIFdelbegin \DIFdel{sentences for the recall topic trajectories) }\DIFdelend \DIFaddbegin ]\DIFadd{), and selected the $K$ that yielded the maximum value}\DIFaddend .  Figure~\ref{fig:model}B displays the event boundaries returned for the video, and Figure~\corrmats~displays the event boundaries returned for each participant's recalls.  \DIFaddbegin \DIFadd{See Figure \kopt~for the optimization functions for the video and recalls.  }\DIFaddend After obtaining these event boundaries, we created stable estimates of \DIFdelbegin \DIFdel{each topic proportions matrix }\DIFdelend \DIFaddbegin \DIFadd{the content represented in each event }\DIFaddend by averaging the topic vectors \DIFdelbegin \DIFdel{within each event}\DIFdelend \DIFaddbegin \DIFadd{across timepoints between each pair of event boundaries}\DIFaddend .  This yielded a number-of-events by number-of-topics matrix for the video and recalls from each participant.

\DIFaddbegin \subsubsection*{\DIFadd{Naturalistic extensions of classic list-learning analyses}}
\DIFadd{In traditional list-learning experiments, participants view a list of items (e.g., words) and then recall the items later.  Our video-recall event matching approach affords us the ability to analyze memory in a similar way. The video and recall events can be treated analogously to studied and recalled ``items'' in a list-learning study.  We can then extend classic analyses of memory performance and dynamics (originally designed for list-learning experiments) to the more naturalistic video recall task used in this study.
}

\DIFadd{Perhaps the simplest and most widely used measure of memory performance is }\textit{\DIFadd{accuracy}}\DIFadd{---i.e., the proportion of studied (experienced) items (in this case, video events) that the participant later remembered.  \mbox{%DIFAUXCMD
\cite{ChenEtal17} }\hspace{0pt}%DIFAUXCMD
used this method to rate each participant's memory quality by computing the proportion of (50, manually identified) scenes mentioned in their recall.  We found a strong across-participants correlation between these independent ratings and the proportion of (30, HMM-identified) video events matched to participants' recalls (Pearson's $r(15) = 0.71, p = 0.002$).  We further considered a number of more nuanced memory performance measures that are typically associated with list-learning studies.  We also provide a software package, }\texttt{\DIFadd{Quail}}\DIFadd{, for carrying out these analyses~\mbox{%DIFAUXCMD
\citep{HeusEtal17b}}\hspace{0pt}%DIFAUXCMD
.
}

\paragraph{\DIFadd{Probability of first recall (PFR).}}  \DIFadd{PFR curves~\mbox{%DIFAUXCMD
\citep{WelcBurn24, PostPhil65, AtkiShif68} }\hspace{0pt}%DIFAUXCMD
reflect the probability that an item will be recalled first as a function of its serial position during encoding. To carry out this analysis, we initialized a number-of-participants (17) by number-of-video-events (30) matrix of zeros. Then for each participant, we found the index of the video event that was recalled first (i.e., the video event whose topic vector was most strongly correlated with that of the first recall event) and filled in that index in the matrix with a 1.  Finally, we averaged over the rows of the matrix, resulting in a 1 by 30 array representing the proportion of participants that recalled an event first, as a function of the order of the event's appearance in the video (Fig.~\ref{fig:list-learning}A).
%DIF >  ------ NOTE: ------
%DIF >  reiterate meaning of error ribbons in list-learning figure? (already noted in figure caption)
%DIF >  - Paxton
}

\paragraph{\DIFadd{Lag conditional probability curve (lag-CRP).}} \DIFadd{The lag-CRP curve~\mbox{%DIFAUXCMD
\citep{Kaha96} }\hspace{0pt}%DIFAUXCMD
reflects the probability of recalling a given item after the just-recalled item, as a function of their relative encoding positions (or }\textit{\DIFadd{lag}}\DIFadd{).  In other words, a lag of 1 indicates that a recalled item was presented immediately after the previously recalled item, and a lag of -3 indicates that a recalled item came 3 items before the previously recalled item.  For each recall transition (following the first recall), we computed the lag between the current recall event and the next recall event, normalizing by the total number of possible transitions.  This yielded a number-of-participants (17) by number-of-lags (-29 to +29; 61 lags total) matrix. We averaged over the rows of this matrix to obtain a group-averaged lag-CRP curve (Fig.~\ref{fig:list-learning}B).
}

\paragraph{\DIFadd{Serial position curve (SPC).}} \DIFadd{SPCs~\mbox{%DIFAUXCMD
\citep{Murd62a} }\hspace{0pt}%DIFAUXCMD
reflect the proportion of participants that remember each item as a function of the items' serial positions during encoding. We initialized a number-of-participants (17) by number-of-video-events (30) matrix of zeros. Then, for each recalled event, for each participant, we found the index of the video event that the recalled event most closely matched (via the correlation between the events' topic vectors) and entered a 1 into that position in the matrix. This resulted in a matrix whose entries indicated whether or not each event was recalled by each participant (depending on whether the corresponding entires were set to one or zero).  Finally, we averaged over the rows of the matrix to yield a 1 by 30 array representing the proportion of participants that recalled each event as a function of the events' order appearance in the video (Fig.~\ref{fig:list-learning}C).
}

\paragraph{\DIFadd{Temporal clustering scores.}} \DIFadd{Temporal clustering describes a participant's tendency to organize their recall sequences by the learned items' encoding positions.  For instance, if a participant recalled the video events in the exact order they occurred (or in exact reverse order), this would yield a score of 1.  If a participant recalled the events in random order, this would yield an expected score of 0.5.  For each recall event transition (and separately for each participant), we sorted all not-yet-recalled events according to their absolute lag (i.e., distance away in the video).  We then computed the percentile rank of the next event the participant recalled.  We averaged these percentile ranks across all of the participant's recalls to obtain a single temporal clustering score for the participant.
}

\paragraph{\DIFadd{Semantic clustering scores.}} \DIFadd{Semantic clustering describes a participant's tendency to recall semantically similar presented items together in their recall sequences.  Here, we used the topic vectors for each event as a proxy for its semantic content. Thus, the similarity between the semantic content for two events can be computed by correlating their respective topic vectors.  For each recall event transition, we sorted all not-yet-recalled events according to how correlated the topic vector }\textit{\DIFadd{of the closest-matching video event}} \DIFadd{was to the topic vector of the closest-matching video event to the just-recalled event.  We then computed the percentile rank of the observed next recall.  We averaged these percentile ranks across all of the participant's recalls to obtain a single semantic clustering score for the participant.
}

%DIF >  To quantify the similarity between the video topic trajectory and individual recall topic trajectories, we considered several novel metrics.  First, we tested whether each participant's recall trajectory matched the video trajectory in a general sense. For each participant, we filtered the video trajectory to only include the events that the participant remembered.  We then computed the root mean squared difference (RMSD) between the remaining video events and the (closest-matching) recalled events.  For example, if the topic vectors for a participant's recall event topic vectors matched the corresponding video event topic vectors exactly (and in order), the expected RMSD for those events would be 0.  However, if the participant's recall events did not perfectly match the video events, or if they were out of order, then the RMSD would be greater than 0.  To assess the significance of the match between the video and recall trajectories, we carried out a permutation procedure whereby, for each of 10000 repetitions, we circularly shifted the recall trajectories (in time) by a random amount and then re-computed the RMSD each time.  This yielded a distribution of ``null'' RMSD values for each participant.  The observed RMSD values reached significance (i.e., $p < 0.05$, reflecting that more than 95\% of the null RMSD values were greater than the observed RMSD value) for nine of the participants (3, 4, 8--13, and 17).  (For the remaining participants this test yielded $0.05 \leq p < 0.25$.)  The observed RMSD values were also reliably correlated with hand-annotated memory performance across participants (Pearson's $r(15) = -0.57, p = 0.016$).  In other words, a closer match between the video and recall topic trajectories was related to better overall recall performance.
%DIF >  ------ NOTE: ------
%DIF >  Andy was thinking about including this in the revision early on.  It wasn't in the preprint version, and with how much else we have in here, I don't think it's worth including anymore.
%DIF >  - Paxton
%DIF >  I agree that we can take it out. -AH


\subsubsection*{\DIFadd{Novel naturalistic memory metrics}}

\paragraph*{\DIFadd{Precision.}}
\DIFadd{We tested whether participants who recalled more events were also more }\textit{\DIFadd{precise}} \DIFadd{in their recollections. For each participant, we computed the average correlation between the topic vectors for each recall event and those of its closest-matching video event. This gave a single value per participant representing the average precision across all recalled events.  We then correlated these values with both hand-annotated and model-derived (i.e., the number of unique video events matched by a participant's recall events) memory performance.
}

\paragraph*{\DIFadd{Distinctiveness.}}
\DIFadd{We also considered the }\textit{\DIFadd{distinctiveness}} \DIFadd{of each recalled event. That is, how unique a participant's description of a video event was, versus their descriptions of other video events.  We hypothesized that participants with high memory performance might describe each event in a more distinctive way (relative to those with lower memory performance who might describe events in a more general way).  To test this hypothesis we define a distinctiveness score for each recall event as
}

\[
  \DIFadd{d(\mathrm{event}) = 1 - \bar{c}(\mathrm{\mathbb{P} \setminus \{event\}}),
}\]
\DIFadd{where $\bar{c}(\mathrm{\mathbb{P} \setminus \{event\}})$ is the average correlation between the given recall event's topic vector and the topic vectors from all other recall events not matched to the same video event (for a single participant).  We then averaged these distinctiveness scores across all of the events recalled by the given participant and correlated resulting values with hand-annotated and model derived memory performance scores across-subjects, as above.
}

\noindent \DIFadd{Note: in all instances where we performed statistical tests involving precision or distinctiveness scores, we used Fisher's $z$-transformation \mbox{%DIFAUXCMD
\citep{Fish25} }\hspace{0pt}%DIFAUXCMD
to stabilize the variance across the distribution of correlation values prior to performing the test.  Similarly, when averaging precision or distinctiveness scores, we $z$-transformed the scores prior to computing the mean, and inverse $z$-transformed the result.
}

\DIFaddend \subsubsection*{Visualizing the video and recall topic trajectories}
We used the UMAP algorithm~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\citep{McInHeal18} }\hspace{0pt}%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\citep{McInEtal18} }\hspace{0pt}%DIFAUXCMD
}\DIFaddend to project the 100-dimensional topic space onto a two-dimensional space for visualization (Figs.~\ref{fig:trajectory}, \ref{fig:topics}).  \DIFdelbegin \DIFdel{To }\DIFdelend \DIFaddbegin \DIFadd{Importantly, to }\DIFaddend ensure that all of the trajectories were projected onto the \textit{same} lower dimensional space, we computed the low-dimensional embedding on a ``stacked\DIFdelbegin \DIFdel{'' }\DIFdelend \DIFaddbegin \DIFadd{" }\DIFaddend matrix created by vertically concatenating the events-by-topics topic proportions matrices for the video\DIFaddbegin \DIFadd{, across-participants average recall }\DIFaddend and all 17 \DIFaddbegin \DIFadd{individual }\DIFaddend participants' recalls.  We then divided the rows of the result (a total-number-of-events by two matrix) back into separate matrices for the video topic trajectory\DIFaddbegin \DIFadd{, across-participant average recall trajectory }\DIFaddend and the trajectories for each \DIFaddbegin \DIFadd{individual }\DIFaddend participant's recalls (Fig.~\ref{fig:trajectory}).  This general approach for discovering a shared low-dimensional embedding for a collections of high-dimensional observations follows \cite{HeusEtal18a}.

\DIFaddbegin \DIFadd{We optimized the manifold space for visualization based on two criteria: First, that the 2D embedding of the video trajectory should reflect its original 100-dimensional structure as faithfully as possible. Second, that the path traversed by the embedded video trajectory should intersect itself a minimal number of times.  The first criteria helps bolster the validity of visual intuitions about relationships between sections of video content, based on their locations in the embedding space.  The second criteria was motivated by the observed low off-diagonal values in the video trajectory's temporal correlation matrix (suggesting that the same topic-space coordinates should not be revisited; see Figure~2A in the main text). For further details on how we created this low-dimensional embedding space, see }\textit{\DIFadd{Supporting Information}}\DIFadd{.
}

\DIFaddend \subsubsection*{Estimating the consistency of flow through topic space across participants}
In Figure~\ref{fig:trajectory}B, we present an analysis aimed at characterizing locations in topic space that different participants move through in a consistent way (via their recall topic trajectories).  The two-dimensional topic space used in our visualizations (Fig.~\ref{fig:trajectory}) \DIFdelbegin \DIFdel{ranged from -5 to 5 (arbitrary ) unitsin the $x$ dimension and from -6.5 to 2 units in the $y$ dimension.  We divided this space into a grid of vertices spaced 0.25~unitsapart}\DIFdelend \DIFaddbegin \DIFadd{comprised a 60 x 60 (arbitrary units) square.  We tiled this space with a 50 x 50 grid of evenly spaced vertices, and defined a circular area centered on each vertex whose radius was two times the distance between adjacent vertices (i.e., 2.4 units)}\DIFaddend .  For each vertex, we examined the set of line segments formed by connecting each pair successively recalled events, across all participants, that passed \DIFdelbegin \DIFdel{within 0.5 units}\DIFdelend \DIFaddbegin \DIFadd{through this circle}\DIFaddend .  We computed the distribution of angles formed by those segments and the $x$-axis, and used a Rayleigh test to determine whether the distribution of angles was reliably ``peaked'' (i.e., consistent across all transitions that passed through that local portion of topic space).  To create Figure~\ref{fig:trajectory}B we drew an arrow originating from each grid vertex, pointing in the direction of the average angle formed by \DIFaddbegin \DIFadd{the }\DIFaddend line segments that passed within \DIFdelbegin \DIFdel{0.5 units}\DIFdelend \DIFaddbegin \DIFadd{its circular radius}\DIFaddend .  We set the arrow lengths to be inversely proportional to the $p$-values of the Rayleigh tests at each vertex.  Specifically, for each vertex we converted all of the angles of segments that passed within \DIFdelbegin \DIFdel{0.5 }\DIFdelend \DIFaddbegin \DIFadd{2.4 }\DIFaddend units to unit vectors, and we set the arrow lengths at each vertex proportional to the length of the (circular) mean vector.  We also indicated any significant results ($p < 0.05$, corrected using the Benjamani-Hochberg procedure) by coloring the arrows in blue (darker blue denotes a lower $p$-value, i.e., a longer mean vector); all tests with $p \geq 0.05$ are displayed in gray and given a lower opacity value.

\subsection*{Searchlight fMRI analyses}
In Figure~\ref{fig:brainz}, we present two analyses aimed at identifying brain \DIFdelbegin \DIFdel{structures }\DIFdelend \DIFaddbegin \DIFadd{regions }\DIFaddend whose responses (as participants viewed the video) exhibited \DIFdelbegin \DIFdel{particular temporal correlations}\DIFdelend \DIFaddbegin \DIFadd{a particular temporal structure}\DIFaddend .  We developed a searchlight analysis \DIFdelbegin \DIFdel{whereby }\DIFdelend \DIFaddbegin \DIFadd{wherein }\DIFaddend we constructed a \DIFdelbegin \DIFdel{cube }\DIFdelend \DIFaddbegin \DIFadd{5 x 5 x 5 cube of voxels (following \mbox{%DIFAUXCMD
\citealp{ChenEtal17}}\hspace{0pt}%DIFAUXCMD
) }\DIFaddend centered on each voxel \DIFdelbegin \DIFdel{(radius: 5~voxels).  For }\DIFdelend \DIFaddbegin \DIFadd{in the brain, and for }\DIFaddend each of these cubes, \DIFdelbegin \DIFdel{we }\DIFdelend computed the temporal correlation matrix of the voxel responses during video viewing.  Specifically, for each of the 1976 volumes collected during video viewing, we correlated the activity patterns in the given cube with the activity patterns (in the same cube) collected during every other timepoint.  This yielded a 1976 by 1976 correlation matrix for each cube.  \DIFaddbegin \DIFadd{Note: participant 5's scan ended 75s early, and in \mbox{%DIFAUXCMD
\citealp{ChenEtal17}}\hspace{0pt}%DIFAUXCMD
's publicly released dataset, their scan data was padded to match the length of the other participants'.  For our searchlight analyses, we removed this padded data (i.e., the last 50 TRs), resulting in a 1925 by 1925 correlation matrix for each cube in participant 5's brain.
}\DIFaddend 

Next, we constructed \DIFdelbegin \DIFdel{two sets }\DIFdelend \DIFaddbegin \DIFadd{a series }\DIFaddend of ``template\DIFdelbegin \DIFdel{'' matrices: one reflected the }\DIFdelend \DIFaddbegin \DIFadd{" matrices: the first reflecting the timecourse of }\DIFaddend video's topic trajectory\DIFdelbegin \DIFdel{and the other reflected }\DIFdelend \DIFaddbegin \DIFadd{, and the others reflecting that of }\DIFaddend each participant's recall topic trajectory.  To construct the video template, we computed the correlations between the topic proportions estimated for every pair of TRs (prior to segmenting the trajectory into discrete events; i.e., the correlation matrix shown in Figs.~\ref{fig:model}B and \ref{fig:brainz}A).  We constructed similar temporal correlation matrices for each participant's recall topic trajectory (Figs.~\ref{fig:model}D, \corrmats).  However, to correct for length differences and potential non-linear transformations between viewing time and recall time, we first used dynamic time warping~\citep{BernClif94} to temporally align participants' recall topic trajectories with the video topic trajectory\DIFdelbegin \DIFdel{(an }\DIFdelend \DIFaddbegin \DIFadd{.  An }\DIFaddend example correlation matrix before and after warping is shown in Fig.~\ref{fig:brainz}B\DIFdelbegin \DIFdel{)}\DIFdelend .  This yielded a 1976 by 1976 correlation matrix for the video template and for each participant's recall template.

\DIFaddbegin \DIFadd{The temporal structure of the video's content (as described by our model) is captured in the block-diagonal structure of the video's temporal correlation matrix (e.g., Figs.~\ref{fig:model}B, \ref{fig:brainz}A), with time periods of thematic stability represented as dark blocks of varying sizes.  Inspecting the video correlation matrix suggests that the video's semantic content is highly temporally specific (i.e., the correlations between topic vectors from distant timepoints are almost entirely near-zero).  By contrast, the activity patterns of individual (cubes of) voxels can encode relatively limited information on their own, and their activity frequently contributes to multiple separate functions \mbox{%DIFAUXCMD
\citep{FreeEtal01, SigmDeha08, CharKoec10, RishEtal13}}\hspace{0pt}%DIFAUXCMD
.  By nature, these two attributes give rise to similarities in activity across large timescales that may not necessarily reflect a single task.  To enable a more sensitive analysis of brain regions whose shifts in activity patterns mirrored shifts in the semantic content of the video or recalls, we restricted the temporal correlations we considered to timescale of semantic information captured by our model.  Specifically, we isolated the upper triangle of the video correlation matrix and created a ``proximal correlation mask" that included only diagonals from the upper triangle of the video correlation matrix up to the first that contained no positive correlations.  Applying this mask to the full video correlation matrix was analogous to excluding diagonals beyond the corner of the largest diagonal block.  In other words, the timescale of temporal correlations we considered corresponded to the longest period of thematic stability in the video, and by extension the longest expected period of thematic stability in participants' recalls and the longest period of stability we might expect to see in voxel activity arising from processing or encoding video content.  Figure \ref{fig:brainz} shows this proximal correlation mask applied to the temporal correlation matrices for the video, an example participant's (warped) recall, and an example cube of voxels from our searchlight analyses.
}

\DIFaddend To determine which (cubes of) voxel responses \DIFdelbegin \DIFdel{reliably }\DIFdelend matched the video template, we correlated the \DIFaddbegin \DIFadd{proximal diagonals from the }\DIFaddend upper triangle of the voxel correlation matrix for each cube with the \DIFdelbegin \DIFdel{upper triangle of the }\DIFdelend \DIFaddbegin \DIFadd{proximal diagonals from }\DIFaddend video template matrix~\citep{KrieEtal08b}.  This yielded, for each participant, a \DIFdelbegin \DIFdel{single correlation value.  We computed the average }\DIFdelend \DIFaddbegin \DIFadd{voxelwise map of correlation values.  We then performed a one-sample $t$-test on the distribution of }\DIFaddend (Fisher $z$-transformed) \DIFdelbegin \DIFdel{correlation coefficient }\DIFdelend \DIFaddbegin \DIFadd{correlations at each voxel, }\DIFaddend across participants.  \DIFdelbegin \DIFdel{We used }\DIFdelend \DIFaddbegin \DIFadd{This resulted in a value for each voxel (cube), describing how reliably its timecourse mirrored that of the video.
}

\DIFadd{We further sought to ensure that our analysis identified regions where the activationsâ€™ temporal structure specifically reflected that of the video, rather than regions whose activity was simply autocorrelated at a width similar to the video templateâ€™s diagonal.  To achieve this, we used a phase shift-based permutation procedure, wherein we circularly shifted the videoâ€™s topic trajectory by }\DIFaddend a \DIFdelbegin \DIFdel{permutation-based procedureto assess significance, whereby we re-computed the average correlations for each of 100 ``null'' videotemplates (constructed by circularly shifting the template by a }\DIFdelend random number of timepoints\DIFdelbegin \DIFdel{)}\DIFdelend \DIFaddbegin \DIFadd{, computed the resulting ``nullâ€ video template, and re-ran the searchlight analysis, in full}\DIFaddend .  (For each \DIFdelbegin \DIFdel{permutation}\DIFdelend \DIFaddbegin \DIFadd{of the 100 permutations}\DIFaddend , the same \DIFaddbegin \DIFadd{random }\DIFaddend shift was used for all participants\DIFdelbegin \DIFdel{.)We then }\DIFdelend \DIFaddbegin \DIFadd{).  We $z$-scored the observed (unshifted) result at each voxel against the distribution of permutation-derived ``nullâ€ results, and }\DIFaddend estimated a $p$-value by computing the proportion of shifted \DIFdelbegin \DIFdel{correlations that were larger than the observed (unshifted) correlation}\DIFdelend \DIFaddbegin \DIFadd{results that yielded larger values}\DIFaddend .  To create the map in Figure~\ref{fig:brainz}\DIFdelbegin \DIFdel{A }\DIFdelend \DIFaddbegin \DIFadd{C, }\DIFaddend we thresholded out any voxels whose \DIFdelbegin \DIFdel{correlation values }\DIFdelend \DIFaddbegin \DIFadd{similarity to the unshifted videoâ€™s structure }\DIFaddend fell below the 95\textsuperscript{th} percentile of the permutation-derived \DIFdelbegin \DIFdel{null distribution}\DIFdelend \DIFaddbegin \DIFadd{similarity results}\DIFaddend .

We used \DIFdelbegin \DIFdel{a similar }\DIFdelend \DIFaddbegin \DIFadd{an analogous }\DIFaddend procedure to identify which voxels' responses reflected the recall templates.  For each participant, we correlated the \DIFaddbegin \DIFadd{proximal diagonals from the }\DIFaddend upper triangle of the correlation matrix for each cube of voxels with \DIFdelbegin \DIFdel{their (time warped}\DIFdelend \DIFaddbegin \DIFadd{the proximal diagonals from the upper triangle of their (time-warped}\DIFaddend ) recall correlation matrix.  As in the video template analysis\DIFaddbegin \DIFadd{, }\DIFaddend this yielded a \DIFdelbegin \DIFdel{single correlation coefficient for each }\DIFdelend \DIFaddbegin \DIFadd{voxelwise map of correlation coefficients per }\DIFaddend participant.  However, whereas the video analysis compared every participant's responses to the same template, here the recall templates were unique for each participant.  \DIFdelbegin \DIFdel{We computed the average }\DIFdelend \DIFaddbegin \DIFadd{As in the analysis described above, we $t$-scored the (Fisher }\DIFaddend $z$-transformed\DIFdelbegin \DIFdel{correlation coefficient across participants}\DIFdelend \DIFaddbegin \DIFadd{) voxelwise correlations}\DIFaddend , and used the same permutation procedure we developed for the video responses to \DIFdelbegin \DIFdel{assess significant correlations}\DIFdelend \DIFaddbegin \DIFadd{ensure specificity to the recall timeseries and assign significance values}\DIFaddend .  To create the map in Figure~\ref{fig:brainz}\DIFdelbegin \DIFdel{B we }\DIFdelend \DIFaddbegin \DIFadd{D we again }\DIFaddend thresholded out any voxels whose \DIFdelbegin \DIFdel{correlation }\DIFdelend \DIFaddbegin \DIFadd{correspondence }\DIFaddend values fell below the 95\textsuperscript{th} percentile of the permutation-derived null distribution.

\DIFdelbegin %DIFDELCMD < \bibliography{memlab}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \subsection*{\DIFadd{Neurosynth decoding analyses}}
\texttt{\DIFadd{Neurosynth}} \DIFadd{parses a massive online database of over 14,000 neuroimaging studies and constructs meta-analysis images for over 13,000 psychology- and neuroscience-related terms, based on NIfTI images accompanying studies where those terms appear at a high frequency.  Then, given a novel image (tagged with its value type; e.g., $t$-, $F$- or $p$-statistics), }\texttt{\DIFadd{Neurosynth}} \DIFadd{returns a list of terms whose meta-analysis images are most similar to this new data.  Our permutation procedure yielded, for each of the two searchlight analyses, a voxelwise map of significance ($p$-statistic) values.  These maps describe the extent to which each voxel }\textit{\DIFadd{specifically}} \DIFadd{reflected the temporal structure of the video or individuals' recalls (i.e., for each voxel, the proportion of phase-shifted topic vector correlation matrices less similar to the voxel activity correlation matrix than the unshifted video's correlation matrix). We input the two statistical maps described above to }\texttt{\DIFadd{Neurosynth}} \DIFadd{to create a list of the 10 most representative terms for each map.
}\DIFaddend 


%DIF >  \bibliography{../../CDL-bibliography/memlab}
\DIFaddbegin \bibliography{CDL-bibliography/memlab}

\DIFaddend \section*{Supporting information}
Supporting information is available in the online version of the paper.

\section*{Acknowledgements}
We thank Luke Chang, Janice Chen, Chris Honey, Lucy Owen, Emily Whitaker, and Kirsten Ziman for feedback and scientific discussions. We also thank Janice Chen, Yuan Chang Leong, Kenneth Norman, and Uri Hasson for sharing the data used in our study.  Our work was supported in part by NSF EPSCoR Award Number 1632738. The content is solely the responsibility of the authors and does not necessarily represent the official views of our supporting organizations.

\section*{Author contributions}
Conceptualization: A.C.H. and J.R.M.; Methodology: A.C.H\DIFaddbegin \DIFadd{., P.C.F}\DIFaddend . and J.R.M.; Software: A.C.H., P.C.F. and J.R.M.; Analysis: A.C.H., P.C.F. and J.R.M.; Writing, Reviewing, and Editing: A.C.H., P.C.F. and J.R.M.; Supervision: J.R.M.

\section*{Author information}
The authors declare no competing financial interests.  Correspondence and requests for materials should be addressed to J.R.M. (jeremy.r.manning@dartmouth.edu).


\end{document}
