\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{geometry}
\usepackage{natbib}
\usepackage{pxfonts}
\usepackage{graphicx}
\usepackage{newfloat}
\usepackage{setspace}
\usepackage{placeins}
%\doublespacing

\newcommand{\argmax}{\mathop{\mathrm{argmax}}\limits}
\newcommand{\argmin}{\mathop{\mathrm{argmin}}\limits}

\title{\textit{Supporting Information for}: Geometric models reveal behavioral and neural signatures of how naturalistic experiences are transformed into episodic memories}
\author{Andrew C. Heusser\textsuperscript{1, 2, \textdagger}, Paxton C. Fitzpatrick\textsuperscript{1, \textdagger}, and Jeremy R. Manning\textsuperscript{1, *}\\\textsuperscript{1}Department of Psychological and Brain Sciences\\Dartmouth College, Hanover, NH 03755, USA\\\textsuperscript{2}Akili Interactive\\Boston, MA 02110\\\textsuperscript{\textdagger}Denotes equal contribution\\\textsuperscript{*}Corresponding author: Jeremy.R.Manning@Dartmouth.edu}

\bibliographystyle{apa}

\begin{document}
\maketitle

\setcounter{equation}{0}
\setcounter{figure}{0}
\setcounter{table}{0}
\setcounter{page}{1}
\setcounter{section}{0}
\makeatletter
\renewcommand{\theequation}{S\arabic{equation}}
\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\bibnumfmt}[1]{[S#1]}
\renewcommand{\citenumfont}[1]{S#1}


\section*{Overview}
This document provides additional details about the methods we used in the main text.  We also include some additional analyses referenced in the main text.

\section*{Additional details about topic modeling methods and results}
\subsection*{Optimizing topic model parameters}
In order to create accurate episode and recall models, we used an optimization method that was driven by our ability to explain hand-annotated memory performance metrics collected by \cite{ChenEtal17}.  Specifically, we used a grid search to compute the $\omega$ (episode sliding window duration, in scenes), $\rho$ (recall sliding window duration, in sentences), and $K$ (number of topics) that satisfied
\[
\argmax_{\omega, \rho, K} \left[\mathrm{corr}\left(\mathrm{corr}\left(\mu\left(\omega, \rho, K\right), \nu\left(\omega, \rho, K\right)\right), \theta\right)\right],
\]
where $\mathrm{corr}(\mu, \nu)$ is the per-participant correlation between the temporal correlation matrices of the episode ($\mu$) and recall ($\nu$) trajectory, and $\theta$ is the per-participant hand-annotated memory performance.  We searched over a grid of pre-specified values for each of these parameters; the resulting correlations are displayed in Figure~\ref{fig:paramsearch}.  The optimal parameters were $\omega = 50$, $\rho = 10$, and $K = 100$. In subsequent refinements of our topic modeling approach, we made various improvements to how we parse the episode and recall text and construct the trajectories' timeseries, but maintained these optimized parameters throughout.


\begin{figure}[]
\centering
\includegraphics[width=1\textwidth]{figs/parameter_search}
\caption{\small \textbf{Optimizing topic model parameters.}  We performed a grid search over episode sliding window length ($\omega \in \{5, 10, 25, 50, 100 \}$), recall sliding window length ($\rho \in \{5, 10, 25, 50, 100 \}$), and number of topics ($K \in \{5, 10, 25, 50, 100 \}$).  The reported correlations are between per-subject episode-recall trajectory correlations and per-subject hand-annotated memory performance ratings.}
\label{fig:paramsearch}
\end{figure}

The optimized model converged on 32 unique topics that were assigned non-zero weights over the course of the episode.  We provide a list of the top ten highest-weighted words from each topic in Figure~\ref{fig:topics}.

\begin{figure}[]
\centering
\includegraphics[width=0.65\textwidth]{figs/topic_words}
\caption{\small \textbf{Topics discovered in \textit{Sherlock}.} We applied a topic model to hand-annotated information about 1000 scenes spanning the 45 minute episode.  We identified 32 unique topics with non-zero weights (we used $K=100$ topics to fit the model).  Each topic comprises a distribution of weights over all words in the vocabulary.  For each topic, we show the words with the 10 largest weights, along with a suggested description of the topic.}
\label{fig:topics}
\end{figure}
%\FloatBarrier

\subsection*{Feature importance analyses}
To determine the contribution of each feature to the structure of the episode topic proportions, we conducted a ``leave one out'' analysis.  Specifically, we compared the original episode topic trajectory (created using all hand-annotated features from the 998 hand-annotated scenes spanning the \textit{Sherlock} episode; see \textit{Methods} for a full list of features) with episode trajectories created using all but one type of feature.  We created temporal correlation matrices for each trajectory (using the topic proportions matrices) and correlated the proximal diagonals from the upper triangles of each impoverished trajectory with the original feature-complete trajectory (see \textit{Methods} for details on how we isolated proximal temporal correlations).  Observing a lower correlation between an impoverished trajectory (holding out a particular feature) and the feature-complete trajectory would suggest that the given feature played a more prominent role in shaping the temporal structure of the feature-complete trajectory.  We found that hand-annotated narrative details provided the most structure to the feature-complete trajectory, whereas the name of the character(s) in focus for each shot provided the least structure (Fig.~\ref{fig:feature-importance}A).

\begin{figure}[]
\centering
\includegraphics[width=1\textwidth]{figs/feature_value}
\caption{\small \textbf{Feature importance analysis.} \textbf{A.} Contributions of each feature type to the structure of the episode trajectory. The bar heights reflect the correlation between the episode trajectory computed using all features with a episode trajectory computed using all features except the indicated feature.  (Lower bars reflect features that contribute more substantially to the episode trajectory's shape.) \textbf{B.} Which features are preserved during recall?  The bar heights reflect the (average) across-participant correlations between the episode and recall trajectories.  Error bars denote bootstrap-estimated standard error of the mean.  \textbf{C.} Feature correlation matrix.  Each entry displays the correlation between episode topic trajectories created using only the indicated (row/column) features.}
\label{fig:feature-importance}
\end{figure}

We also carried out an analysis of which annotated features tended to shape aspects of the episode topic trajectory's structure that were preserved in participants' recalls.  Specifically, we computed the timepoint-by-timepoint correlation matrix of the episode topic trajectory, and correlated the proximal diagonals from its upper triangle with those of the timepoint-by-timepoint correlation matrices for each participant's recall topic trajectory (resampled using linear interpolation to have the same number of timepoints as the episode trajectory).  This yielded a single correlation coefficient for each participant.  We then repeated this analysis with each annotated feature held out in turn.  Observing a lower correlation between the episode and recall trajectories (when a given feature was held out) would indicate that participants utilize changes in that feature's content to discriminate between sections of the episode when organizing their recalls.  We found that hand-annotated narrative details were the most heavily utilized type of feature, whereas changes in the text present on-screen, the indoor/outdoor distinction between scenes, the camera angle, the names of the various characters on screen, and which character was speaking tended not to impact participants' recall structures (i.e., removing those features resulted in a \textit{greater} episode-recall topic vector correlation than including them; Fig.~\ref{fig:feature-importance}B).

Next, we wondered how the different types of features might relate.  For example, knowing which character is in focus during a given scene may also provide information about which character is speaking.  We computed episode topic trajectories from annotations of each individual feature, in turn, and then compared the proximal diagonals from the upper triangles of the temporal correlation matrices of all pairs of features.  This provided additional confirmation that the shape of the full trajectory (including all types of features) was largely driven by narrative details.  We also found that character-driven features (characters on screen, characters speaking, and characters in focus) were strongly correlated.  Other details, such as the presence or absence of music, led to very different topic trajectories (Fig.~\ref{fig:feature-importance}C).
%\FloatBarrier

\subsection*{Creating a low-dimensional embedding space}
Figures~7 and 8C in the main text display two-dimensional projections of the 100-dimensional topic trajectories for the episode (7A, 8C), average recall (7B), and each individual's recall (7C, 8C).  We created these embeddings using the Uniform Manifold Approximation and Projection algorithm \citep[UMAP;][]{McInEtal18} called from our high-dimensional visualization and text analysis software, \texttt{HyperTools}~\citep{HeusEtal18a}.  An advantage of the UMAP algorithm over comparable manifold learning techniques (such as $t$-SNE) is that UMAP explicitly attempts to preserve the global structure of the data \citep{McInEtal18,BechEtal19} by constructing a space where distance on the manifold is standard Euclidean distance, with respect to the global coordinate system.  This was important in our use case, as we wanted to visualize both the evolving structure of the episode and the spatial relationships between presented and recalled content.

UMAP achieves a balance between representing local and global structure via a subset of its hyper-parameters: \texttt{n\_neighbors} , \texttt{spread}, and \texttt{min\_dist}.  The \texttt{n\_neighbors} hyper-parameter ($K$) denotes the number of nearest neighbors to consider in constructing the high-dimensional fuzzy simplicial set for each datapoint.  \texttt{Spread} ($\gamma$) and \texttt{min\_dist} ($\delta$) function together to create the differentiable decay curve used to approximate the injective function for mapping between high- and low-dimensional fuzzy simplicial sets.  In essence, \texttt{min\_dist} determines the degree to which nearby points are clustered or expanded, relative to to the overall \texttt{spread}.

Two other parameters ultimately affect this balance between preserving local and global structure: the seed ($\tau$) for the (pseudo-)random number generator, and the order of the observations (i.e., trajectories) to be embedded ($\varphi$).  As described in \textit{Methods}, we ensured the episode and recall events were projected onto the \textit{same} low-dimensional manifold by fitting the embedding model to a stacked matrix of all episode, average recall, and individuals' recall events.  After initializing the low-dimensional simplicial set (by default, using a spectral embedding of the high-dimensional simplicial set's fuzzy 1-skeleton), UMAP optimizes the embedding using stochastic gradient descent with cross-entropy as a cost function.  During optimization, indices of the data are sampled at random, and thus the local minimum achieved by the optimization is dependent on both the state of the RNG and the sequence in which observations are concatenated.

We performed a grid search over pre-specified values of these hyper-parameters, and optimized the manifold space for visualization based on two criteria: First, that the 2D embedding of the episode trajectory should reflect its original 100-dimensional structure as faithfully as possible. Second, that the path traversed by the embedded episode trajectory should intersect itself a minimal number of times.  The first criteria helps bolster the validity of visual intuitions about relationships between sections of episode content, based on their locations in the embedding space.  The second criteria was motivated by the observed low off-diagonal values in the episode trajectory's temporal correlation matrix (suggesting that the same topic-space coordinates should not be revisited; see Figure~2A in the main text) and the inability to accurately describe the consistency of participants' transitions through topic space at locations where those transitions frequently occurred in multiple, orthogonal directions (see \textit{Methods} for analysis details).  We formalized this optimization as the combination of hyper-parameter values that satisfied
\[
\argmax_{\left\{K, \gamma, \delta, \tau, \varphi~\in~E~\mathrm{|}~\Phi\left(K, \gamma, \delta, \tau, \varphi \right) \right\}} \left[\mathrm{corr}\left(\Upsilon\left(\xi_{K, \gamma, \delta, \tau, \varphi}\right),~\Upsilon\left(\chi\right)\right)\right],~\mathrm{where}
\]
\[
\Phi\left(K, \gamma, \delta, \tau, \varphi \right) = \argmin_{K, \gamma, \delta, \tau, \varphi} \left[\Gamma\left(\xi_{K, \gamma, \delta, \tau, \varphi}\right)\right],
\]
$\xi$ is the episode's trajectory through the manifold space; $\chi$ is the original, 100-dimensional episode trajectory; $\Upsilon$ is a function that computes a condensed matrix of pairwise distances between event vectors (computed using correlation distance in the original topic space and Euclidean distance in the manifold space); $\mathrm{corr}\left(\Upsilon\left(\xi\right), \Upsilon\left(\chi\right)\right)$ is the correlation between the sets of pairwise distances, and $\Gamma$ is the number of instances in which lines drawn between consecutive episode event embeddings intersected each other.   The sets of hyper-parameter values we searched over comprised: $K \in \{10x \in \mathbb{Z}~|~10 < x <  22\} \cup \{161\}$ (a range roughly centered on half the total number of events, 161, in order to balance representations of local and global structure); $\gamma \in \{1, 3, 5, 7, 9\}$; $\delta \in \{0.1, 0.3, 0.5, 0.7, 0.9\}$; $\tau \in \{x \in \mathbb{Z}~|~0 < x < 101\}$; and $\varphi \in {S\choose3},~\mathrm{where}~S=\{\mathrm{episode~events,~average~recall~events, individual~recall~events}\}$.  The optimal parameters (that yielded $\Phi=0$) were $K=170$, $\gamma=7$, $\delta=0.7$, $\tau=41$, with the order of sequence $\varphi$ as the average recall events, episode events, and individuals' recall events, vertically concatenated, in order.


\FloatBarrier
\section*{Participant-level figures referenced in the main text}

\begin{figure}[p!]
\centering
\includegraphics[width=\textwidth]{figs/corrmats}
\caption{\small \textbf{Recall trajectory temporal correlation matrices and event segmentation fits.} Each panel is in the same format as Figure~2E in the main text.  The yellow boxes indicate HMM-identified event boundaries.}
\label{fig:corrmats}
\end{figure}

\begin{figure}[p!]
\centering
\includegraphics[width=\textwidth]{figs/matchmats}
\caption{\small \textbf{episode-recall event correlation matrices.}  Each panel is in the same format as Figure~2G in the main text.  The yellow boxes mark the matched episode event for each recall event (i.e., the maximum correlation in each column).}
\label{fig:matchmats}
\end{figure}

\begin{figure}[p!]
\centering
\includegraphics[width=.9\textwidth]{figs/k_optimization}
\caption{\small \textbf{episode and recall trajectory \textit{K}-optimization functions.}  We selected the optimal $K$-value for the episode and each recall trajectory, using the formula described in \textit{Methods}. This computation resulted in a curve for each trajectory, describing the Wasserstein distance between the distributions of within-event and across-event topic vector correlations, as a function of $K$.}
\label{fig:k_optimization}
\end{figure}

\FloatBarrier

\newpage
\renewcommand{\refname}{Supplemental references}
% \bibliography{../../CDL-bibliography/memlab}
\bibliography{CDL-bibliography/memlab}



\end{document}
