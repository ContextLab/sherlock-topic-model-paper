\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{geometry}
\usepackage{natbib}
\usepackage{pxfonts}
\usepackage{graphicx}
\usepackage{newfloat}
\usepackage{setspace}
%\doublespacing

\newcommand{\argmax}{\mathop{\mathrm{argmax}}\limits}

\title{\textit{Supplemental materials for}: How is experience transformed into memory?}
\author{Andrew C. Heusser, Paxton C. Fitzpatrick, and Jeremy R. Manning\\Department of Psychological and Brain Sciences\\Dartmouth College, Hanover, NH 03755, USA\\Corresponding author: jeremy.r.manning@dartmouth.edu}

\bibliographystyle{apa}

\begin{document}
\maketitle

\setcounter{equation}{0}
\setcounter{figure}{0}
\setcounter{table}{0}
\setcounter{page}{1}
\setcounter{section}{0}
\makeatletter
\renewcommand{\theequation}{S\arabic{equation}}
\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\bibnumfmt}[1]{[S#1]}
\renewcommand{\citenumfont}[1]{S#1}


\section*{Overview}
This document provides additional details about the methods we used in the main text.  We also include some additional figures referenced in the main text.

\section*{Additional details about topic modeling methods and results}
\subsection*{Optimizing topic model parameters}
In order to create accurate video and recall models, we used an optimization method that was driven by our ability to explain hand-annotated memory performance metrics collected by \cite{ChenEtal17}.  Specifically, we used a grid search to compute the $\omega$ (movie sliding window duration, in scenes), $\rho$ (recall sliding window duration, in sentences), and $K$ (number of topics) that satisfied
\[
\argmax_{\omega, \rho, K} \left[\mathrm{corr}\left(\mathrm{corr}\left(\mu\left(\omega, \rho, K\right), \nu\left(\omega, \rho, K\right)\right), \theta\right)\right],
\]
where $\mathrm{corr}(\mu, \nu)$ is the per-participant correlation between the upper triangles of the temporal correlation matrices of the video ($\mu$) and recall ($\nu$) trajectory, and $\theta$ is the per-participant hand-annotated memory performance.  We searched over a grid of pre-specified values for each of these parameters; the resulting correlations are displayed in Figure~\ref{fig:paramsearch}.  The optimal parameters were $\omega = 250$, $\rho = 50$, and $K = 100$.


\begin{figure}[tp]
\centering
\includegraphics[width=1\textwidth]{figs/parameter_search}
\caption{\small \textbf{Optimizing topic model parameters.}  We performed a grid search over movie sliding window length ($\omega \in \{5, 10, 25, 50, 100 \}$), recall sliding window length ($\rho \in \{5, 10, 25, 50, 100 \}$, and number of topics ($K \in \{5, 10, 25, 50, 100 \}$.  The reported correlations are between per-subject video-recall trajectory correlations and per-subject hand-annotated memory performance ratings.}
\label{fig:paramsearch}
\end{figure}

The optimized model converged on 28 unique topics that were assigned non-zero weights over the course of the video.  We provide a list of the top ten highest-weighted words from each topic in Figure~\ref{fig:topics}.





\begin{figure}[tp]
\centering
\includegraphics[width=0.75\textwidth]{figs/topic_words}
\caption{\small \textbf{Topics discovered in \textit{Sherlock}.} We applied a topic model to hand-annotated information about 1000 scenes spanning the 45 minute episode.  We identified 28 unique topics with non-zero weights (we used $K=100$ topics to fit the model).  Each topic comprises a distribution of weights over all words in the vocabulary.  For each topic, we show the words with the 10 largest weights, along with a suggested description of the topic.}
\label{fig:topics}
\end{figure}


\subsection*{Feature importance analyses}
To determine the contribution of each feature to the structure of the video topic proportions, we conducted a ``leave one out'' anlaysis.  Specifically, we compared the original video topic trajectory (created using all hand-annotated features from the 1000 hand-annotated scenes spanning the \textit{Sherlock episode}; see \textit{Methods} for a full list of features) with video trajectories created using all but one type of feature.  We created temporal correlation matrices for each trajectory (using the topic proportions matrices) and correlated the upper triangles of each impoverished trajectory with the original feature-complete trajectory.  Observing a lower correlation between an impoverished trajectory (holding out a particular feature) and the feature-complete trajectory would suggest that the given feature played a more prominent role in shaping the structue of the feature-complete trajectory.  We found that hand-annotated narrative details provided the most structure to the feature-complete trajectory, whereas transcriptions of onscreen text provided the least structure (Fig.~\ref{fig:feature-importance}A).

\begin{figure}[tb]
\centering
\includegraphics[width=1\textwidth]{figs/feature_value}
\caption{\small \textbf{Feature importance analysis.} \textbf{A.} Contributions of each feature type to the structure of the video trajectory. The bar heights reflect the correlation between the video trajectory computed using all features with a video trajectory computed using all features except the indicated feature.  (Lower bars reflect features that contribute more substantially to the video trajectory's shape.) \textbf{B.} Which features are preserved during recall?  The bar heights reflect the (average) across-participant correlations between the video and recall trajectories.  Error bars denote bootstrap-estimated standard error of the mean.  \textbf{C.} Feature correlation matrix.  Each entry displays the correlation between video topic trajectories created using only the indicated (row/column) features.}
\label{fig:feature-importance}
\end{figure}

We also carried out an analysis of which annotated features tended to shape aspects of the video topic trajectory that were preserved in participants' recalls.  Specifically, we computed the timepoint-by-timepoint correlation matrix of the video topic trajectory, and correlated its upper triangle with the timepoint-by-timepoint correlation matrices of each participant's recall topic trajectory (resampled using linear interpolation to have the same number of timepoints as the video trajectory).  This yielded a single correlation coefficient for each participant.  We then carried out a series of analyses whereby we repeated the analysis with each annotated feature held out in turn.  Observing a lower correlation between the video and recall trajectories (when a given feature was held out) would indicate that the feature tends to be preserved in participants' recalls.  We found that hand-annotated narrative details were the most preserved type of feature, whereas information about the camera angle tended not to feature in participants' recalls (Fig.~\ref{fig:feature-importance}B).

Next, we wondered how the different types of features might relate.  For example, knowing which characters are on screen during a given scene may also provide information about which characters are speaking.  We computed video topic trajectories for each feature in turn, and then compared the temporal correlation matrices of all pairs of features.  This provided additional confirmation that the full model (including all types of features) was largely driven by narrative details.  We also found that character-driven features (characters on screen, characters speaking, and characters in focus) were strongly correlated.  Other details, such as the presense or absense of music, led to very different topic trajectories (Fig.~\ref{fig:feature-importance}C).


\section*{Additional analyses of memory performance}

\subsection*{Naturalistic extensions of classic list-learning analyses}
In traditional list-learning experiments, participants view a list of items (e.g., words) and then recall the items later.  Our video-recall event matching approach affords us the ability to analyze memory in a similar way. The video and recall events can be treated analogously to studied and recalled ``items'' in a list-learning study.  We can then extend classic analyses of memory performance and dynamics (originally designed for list-learning experiments) to the more naturalistic video recall task used in our study.

Perhaps the simplest and most widely used measure of memory performance is \textit{accuracy}-- i.e., the proportion of studied (experienced) items (in this case, the 34 video events) that the participant later remembered.  \cite{ChenEtal17} developed a human rating system whereby the quality of each participant's memory was evaluated by an independent rater.  We found a strong across-participants correlation between these independant ratings and the overall number of events that our HMM approach identified in participants' recalls ($r = 0.67, p = 0.003$).

As described below, we next considered three more naunced measures of the memory performance and dynamics that are typically associated with list-learning studies.  We also provide a software package, \texttt{Quail}, for carrying out these analyses~\citep{HeusEtal17b}.

\begin{figure}[tb]
\centering
\includegraphics[width=1\textwidth]{figs/list_learning}
\caption{\small \textbf{Naturalistic extensions of classic list-learning memory analyses.} \textbf{A.} The probability of first recall as a function of the serial position of the event in the video. \textbf{B}.  The probability of recalling each event, conditioned on having just recalled the event \textit{lag} events away in the video.  \textbf{C.} The proportion of participants who recalled each event, as a function of the serial position of the events in the video.  All panels: error bars denote bootstrap-estimated standard error of the mean.}
\label{fig:list-learning}
\end{figure}

\paragraph{Probability of first recall (PFR).}  PFR curves~\citep{WelcBurn24, PostPhil65, AtkiShif68} reflect the probability that an item will be recalled first as a function of its serial position during encoding. To carry out this analysis, we initialized a number-of-participants (17) by number-of-video-events (34) matrix. Then for each participant, we found the index of the video event that was recalled first (i.e., the video event whose topic vector was most strongly correlated with that of the first recall event) and filled in that index in the matrix with a 1.  Finally, we averaged over the rows of the matrix, resulting in a 1 by 34 array representing the proportion of participant that recalled an event as a function of serial position during encoding (Fig.~\ref{fig:list-learning}A).

\paragraph{Lag conditional probability curve (lag-CRP).} The lag-CRP curve~\citep{Kaha96} reflects the probability of recalling a given event after the just-recalled event, as a function of their relative positions (or \textit{lag}).  In other words, a lag of 1 indicates that a recalled event came immediately after the previously recalled event in the video, and a lag of -3 indicates that a recalled event came 3 items before the previously recalled event.  For each recall transition (following the first recall), we computed the lag between the current recall event and the next recall event, normalizing by the total number of possible transitions.  This yielded a number-of-participants (17) by number-of-lags (-33 to +33; 67 lags total) matrix. We averaged over the rows of this matrix to obtain a group-averaged lag-CRP curve (Fig.~\ref{fig:list-learning}B).

\paragraph{Serial position curve (SPC).} SPCs~\citep{Murd62a} reflect the proportion of participants that remember each item as a function of their serial position during encoding. We initialized a number-of-participants (17) by number-of-video-events (34) matrix of zeros. Then, for each recalled event (and each participant), we found the index of the video event that the recalled event most closely matched (via the correlation between the events' topic vectors) and entered a 1 into that position in the matrix (i.e., for the given participant and event). This resulted in a matrix whose entries indicated whether or not each event was recalled by each participant (depending on whether the corresponding entires were set to one or zero.  Finally, we averaged over the rows of the matrix to yield a 1 by 34 array representing the proportion of participants that recalled each event as a function of its serial position (Fig.~\ref{fig:list-learning}C).

\paragraph{Temporal clustering scores.} Temporal clustering refers to the extent to which participants group their recall responses according to encoding position~\citep{PolyEtal09}. For instance, if the participant recalled the video events in the exact order they occurred, this would yield a score of 1.  If the participant recalled the video events in reverse order, this would yield a score of 0.  Similarly, if the participant recalled the events in random order, this yield yield an expected score of 0.5.  For each recall event transition (and separately for each participant), we sorted all not-yet-recalled events according to their absolute lag (i.e., distance away in the video).  We then computed the percentile rank of the next event the participant recalled.  We averaged these percentile ranks across all of the participant's recalls to obtain a single temporal clustering score for the participant (mean: 0.808, SEM: 0.022).  Overall, we found that participants with higher temporal clustering scores also tended to recall more events ($r = 0.62, p = 0.007$).

\paragraph{Semantic clustering scores.} Semantic clustering measures the extent to which participants grouped their recall responses according to semantic similarity~\citep{PolyEtal09}. Here, we used the topic vectors for each event as a proxy for its semantic content. Thus, the similarity between the semantic content for two events can be computed by correlating their respective topic vectors.  For each recall event transition, we sorted all not-yet-recalled events according to how correlated the topic vector \textit{of the closest-matching video event} was to the topic vector of the closest-matching video event to the just-recalled event.  We then computed the percentile rank of the observed next recall.  We averaged these percentile ranks across all of the participant's recalls to obtain a single semantic clustering score for the participant (mean: 0.813, SEM: 0.022).  We found that participants who exhibited stronger semantic clustering scores overall remembered more video events ($r = 0.55, p = 0.02$).


\subsection*{Additional measures of naturalistic memory}
To quantify the similarity between the video topic trajectory and individual recall topic trajectories, we considered several novel metrics.  First, we tested whether each participant's recall trajectory matched the video trajectory in a general sense. For each participant we filtered the video trajectory to only include the events that the participant remembered.  We then computed the root mean squared difference (RMSD) between the remaining video events and the (closest-matching) recalled events.  For example, if the topic vectors for a participant's recall event topic vectors matched the corresponding video event topic vectors exactly (and in order), the expected RMSD for those events would be 0.  However, if the participant's recall events did not perfectly match the video events, or if they were out of order, then the RMSD would be greater than 0.  To assess the significance of the match between the video and recall trajectories, we carried out a permutation procedure whereby, for each of 10000 repetitions, we circularly shifted the recall trajectories (in time) by a random amount and then re-computed the RMSD each time.  This yielded a distribution of ``null'' RMSD values for each participant.  The observed RMSD values reached significance (i.e., $p < 0.05$, reflecting that more than 95\% of the null RMSD values were greater than the observed RMSD value) for nine of the participants (3, 4, 8--13, and 17).  (For the remaining participants this test yielded $p$s$ < 0.25$.)  The observed RMSD values were also reliably correlated with hand-annotated memory performance across participants ($r = -0.57, p = 0.016$).  In other words, a closer match between the video and recall topic trajectories was related to better overall recall performance.

\paragraph*{Precision.}
We tested whether participants who recalled more events were also more \textit{precise} in their recollections. For each participant, we computed the correlation between the topic vectors for each recall event and that of its matching video event (only for the events which they recalled). This resulted in a single number for each recalled event,  indexing how similar the recall event was to its matching video event.  We defined the precision as the average video-recall correlation across all of the events a participant recalled.  We found a strong correlation between hand-annotated memory performance and precision, suggesting that participants who remembered more events also remembered them more veridically ($r= 0.74, p = 0.0006$).



\paragraph*{Distinctiveness.}
We also considered the \textit{distinctiveness} of each recalled event. That is, how uniquely a recalled event's topic vector matched a given video event topic vector, versus the topic vectors for the other video events.
\textbf{JRM STOPPED HERE}
We hypothesized that participants with high memory performance might describe each event in a more distinctive way (relative to those with lower memory performance who might describe events in a more general way). To this end, we computed a `distinctiveness' score for each participant (i.e., 1 - the correlation between a recall event and all non-matching video events).  Then, we averaged this measure over recall events within participant.  We found that participants with higher hand annotated memory performance also had higher distinctiveness scores (Pearson's $r(16) = 0.8, p = 0.0001$).

Lastly, we tested whether participants with better memory performance were also more likely to remember the events in order.  For each participant, we computed the Spearman rank correlation between the order of events that the participant recalled and the actual order of events (filtering events that were actually recalled).  We found that participants who recalled more events also recalled more of them in order (Pearson's $r(16) = 0.5, p = 0.04$). In summary, we found that better memory performance was associated with more precise, distinctive and ordered recalls.


\subsection*{Additional measures of naturalistic memory}
\textbf{Precision.} This measure gives us an indication of the specific match between a video event and recall event, where values approaching 1 are highly precise and lower values are inprecise.  We defined ``precision'' as the correlation between a recall event and its matching (i.e., argmax) video event.

\textbf{Distinctiveness.} Distinctiveness quantifies how similar a recall event is to all non-matching video events.  It provides a metric of how uniquely a particular recall event describes a particular video event.  To compute it, a given recall event is correlated to all video events, the argmax is removed and the rest of the values are averaged. The resulting value is subtracted from 1 such that larger values indicate a more distinctive recall event.



\section*{Participant-level results referenced in the main text}

% Supplemental materials


\begin{figure}[tp]
\centering
\includegraphics[width=1\textwidth]{figs/corrmats}
\caption{\small \textbf{Recall model correlation matrices and event segmentation fits.} Each participant's timepoint-by-timepoint recall correlation matrix.  The yellow boxes represent ``events'' identified by a hidden Markov model.}
\label{fig:corrmats}
\end{figure}

\begin{figure}[tp]
\centering
\includegraphics[width=1\textwidth]{figs/matchmats}
\caption{\small \textbf{Video-recall event model correlation matrices.} Each participant's video event by recall event correlation matrix.  The yellow boxes represent the maximum correlation in each column.}
\label{fig:matchmats}
\end{figure}









%\newpage
\renewcommand{\refname}{Supplemental references}
\bibliography{memlab}


\end{document}
